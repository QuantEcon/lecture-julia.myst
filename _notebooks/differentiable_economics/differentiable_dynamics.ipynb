{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7232fdbb",
   "metadata": {},
   "source": [
    "\n",
    "<a id='lssm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d88ec4",
   "metadata": {},
   "source": [
    "# Differentiating Models of Economic Dynamics\n",
    "\n",
    "\n",
    "<a id='index-0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb4fa0",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Differentiating Models of Economic Dynamics](#Differentiating-Models-of-Economic-Dynamics)  \n",
    "  - [Overview](#Overview)  \n",
    "  - [Simulating a Linear State Space Model](#Simulating-a-Linear-State-Space-Model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b8fae7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This lecture provides an introduction to differentiable simulation of dynamic systems in Julia using Enzyme.jl.  See [auto-differentiation](https://julia.quantecon.org/../more_julia/auto_differentiation.html) for background on in-place patterns and Enzyme wrappers.\n",
    "\n",
    "It builds on the **linear state space** models we introduced in [linear models](https://julia.quantecon.org/../introduction_dynamics/linear_models.html) and the [kalman filter](https://julia.quantecon.org/../introduction_dynamics/kalman.html).\n",
    "\n",
    "Example applications of these methods include:\n",
    "\n",
    "- calibration  \n",
    "- simulated method of moments  \n",
    "- estimation  \n",
    "\n",
    "\n",
    "**Caution** : The code in this section is significantly more advanced than some of the other lectures, and requires some experience with both auto-differentiation concepts and a more detailed understanding of type-safety and memory management in Julia.\n",
    "\n",
    "[Enzyme.jl](https://enzyme.mit.edu/julia/stable/) is under active development and while state-of-the-art, it is often bleeding-edge. Some of the patterns shown here may change in future releases. See [auto-differentiation](https://julia.quantecon.org/../more_julia/auto_differentiation.html) for the latest best practices.\n",
    "\n",
    "In practice, you may find using an LLM valuable for navigating the perplexing error messages of Enzyme.jl. Compilation times can be very slow, and performance intuition is not always straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feff214",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Random, Plots, Test, Enzyme, Statistics\n",
    "using Optimization, OptimizationOptimJL, EnzymeTestUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f291d4",
   "metadata": {},
   "source": [
    "\n",
    "<a id='simm-lss'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb20c8d9",
   "metadata": {},
   "source": [
    "## Simulating a Linear State Space Model\n",
    "\n",
    "Take the following parameterization of a linear state space model\n",
    "\n",
    "$$\n",
    "x_{t+1} = A x_t + C w_{t+1}, \\qquad\n",
    "y_t = G x_t + H v_t\n",
    "$$\n",
    "\n",
    "where $ w_{t+1} $ and $ v_t $ are i.i.d. standard normal shocks. States $ x_t \\in \\mathbb R^N $ and observations $ y_t \\in \\mathbb R^M $ are stored column-wise.\n",
    "\n",
    "See the [auto-differentiation](https://julia.quantecon.org/../more_julia/auto_differentiation.html)  lecture for more on efficient in-place operations and `mul!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba680a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "function simulate_lss!(x, y, model, x_0, w, v)\n",
    "    (; A, C, G, H) = model\n",
    "    N, T1 = size(x)\n",
    "    M, T1y = size(y)\n",
    "    T = size(w, 2)\n",
    "\n",
    "    @assert T1 == T + 1 == T1y\n",
    "    @assert size(v, 2) == T + 1\n",
    "    @assert size(A) == (N, N)\n",
    "    @assert size(G) == (M, N)\n",
    "    @assert size(C, 1) == N\n",
    "    @assert size(H, 1) == M\n",
    "    @assert length(x_0) == N\n",
    "\n",
    "    # Enzyme has challenges with activity analysis on copyto!/broadcasting assignments\n",
    "    @inbounds for i in 1:N\n",
    "        x[i, 1] = x_0[i]\n",
    "    end\n",
    "    # Apply evolution and observation equations\n",
    "    @inbounds for t in 1:T\n",
    "        @views mul!(x[:, t + 1], A, x[:, t])             # x_{t+1} = A x_t\n",
    "        @views mul!(x[:, t + 1], C, w[:, t], 1.0, 1.0)   # + C w_{t+1}\n",
    "\n",
    "        @views mul!(y[:, t], G, x[:, t])                 # y_t = G x_t\n",
    "        @views mul!(y[:, t], H, v[:, t], 1.0, 1.0)       # + H v_t\n",
    "    end\n",
    "    # Apply observation equation at T+1\n",
    "    @views mul!(y[:, T + 1], G, x[:, T + 1])\n",
    "    @views mul!(y[:, T + 1], H, v[:, T + 1], 1.0, 1.0)\n",
    "\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a346b50",
   "metadata": {},
   "source": [
    ">**Note**\n",
    ">\n",
    ">We use a manual loop instead of `copyto!` or broadcasting for the initial condition assignment because Enzyme has challenges with activity analysis on these operations. When `x_0` is `Const` but `x` is `Duplicated`, `copyto!` triggers a “Detected potential need for runtime activity” error. The explicit loop avoids this issue. See the [auto-differentiation](https://julia.quantecon.org/../more_julia/auto_differentiation.html) lecture for details.\n",
    "\n",
    "Crucially, this function modifies the preallocated `x` and `y` arrays in place without any allocations.\n",
    "\n",
    "We can use this function to simulate from example matrices and a sequence of shocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ac6f6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "Random.seed!(1234)\n",
    "\n",
    "N, M, K, L = 3, 2, 2, 2\n",
    "T = 10\n",
    "\n",
    "A = [0.8 0.1 0.0\n",
    "     0.0 0.7 0.1\n",
    "     0.0 0.0 0.6]\n",
    "C = 0.1 .* randn(N, K)\n",
    "G = [1.0 0.0 0.0\n",
    "     0.0 1.0 0.3]\n",
    "H = 0.05 .* randn(M, L)\n",
    "model = (; A, C, G, H)\n",
    "\n",
    "x_0 = randn(N)\n",
    "w = randn(K, T)\n",
    "v = randn(L, T + 1)\n",
    "\n",
    "x = zeros(N, T + 1)\n",
    "y = zeros(M, T + 1)\n",
    "\n",
    "simulate_lss!(x, y, model, x_0, w, v)\n",
    "\n",
    "time = 0:T\n",
    "plot(time, x', lw = 2, xlabel = \"t\", ylabel = \"state\", label = [\"x1\" \"x2\" \"x3\"],\n",
    "     title = \"State Paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5442c27",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "plot(time, y', lw = 2, xlabel = \"t\", ylabel = \"observation\",\n",
    "     label = [\"y1\" \"y2\"], title = \"Observation Paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9408fd",
   "metadata": {},
   "source": [
    "\n",
    "<a id='simm-lss-diff'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38dd3fd",
   "metadata": {},
   "source": [
    "### Differentiating the Simulation\n",
    "\n",
    "Forward-mode in Enzyme is convenient for impulse-style effects: for example, here we perturb only the $ w_1 $ leaving everything else fixed and can see the change in the $ x $ and $ y $ paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaff564",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# forward-mode on w[1]\n",
    "x = zeros(N, T + 1)\n",
    "y = zeros(M, T + 1)\n",
    "dx = Enzyme.make_zero(x)\n",
    "dx_0 = Enzyme.make_zero(x_0)\n",
    "dy = Enzyme.make_zero(y)\n",
    "dw = Enzyme.make_zero(w)\n",
    "dw[1] = 1.0                         # unit perturbation to first shock\n",
    "\n",
    "autodiff(Forward,\n",
    "         simulate_lss!,\n",
    "         Duplicated(x, dx),\n",
    "         Duplicated(y, dy),\n",
    "         Const(model), # leaving model fixed\n",
    "         Duplicated(x_0, dx_0), # won't perturb\n",
    "         Duplicated(w, dw), # perturbing w\n",
    "         Const(v))\n",
    "\n",
    "dx[:, 1:3]   # early-state sensitivities (impulse response flavor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589d2cd",
   "metadata": {},
   "source": [
    "Batch tangents let us reuse one primal evaluation while seeding multiple partials. Below we differentiate with respect to two entries of $ A $ in one call; Enzyme accumulates the tangents into separate shadow arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7a7e9",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "dx_batch = (Enzyme.make_zero(x), Enzyme.make_zero(x))\n",
    "dy_batch = (Enzyme.make_zero(y), Enzyme.make_zero(y))\n",
    "dmodels = (Enzyme.make_zero(model), Enzyme.make_zero(model))\n",
    "dmodels[1].A[1] = 1.0\n",
    "dmodels[2].A[2] = 1.0\n",
    "\n",
    "autodiff(Forward,\n",
    "         simulate_lss!,\n",
    "         BatchDuplicated(x, dx_batch), # batch duplicated to match dmodels\n",
    "         BatchDuplicated(y, dy_batch),\n",
    "         BatchDuplicated(model, dmodels),\n",
    "         Const(x_0),\n",
    "         Const(w),\n",
    "         Const(v))\n",
    "@show dy_batch[1], dy_batch[2];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b4f06a",
   "metadata": {},
   "source": [
    "### Checking Type Stability and Allocations\n",
    "\n",
    "With complicated code, we first need to ensure the code is type-stable.  The following call is silent, which indicates there are no type-stability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30f934",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "@inferred simulate_lss!(x, y, model, x_0, w, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8de9a",
   "metadata": {},
   "source": [
    "Next we check that it does not allocate any memory during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7d41f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "function count_allocs()\n",
    "    return simulate_lss!(x, y, model, x_0, w, v)\n",
    "end\n",
    "@btime count_allocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be3e73",
   "metadata": {},
   "source": [
    "Finally, for complicated functions such as simulations, we cannot\n",
    "assume that Enzyme (or any AD system) will necessarily be correct.\n",
    "\n",
    "To aid in this, the `EnzymeTestUtils` provides utilities for this purpose which automatically check against finite-difference approximations using the appropriate seeding.\n",
    "\n",
    "When using in-place functions mutating the arguments `test_forward` requires that you pass any mutated arguments as output of the function itself, which can be done by a small wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb68fe",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "function test_forward_simulate_lss!(x, y, model, x_0, w, v)\n",
    "    simulate_lss!(x, y, model, x_0, w, v)\n",
    "    return x, y\n",
    "end\n",
    "test_forward(test_forward_simulate_lss!,\n",
    "             Duplicated,\n",
    "             (x, Duplicated),\n",
    "             (y, Duplicated),\n",
    "             (model, Const),\n",
    "             (x_0, Duplicated),\n",
    "             (w, Const),\n",
    "             (v, Const))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e26786",
   "metadata": {},
   "source": [
    "Unlike `test_forward`, the automatic checks on reverse-mode AD with `test_reverse` require a scalar output, which we discuss below in the calibration section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3b367",
   "metadata": {},
   "source": [
    "### Differentiating Functions of Simulation\n",
    "\n",
    "Often we care about scalars of the simulated paths. For example, the average of the first observable is\n",
    "\n",
    "$$\n",
    "g(w, v, \\theta) = \\frac{1}{T+1} \\sum_{t=0}^T y_{1,t}.\n",
    "$$\n",
    "\n",
    "Reverse-mode gives the gradient with respect to all shocks in one sweep while holding parameters fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba0b27",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "function mean_first_observation(y)\n",
    "    return mean(@view y[1, :]) # view to avoid allocation\n",
    "end\n",
    "\n",
    "function g(x, y, model, x_0, w, v)\n",
    "    simulate_lss!(x, y, model, x_0, w, v)\n",
    "    return mean_first_observation(y)\n",
    "end\n",
    "\n",
    "x_rev = zeros(N, T + 1)\n",
    "y_rev = zeros(M, T + 1)\n",
    "dx_rev = Enzyme.make_zero(x_rev)\n",
    "dy_rev = Enzyme.make_zero(y_rev)\n",
    "dw_rev = Enzyme.make_zero(w)\n",
    "dv_rev = Enzyme.make_zero(v)\n",
    "\n",
    "autodiff(Reverse,\n",
    "         g,\n",
    "         Duplicated(x_rev, dx_rev), # output/buffer\n",
    "         Duplicated(y_rev, dy_rev), # output/buffer\n",
    "         Const(model),\n",
    "         Const(x_0),\n",
    "         Duplicated(w, dw_rev),   # active shocks\n",
    "         Duplicated(v, dv_rev))\n",
    "\n",
    "@show g(x, y, model, x_0, w, v)\n",
    "\n",
    "dw_rev # sensitivity wrt evolution shock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2cb760",
   "metadata": {},
   "source": [
    "These examples mirror the larger workflow: write allocation-free, in-place simulations, seed tangents with `Duplicated`, and use forward or reverse mode depending on whether you want many outputs per input (forward) or many inputs to one scalar (reverse).\n",
    "\n",
    "As before, for complicated functions we may want to check that the gradients are correct using finite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330058dd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "test_reverse(g,\n",
    "             Active,\n",
    "             (x_rev, Duplicated),\n",
    "             (y_rev, Duplicated),\n",
    "             (model, Const),\n",
    "             (x_0, Const),\n",
    "             (w, Duplicated),\n",
    "             (v, Duplicated))\n",
    "\n",
    "# Or with a different set of arguments\n",
    "test_reverse(g,\n",
    "             Active,\n",
    "             (x_rev, Duplicated),\n",
    "             (y_rev, Duplicated),\n",
    "             (model, Duplicated),\n",
    "             (x_0, Duplicated),\n",
    "             (w, Const),\n",
    "             (v, Const))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564bb531",
   "metadata": {},
   "source": [
    "In all cases we must ensure the mutated arguments are passed as `Duplicated`.\n",
    "\n",
    "Functions which internally use buffers can allocate them, but will need to ensure that the buffers are of the appropriate type (i.e., duplicated if they are active).  This can be achieved with the `eltype`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5b785",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "function g2(model, x_0, w, v)\n",
    "    x = zeros(eltype(x_0), N, T + 1)\n",
    "    y = zeros(eltype(x_0), M, T + 1)\n",
    "    simulate_lss!(x, y, model, x_0, w, v)\n",
    "    return mean_first_observation(y)\n",
    "end\n",
    "g2(model, x_0, w, v)\n",
    "gradient(Reverse, g2, model, x_0, w, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80306087",
   "metadata": {},
   "source": [
    "### Calibration\n",
    "\n",
    "Reverse-mode in particular can be useful for calibration with simulated dynamics.\n",
    "\n",
    "For example, consider if the our $ A $ matrix was parameterized by a scalar $ a $ in its upper left corner, and we wanted to calibrate $ a $ so that the time average of the first observation matched a target value $ y^* = 0 $.\n",
    "\n",
    "For some technical reasons discussed below, we rewrite the simulation to take the model parameters individually rather than as a named tuple. A version that works with the original function is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1324e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "function simulate_lss!(x, y, A, C, G, H, x_0, w, v)\n",
    "    N, T1 = size(x)\n",
    "    M, T1y = size(y)\n",
    "    T = size(w, 2)\n",
    "\n",
    "    @assert T1 == T + 1 == T1y\n",
    "    @assert size(v, 2) == T + 1\n",
    "    @assert size(A) == (N, N)\n",
    "    @assert size(G) == (M, N)\n",
    "    @assert size(C, 1) == N\n",
    "    @assert size(H, 1) == M\n",
    "    @assert length(x_0) == N\n",
    "\n",
    "    # Enzyme has challenges with activity analysis on broadcasting assignments\n",
    "    @inbounds for i in 1:N\n",
    "        x[i, 1] = x_0[i]\n",
    "    end\n",
    "    # Apply evolution and observation equations\n",
    "    @inbounds for t in 1:T\n",
    "        @views mul!(x[:, t + 1], A, x[:, t])             # x_{t+1} = A x_t\n",
    "        @views mul!(x[:, t + 1], C, w[:, t], 1.0, 1.0)   # + C w_{t+1}\n",
    "\n",
    "        @views mul!(y[:, t], G, x[:, t])                 # y_t = G x_t\n",
    "        @views mul!(y[:, t], H, v[:, t], 1.0, 1.0)       # + H v_t\n",
    "    end\n",
    "    # Apply observation equation at T+1\n",
    "    @views mul!(y[:, T + 1], G, x[:, T + 1])\n",
    "    @views mul!(y[:, T + 1], H, v[:, T + 1], 1.0, 1.0)\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function parameterized_A(a)\n",
    "    return [a 0.1 0.0; 0.0 0.7 0.1; 0.0 0.0 0.6]\n",
    "end\n",
    "\n",
    "function loss(u, p)\n",
    "    (; x_0, w, v, y_target, C, G, H) = p\n",
    "    T = size(w, 2)\n",
    "    a = u[1]\n",
    "\n",
    "    A = parameterized_A(a)\n",
    "\n",
    "    # Allocate buffers and simulate\n",
    "    x = zeros(eltype(A), length(x_0), T + 1)\n",
    "    y = zeros(eltype(A), size(G, 1), T + 1)\n",
    "    simulate_lss!(x, y, A, C, G, H, x_0, w, v)\n",
    "\n",
    "    y_mean = mean_first_observation(y)\n",
    "    return (y_mean - y_target)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a2675",
   "metadata": {},
   "source": [
    "There are a few tricks to note here, which work around challenges with using Enzyme in its current state.\n",
    "\n",
    "- With the SciML packages, such as [Optimization.jl](https://github.com/SciML/Optimization.jl), the `AutoEnzyme()` automatically determines which variables to mark as `Active` following certain patterns  \n",
    "- In particular, `p` holds parameters assumed to be constant during optimization, while `u` holds the optimization variables.  \n",
    "- Enzyme generally avoids allocations, but allocation is necessary here since we need to create a new model with the appropriate `a` value.  \n",
    "- For the buffers, note the use of `eltype(A)` to ensure the correct type, since the `A` matrix itself will be differentiable.  \n",
    "\n",
    "\n",
    "Using this setup, we can create the `p` parameter named tuple, none of which will be differentiable, and then use the LBFGS optimizer in OptimizationOptimJL to find the optimal `a` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37bfc6",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y_target = 0.0\n",
    "\n",
    "# Bundle parameters into a named tuple\n",
    "p = (; x_0, w, v, y_target, C, G, H)\n",
    "\n",
    "# Initial Guess for 'a'\n",
    "u_0 = [0.8]\n",
    "\n",
    "println(\"Initial a=$(u_0[1]), Loss: $(loss(u_0, p))\")\n",
    "\n",
    "# Define the problem type, associate initial condition and parameters\n",
    "# AutoEnzyme() will handle the AD setup\n",
    "optf = OptimizationFunction(loss, AutoEnzyme())\n",
    "prob = OptimizationProblem(optf, u_0, p)\n",
    "\n",
    "sol = solve(prob, OptimizationOptimJL.LBFGS())\n",
    "println(\"Final a=$(sol.u[1]), Loss: $(loss(sol.u, p))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b446b",
   "metadata": {},
   "source": [
    "### Calibration with More Complicated Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8023c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "function loss_2(u, p)\n",
    "\n",
    "    # Unpack constants\n",
    "    (; x_0, w, v, y_target, C, G, H) = p\n",
    "    T = size(w, 2)\n",
    "    a = u[1]\n",
    "\n",
    "    A = parameterized_A(a) # A is Active (depends on u)\n",
    "\n",
    "    # Trick: \"Launder\" the constants by copying them.\n",
    "    # Creates new locals which Enzyme sees these as \"Local Active Variables\"\n",
    "    # Now build the struct with homogeneous (all local) variables\n",
    "    model = (; A, C = copy(C), G = copy(G), H = copy(H))\n",
    "\n",
    "    # Allocate buffers and simulate\n",
    "    x = zeros(eltype(A), length(x_0), T + 1)\n",
    "    y = zeros(eltype(A), size(G, 1), T + 1)\n",
    "    simulate_lss!(x, y, model, x_0, w, v)\n",
    "\n",
    "    y_mean = mean_first_observation(y)\n",
    "    return (y_mean - y_target)^2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd95bde",
   "metadata": {},
   "source": [
    "This version uses our original `simulate_lss!` function, which takes a named tuple for the model parameters. Note that it contains a trick where the constant parameters `C`, `G`, and `H` are “laundered” by copying them into new local variables before building the named tuple. This ensures that Enzyme can properly analyze which variables are active versus constant when creating the named tuple.\n",
    "\n",
    "The `simulate_lss!` approach that splits out the `A` matrix might be more efficient for large $ C, G, H $ since it avoids the copies, but it may not matter in practice.\n",
    "\n",
    "The other code is identical with this new loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f7e53",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "y_target = 0.0\n",
    "\n",
    "# Bundle parameters into a named tuple\n",
    "p = (; x_0, w, v, y_target, C, G, H)\n",
    "\n",
    "# Initial Guess for 'a'\n",
    "u_0 = [0.8]\n",
    "\n",
    "println(\"Initial a=$(u_0[1]), Loss: $(loss_2(u_0, p))\")\n",
    "\n",
    "# Define the problem type, associate initial condition and parameters\n",
    "# AutoEnzyme() will handle the AD setup\n",
    "optf = OptimizationFunction(loss_2, AutoEnzyme())\n",
    "prob = OptimizationProblem(optf, u_0, p)\n",
    "\n",
    "sol = solve(prob, OptimizationOptimJL.LBFGS())\n",
    "println(\"Final a=$(sol.u[1]), Loss: $(loss_2(sol.u, p))\")"
   ]
  }
 ],
 "metadata": {
  "date": 1770675606.2877274,
  "filename": "differentiable_dynamics.md",
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia-1.12"
  },
  "title": "Differentiating Models of Economic Dynamics"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}