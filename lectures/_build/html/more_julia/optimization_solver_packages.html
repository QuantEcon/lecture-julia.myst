
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>10. Solvers, Optimizers, and Automatic Differentiation &#8212; Quantitative Economics with Julia</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/quantecon-book-theme.9deb0a26de8466f54124a1959c24cd33.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">


    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="../_static/quantecon-book-theme.43eb86aa48ec3aed660cb313c38068cd.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://julia.quantecon.org/more_julia/optimization_solver_packages.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11. Julia Tools and Editors" href="tools_editors.html" />
    <link rel="prev" title="9. Data and Statistics Packages" href="data_statistical_packages.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Jesse Perla &amp; Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Julia, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on quantitative economic modeling, designed and written by Jesse Perla, Thomas J. Sargent and John Stachurski. The language instruction is Julia. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Solvers, Optimizers, and Automatic Differentiation"/>
<meta name="twitter:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Jesse Perla, Thomas J. Sargent and John Stachurski. The language instruction is Julia.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Solvers, Optimizers, and Automatic Differentiation" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://julia.quantecon.org/more_julia/optimization_solver_packages.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on quantitative economic modeling, designed and written by Jesse Perla, Thomas J. Sargent and John Stachurski. The language instruction is Julia." />
<meta property="og:site_name" content="Quantitative Economics with Julia" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="wrapper">

        <div class="main">

            <div class="page" id=more_julia/optimization_solver_packages>

                <div class="page__toc">

                    <div class="inner">

                        
                        <div class="page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   10.1. Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setup">
     10.1.1. Setup
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-differentiable-programming">
   10.2. Introduction to Differentiable Programming
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-mode-automatic-differentiation">
     10.2.1. Forward-Mode Automatic Differentiation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forward-mode-with-dual-numbers">
     10.2.2. Forward-Mode with Dual Numbers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forwarddiff-jl">
     10.2.3. ForwardDiff.jl
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#zygote-jl">
     10.2.4. Zygote.jl
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization">
   10.3. Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optim-jl">
     10.3.1. Optim.jl
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#univariate-functions-on-bounded-intervals">
       10.3.1.1. Univariate Functions on Bounded Intervals
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unconstrained-multivariate-optimization">
       10.3.1.2. Unconstrained Multivariate Optimization
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jump-jl">
     10.3.2. JuMP.jl
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#blackboxoptim-jl">
     10.3.3. BlackBoxOptim.jl
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#systems-of-equations-and-least-squares">
   10.4. Systems of Equations and Least Squares
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roots-jl">
     10.4.1. Roots.jl
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nlsolve-jl">
     10.4.2. NLsolve.jl
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#leastsquaresoptim-jl">
   10.5. LeastSquaresOptim.jl
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#additional-notes">
   10.6. Additional Notes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   10.7. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     10.7.1. Exercise 1
    </a>
   </li>
  </ul>
 </li>
</ul>

                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="../_static/qe-logo-large.png" class="logo" alt="logo"></a>
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="page__header">

                    <div class="page__header-copy">

                        <p class="page__header-heading"><a href="intro.html">Quantitative Economics with Julia</a></p>

                        <p class="page__header-subheading">Solvers, Optimizers, and Automatic Differentiation</p>

                    </div>

                    <p class="page__header-authors">Jesse Perla & Thomas J. Sargent & John Stachurski</p>

                </div> <!-- .page__header -->



                
                <main class="page__content" role="main">
                    
                    <div>
                        
  <div id="qe-notebook-header" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div><div class="section" id="solvers-optimizers-and-automatic-differentiation">
<h1><a class="toc-backref" href="#id1"><span class="section-number">10. </span>Solvers, Optimizers, and Automatic Differentiation</a><a class="headerlink" href="#solvers-optimizers-and-automatic-differentiation" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#solvers-optimizers-and-automatic-differentiation" id="id1">Solvers, Optimizers, and Automatic Differentiation</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id2">Overview</a></p></li>
<li><p><a class="reference internal" href="#introduction-to-differentiable-programming" id="id3">Introduction to Differentiable Programming</a></p></li>
<li><p><a class="reference internal" href="#optimization" id="id4">Optimization</a></p></li>
<li><p><a class="reference internal" href="#systems-of-equations-and-least-squares" id="id5">Systems of Equations and Least Squares</a></p></li>
<li><p><a class="reference internal" href="#leastsquaresoptim-jl" id="id6">LeastSquaresOptim.jl</a></p></li>
<li><p><a class="reference internal" href="#additional-notes" id="id7">Additional Notes</a></p></li>
<li><p><a class="reference internal" href="#exercises" id="id8">Exercises</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id2"><span class="section-number">10.1. </span>Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>In this lecture we introduce a few of the Julia libraries that we’ve found particularly useful for quantitative work in economics.</p>
<div class="section" id="setup">
<h3><span class="section-number">10.1.1. </span>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="introduction-to-differentiable-programming">
<h2><a class="toc-backref" href="#id3"><span class="section-number">10.2. </span>Introduction to Differentiable Programming</a><a class="headerlink" href="#introduction-to-differentiable-programming" title="Permalink to this headline">¶</a></h2>
<p>The promise of differentiable programming is that we can move towards taking the derivatives of almost arbitrarily
complicated computer programs, rather than simply thinking about the derivatives of mathematical functions.  Differentiable
programming is the natural evolution of automatic differentiation (AD, sometimes called algorithmic differentiation).</p>
<p>Stepping back, there are three ways to calculate the gradient or Jacobian</p>
<ul class="simple">
<li><p>Analytic derivatives / Symbolic differentiation</p>
<ul>
<li><p>You can sometimes calculate the derivative on pen-and-paper, and potentially simplify the expression.</p></li>
<li><p>In effect, repeated applications of the chain rule, product rule, etc.</p></li>
<li><p>It is sometimes, though not always, the most accurate and fastest option if there are algebraic simplifications.</p></li>
<li><p>Sometimes symbolic integration on the computer a good solution, if the package can handle your functions. Doing algebra by hand is tedious and error-prone, but
is sometimes invaluable.</p></li>
</ul>
</li>
<li><p>Finite differences</p>
<ul>
<li><p>Evaluate the function at least <span class="math notranslate nohighlight">\(N+1\)</span> times to get the gradient – Jacobians are even worse.</p></li>
<li><p>Large <span class="math notranslate nohighlight">\(\Delta\)</span> is numerically stable but inaccurate, too small of <span class="math notranslate nohighlight">\(\Delta\)</span> is numerically unstable but more accurate.</p></li>
<li><p>Choosing the <span class="math notranslate nohighlight">\(\Delta\)</span> is hard, so use packages such as <a class="reference external" href="https://github.com/JuliaDiffEq/DiffEqDiffTools.jl">DiffEqDiffTools.jl</a>.</p></li>
<li><p>If a function is <span class="math notranslate nohighlight">\(R^N \to R\)</span> for a large <span class="math notranslate nohighlight">\(N\)</span>, this requires <span class="math notranslate nohighlight">\(O(N)\)</span> function evaluations.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[
\partial_{x_i}f(x_1,\ldots x_N) \approx \frac{f(x_1,\ldots x_i + \Delta,\ldots x_N) - f(x_1,\ldots x_i,\ldots x_N)}{\Delta}
\]</div>
<ul class="simple">
<li><p>Automatic Differentiation</p>
<ul>
<li><p>The same as analytic/symbolic differentiation, but where the <strong>chain rule</strong> is calculated <strong>numerically</strong> rather than symbolically.</p></li>
<li><p>Just as with analytic derivatives, can establish rules for the derivatives of individual functions (e.g. <span class="math notranslate nohighlight">\(d\left(sin(x)\right)\)</span> to <span class="math notranslate nohighlight">\(cos(x) dx\)</span>) for intrinsic derivatives.</p></li>
</ul>
</li>
</ul>
<p>AD has two basic approaches, which are variations on the order of evaluating the chain rule: reverse and forward mode (although mixed mode is possible).</p>
<ol class="simple">
<li><p>If a function is <span class="math notranslate nohighlight">\(R^N \to R\)</span>, then <strong>reverse-mode</strong> AD can find the gradient in <span class="math notranslate nohighlight">\(O(1)\)</span> sweep (where a “sweep” is <span class="math notranslate nohighlight">\(O(1)\)</span> function evaluations).</p></li>
<li><p>If a function is <span class="math notranslate nohighlight">\(R \to R^N\)</span>, then <strong>forward-mode</strong> AD can find the jacobian in <span class="math notranslate nohighlight">\(O(1)\)</span> sweeps.</p></li>
</ol>
<p>We will explore two types of automatic differentiation in Julia (and discuss a few packages which implement them).  For both, remember the <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a></p>
<div class="math notranslate nohighlight">
\[
\frac{dy}{dx} = \frac{dy}{dw} \cdot \frac{dw}{dx}
\]</div>
<p>Forward-mode starts the calculation from the left with <span class="math notranslate nohighlight">\(\frac{dy}{dw}\)</span> first, which then calculates the product with <span class="math notranslate nohighlight">\(\frac{dw}{dx}\)</span>.  On the other hand, reverse mode starts on the right hand side with <span class="math notranslate nohighlight">\(\frac{dw}{dx}\)</span> and works backwards.</p>
<p>Take an example a function with fundamental operations and known analytical derivatives</p>
<div class="math notranslate nohighlight">
\[
f(x_1, x_2) = x_1 x_2 + \sin(x_1)
\]</div>
<p>And rewrite this as a function which contains a sequence of simple operations and temporaries.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span> <span class="n">f</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">)</span>
    <span class="n">w_1</span> <span class="o">=</span> <span class="n">x_1</span>
    <span class="n">w_2</span> <span class="o">=</span> <span class="n">x_2</span>
    <span class="n">w_3</span> <span class="o">=</span> <span class="n">w_1</span> <span class="o">*</span> <span class="n">w_2</span>
    <span class="n">w_4</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">w_1</span><span class="p">)</span>
    <span class="n">w_5</span> <span class="o">=</span> <span class="n">w_3</span> <span class="o">+</span> <span class="n">w_4</span>
    <span class="k">return</span> <span class="n">w_5</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Here we can identify all of the underlying functions (<code class="docutils literal notranslate"><span class="pre">*,</span> <span class="pre">sin,</span> <span class="pre">+</span></code>), and see if each has an
intrinsic derivative.  While these are obvious, with Julia we could come up with all sorts of differentiation rules for arbitrarily
complicated combinations and compositions of intrinsic operations.  In fact, there is even <a class="reference external" href="https://github.com/JuliaDiff/ChainRules.jl">a package</a> for registering more.</p>
<div class="section" id="forward-mode-automatic-differentiation">
<h3><span class="section-number">10.2.1. </span>Forward-Mode Automatic Differentiation<a class="headerlink" href="#forward-mode-automatic-differentiation" title="Permalink to this headline">¶</a></h3>
<p>In forward-mode AD, you first fix the variable you are interested in (called “seeding”), and then evaluate the chain rule in left-to-right order.</p>
<p>For example, with our <span class="math notranslate nohighlight">\(f(x_1, f_2)\)</span> example above, if we wanted to calculate the derivative with respect to <span class="math notranslate nohighlight">\(x_1\)</span> then
we can seed the setup accordingly.  <span class="math notranslate nohighlight">\(\frac{\partial  w_1}{\partial  x_1} = 1\)</span> since we are taking the derivative of it, while <span class="math notranslate nohighlight">\(\frac{\partial  w_1}{\partial  x_1} = 0\)</span>.</p>
<p>Following through with these, redo all of the calculations for the derivative in parallel with the function itself.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{l|l}
f(x_1, x_2) &amp;
\frac{\partial f(x_1,x_2)}{\partial x_1}
\\
\hline
w_1 = x_1 &amp;
\frac{\partial  w_1}{\partial  x_1} = 1 \text{ (seed)}\\
w_2 = x_2 &amp;
\frac{\partial   w_2}{\partial  x_1} = 0 \text{ (seed)}
\\
w_3 = w_1 \cdot w_2 &amp;
\frac{\partial  w_3}{\partial x_1} = w_2 \cdot \frac{\partial   w_1}{\partial  x_1} + w_1 \cdot \frac{\partial   w_2}{\partial  x_1}
\\
w_4 = \sin w_1 &amp;
\frac{\partial   w_4}{\partial x_1} = \cos w_1 \cdot \frac{\partial  w_1}{\partial x_1}
\\
w_5 = w_3 + w_4 &amp;
\frac{\partial  w_5}{\partial x_1} = \frac{\partial  w_3}{\partial x_1} + \frac{\partial  w_4}{\partial x_1}
\end{array}
\end{split}\]</div>
<p>Since these two could be done at the same time, we say there is “one pass” required for this calculation.</p>
<p>Generalizing a little, if the function was vector-valued, then that single pass would get the entire row of the Jacobian in that single pass.  Hence for a <span class="math notranslate nohighlight">\(R^N \to R^M\)</span> function, requires <span class="math notranslate nohighlight">\(N\)</span> passes to get a dense Jacobian using forward-mode AD.</p>
<p>How can you implement forward-mode AD?  It turns out to be fairly easy with a generic programming language to make a simple example (while the devil is in the details for
a high-performance implementation).</p>
</div>
<div class="section" id="forward-mode-with-dual-numbers">
<h3><span class="section-number">10.2.2. </span>Forward-Mode with Dual Numbers<a class="headerlink" href="#forward-mode-with-dual-numbers" title="Permalink to this headline">¶</a></h3>
<p>One way to implement forward-mode AD is to use <a class="reference external" href="https://en.wikipedia.org/wiki/Dual_number">dual numbers</a>.</p>
<p>Instead of working with just a real number, e.g. <span class="math notranslate nohighlight">\(x\)</span>, we will augment each with an infinitesimal <span class="math notranslate nohighlight">\(\epsilon\)</span> and use <span class="math notranslate nohighlight">\(x + \epsilon\)</span>.</p>
<p>From Taylor’s theorem,</p>
<div class="math notranslate nohighlight">
\[
f(x + \epsilon) = f(x) + f'(x)\epsilon + O(\epsilon^2)
\]</div>
<p>where we will define the infinitesimal such that <span class="math notranslate nohighlight">\(\epsilon^2 = 0\)</span>.</p>
<p>With this definition, we can write a general rule for differentiation of <span class="math notranslate nohighlight">\(g(x,y)\)</span> as the chain rule for the total derivative</p>
<div class="math notranslate nohighlight">
\[
g(x + \epsilon, y + \epsilon) = g(x, y) + (\partial_x g(x,y) + \partial_y g(x,y))\epsilon
\]</div>
<p>But, note that if we keep track of the constant in front of the <span class="math notranslate nohighlight">\(\epsilon\)</span> terms (e.g. a <span class="math notranslate nohighlight">\(x'\)</span> and <span class="math notranslate nohighlight">\(y'\)</span>)</p>
<div class="math notranslate nohighlight">
\[
g(x + x'\epsilon, y + y'\epsilon) = g(x, y) + (\partial_x g(x,y)x' + \partial_y g(x,y)y')\epsilon
\]</div>
<p>This is simply the chain rule.  A few more examples</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        (x + x'\epsilon) + (y + y'\epsilon) &amp;= (x + y) + (x' + y')\epsilon\\
(x + x'\epsilon)\times(y + y'\epsilon) &amp;= (xy) + (x'y + y'x)\epsilon\\
\exp(x + x'\epsilon) &amp;= \exp(x) + (x'\exp(x))\epsilon\\
        \end{aligned}
\end{split}\]</div>
<p>Using the generic programming in Julia, it is easy to define a new dual number type which can encapsulate the pair <span class="math notranslate nohighlight">\((x, x')\)</span> and provide a definitions for
all of the basic operations.  Each definition then has the chain-rule built into it.</p>
<p>With this approach, the “seed” process is simple the creation of the <span class="math notranslate nohighlight">\(\epsilon\)</span> for the underlying variable.</p>
<p>So if we have the function <span class="math notranslate nohighlight">\(f(x_1, x_2)\)</span> and we wanted to find the derivative <span class="math notranslate nohighlight">\(\partial_{x_1} f(3.8, 6.9)\)</span> then then we would seed them with the dual numbers <span class="math notranslate nohighlight">\(x_1 \to (3.8, 1)\)</span> and <span class="math notranslate nohighlight">\(x_2 \to (6.9, 0)\)</span>.</p>
<p>If you then follow all of the same scalar operations above with a seeded dual number, it will calculate both the function value and the derivative in a single “sweep” and without modifying any of your (generic) code.</p>
</div>
<div class="section" id="forwarddiff-jl">
<h3><span class="section-number">10.2.3. </span>ForwardDiff.jl<a class="headerlink" href="#forwarddiff-jl" title="Permalink to this headline">¶</a></h3>
<p>Dual-numbers are at the heart of one of the AD packages we have already seen.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">ForwardDiff</span>
<span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">sinh</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="c"># multivariate.</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.4</span> <span class="mf">2.2</span><span class="p">]</span>
<span class="nd">@show</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">h</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="c"># use AD, seeds from x</span>

<span class="c">#Or, can use complicated functions of many variables</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum</span><span class="p">(</span><span class="n">sin</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">prod</span><span class="p">(</span><span class="n">tan</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum</span><span class="p">(</span><span class="n">sqrt</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">);</span> <span class="c"># g() is now the gradient</span>
<span class="n">g</span><span class="p">(</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span> <span class="c"># gradient at a random point</span>
<span class="c"># ForwardDiff.hessian(f,x&#39;) # or the hessian</span>
</pre></div>
</div>
<p>We can even auto-differentiate complicated functions with embedded iterations.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span> <span class="n">squareroot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c">#pretending we don&#39;t know sqrt()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># Initial starting point for Newton’s method</span>
    <span class="k">while</span> <span class="n">abs</span><span class="p">(</span><span class="n">z</span><span class="o">*</span><span class="n">z</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-13</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">-</span> <span class="p">(</span><span class="n">z</span><span class="o">*</span><span class="n">z</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="n">z</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">z</span>
<span class="k">end</span>
<span class="n">squareroot</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">ForwardDiff</span>
<span class="n">dsqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">ForwardDiff</span><span class="o">.</span><span class="n">derivative</span><span class="p">(</span><span class="n">squareroot</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">dsqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="zygote-jl">
<h3><span class="section-number">10.2.4. </span>Zygote.jl<a class="headerlink" href="#zygote-jl" title="Permalink to this headline">¶</a></h3>
<p>Unlike forward-mode auto-differentiation, reverse-mode is very difficult to implement efficiently, and there are many variations on the best approach.</p>
<p>Many reverse-mode packages are connected to machine-learning packages, since the efficient gradients of <span class="math notranslate nohighlight">\(R^N \to R\)</span> loss functions are necessary for the gradient descent optimization algorithms used in machine learning.</p>
<p>One recent package is <a class="reference external" href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>, which is used in the Flux.jl framework.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">Zygote</span>

<span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mi">3</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">y</span><span class="o">*</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span>
<span class="n">gradient</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we see that Zygote has a gradient function as the interface, which returns a tuple.</p>
<p>You could create this as an operator if you wanted to.,</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">D</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">=</span> <span class="n">x</span><span class="o">-&gt;</span> <span class="n">gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c"># returns first in tuple</span>

<span class="n">D_sin</span> <span class="o">=</span> <span class="n">D</span><span class="p">(</span><span class="n">sin</span><span class="p">)</span>
<span class="n">D_sin</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
</pre></div>
</div>
<p>For functions of one (Julia) variable, we can find the by simply using the <code class="docutils literal notranslate"><span class="pre">'</span></code> after a function name</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">Statistics</span>
<span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">abs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">p</span><span class="o">&#39;</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">])</span>
</pre></div>
</div>
<p>Or, using the complicated iterative function we defined for the squareroot,</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">squareroot</span><span class="o">&#39;</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
<p>Zygote supports combinations of vectors and scalars as the function parameters.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">n</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">.^</span><span class="n">n</span><span class="p">))</span><span class="o">^</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span>
<span class="n">gradient</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
<p>The gradients can be very high dimensional.  For example, to do a simple nonlinear optimization problem
with 1 million dimensions, solved in a few seconds.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">Optim</span><span class="p">,</span> <span class="n">LinearAlgebra</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1000000</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">λ</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">obj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">.-</span> <span class="n">y</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">λ</span><span class="o">*</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x_iv</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="k">function</span> <span class="n">g!</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">G</span> <span class="o">.=</span>  <span class="n">obj</span><span class="o">&#39;</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">end</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="n">g!</span><span class="p">,</span> <span class="n">x_iv</span><span class="p">,</span> <span class="n">LBFGS</span><span class="p">())</span> <span class="c"># or ConjugateGradient()</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;minimum = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimum</span><span class="p">)</span><span class="s"> with in &quot;</span><span class="o">*</span>
<span class="s">&quot;</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">iterations</span><span class="p">)</span><span class="s"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Caution: while Zygote is the most exciting reverse-mode AD implementation in Julia, it has many rough edges.</p>
<ul class="simple">
<li><p>If you write a function, take its gradient, and then modify the function, you need to call <code class="docutils literal notranslate"><span class="pre">Zygote.refresh()</span></code> or else the gradient will be out of sync.  This may not apply for Julia 1.3+.</p></li>
<li><p>It provides no features for getting Jacobians, so you would have to ask for each row of the Jacobian separately.  That said, you
probably want to use  <code class="docutils literal notranslate"><span class="pre">ForwardDiff.jl</span></code> for Jacobians if the dimension of the output is similar to the dimension of the input.</p></li>
<li><p>You cannot, in the current release, use mutating functions (e.g. modify a value in an array/etc.) although that feature is in progress.</p></li>
<li><p>Compiling can be very slow for complicated functions.</p></li>
</ul>
</div>
</div>
<div class="section" id="optimization">
<h2><a class="toc-backref" href="#id4"><span class="section-number">10.3. </span>Optimization</a><a class="headerlink" href="#optimization" title="Permalink to this headline">¶</a></h2>
<p>There are a large number of packages intended to be used for optimization in Julia.</p>
<p>Part of the reason for the diversity of options is that Julia makes it possible to efficiently implement a large number of variations on optimization routines.</p>
<p>The other reason is that different types of optimization problems require different algorithms.</p>
<div class="section" id="optim-jl">
<h3><span class="section-number">10.3.1. </span>Optim.jl<a class="headerlink" href="#optim-jl" title="Permalink to this headline">¶</a></h3>
<p>A good pure-Julia solution for the (unconstrained or box-bounded) optimization of
univariate and multivariate function is the <a class="reference external" href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> package.</p>
<p>By default, the algorithms in <code class="docutils literal notranslate"><span class="pre">Optim.jl</span></code> target minimization rather than
maximization, so if a function is called <code class="docutils literal notranslate"><span class="pre">optimize</span></code> it will mean minimization.</p>
<div class="section" id="univariate-functions-on-bounded-intervals">
<h4><span class="section-number">10.3.1.1. </span>Univariate Functions on Bounded Intervals<a class="headerlink" href="#univariate-functions-on-bounded-intervals" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="http://julianlsolvers.github.io/Optim.jl/stable/user/minimization/#minimizing-a-univariate-function-on-a-bounded-interval">Univariate optimization</a>
defaults to a robust hybrid optimization routine called <a class="reference external" href="https://en.wikipedia.org/wiki/Brent%27s_method">Brent’s method</a>.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">Optim</span>
<span class="k">using</span> <span class="n">Optim</span><span class="o">:</span> <span class="n">converged</span><span class="p">,</span> <span class="n">maximum</span><span class="p">,</span> <span class="n">maximizer</span><span class="p">,</span> <span class="n">minimizer</span><span class="p">,</span> <span class="n">iterations</span> <span class="c">#some extra functions</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">x</span><span class="o">-&gt;</span> <span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p>Always check if the results converged, and throw errors otherwise</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">converged</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">||</span> <span class="n">error</span><span class="p">(</span><span class="s">&quot;Failed to converge in </span><span class="si">$</span><span class="p">(</span><span class="n">iterations</span><span class="p">(</span><span class="n">result</span><span class="p">))</span><span class="s"> iterations&quot;</span><span class="p">)</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">minimizer</span>
<span class="n">result</span><span class="o">.</span><span class="n">minimum</span>
</pre></div>
</div>
<p>The first line is a logical OR between <code class="docutils literal notranslate"><span class="pre">converged(result)</span></code> and <code class="docutils literal notranslate"><span class="pre">error(&quot;...&quot;)</span></code>.</p>
<p>If the convergence check passes, the logical sentence is true, and it will proceed to the next line; if not, it will throw the error.</p>
<p>Or to maximize</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">maximize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">converged</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="o">||</span> <span class="n">error</span><span class="p">(</span><span class="s">&quot;Failed to converge in </span><span class="si">$</span><span class="p">(</span><span class="n">iterations</span><span class="p">(</span><span class="n">result</span><span class="p">))</span><span class="s"> iterations&quot;</span><span class="p">)</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="n">maximizer</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">fmax</span> <span class="o">=</span> <span class="n">maximum</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Note:</strong> Notice that we call <code class="docutils literal notranslate"><span class="pre">optimize</span></code> results using <code class="docutils literal notranslate"><span class="pre">result.minimizer</span></code>, and <code class="docutils literal notranslate"><span class="pre">maximize</span></code> results using <code class="docutils literal notranslate"><span class="pre">maximizer(result)</span></code>.</p>
</div>
<div class="section" id="unconstrained-multivariate-optimization">
<h4><span class="section-number">10.3.1.2. </span>Unconstrained Multivariate Optimization<a class="headerlink" href="#unconstrained-multivariate-optimization" title="Permalink to this headline">¶</a></h4>
<p>There are a variety of <a class="reference external" href="http://julianlsolvers.github.io/Optim.jl/stable/user/minimization/#_top">algorithms and options</a> for multivariate optimization.</p>
<p>From the documentation, the simplest version is</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x_iv</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x_iv</span><span class="p">)</span> <span class="c"># i.e. optimize(f, x_iv, NelderMead())</span>
</pre></div>
</div>
<p>The default algorithm in <code class="docutils literal notranslate"><span class="pre">NelderMead</span></code>, which is derivative-free and hence requires many function evaluations.</p>
<p>To change the algorithm type to <a class="reference external" href="http://julianlsolvers.github.io/Optim.jl/stable/algo/lbfgs/">L-BFGS</a></p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x_iv</span><span class="p">,</span> <span class="n">LBFGS</span><span class="p">())</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;minimum = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimum</span><span class="p">)</span><span class="s"> with argmin = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimizer</span><span class="p">)</span><span class="s"> in &quot;</span><span class="o">*</span>
<span class="s">&quot;</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">iterations</span><span class="p">)</span><span class="s"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that this has fewer iterations.</p>
<p>As no derivative was given, it used <a class="reference external" href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a> to approximate the gradient of <code class="docutils literal notranslate"><span class="pre">f(x)</span></code>.</p>
<p>However, since most of the algorithms require derivatives, you will often want to use auto differentiation or pass analytical gradients if possible.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x_iv</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x_iv</span><span class="p">,</span> <span class="n">LBFGS</span><span class="p">(),</span> <span class="n">autodiff</span><span class="o">=:</span><span class="n">forward</span><span class="p">)</span> <span class="c"># i.e. use ForwardDiff.jl</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;minimum = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimum</span><span class="p">)</span><span class="s"> with argmin = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimizer</span><span class="p">)</span><span class="s"> in &quot;</span><span class="o">*</span>
<span class="s">&quot;</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">iterations</span><span class="p">)</span><span class="s"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that we did not need to use <code class="docutils literal notranslate"><span class="pre">ForwardDiff.jl</span></code> directly, as long as our <code class="docutils literal notranslate"><span class="pre">f(x)</span></code> function was written to be generic (see the <a class="reference internal" href="generic_programming.html"><span class="doc">generic programming lecture</span></a> ).</p>
<p>Alternatively, with an analytical gradient</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x_iv</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="k">function</span> <span class="n">g!</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">G</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span> <span class="mf">400.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">G</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">200.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span>
<span class="k">end</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">g!</span><span class="p">,</span> <span class="n">x_iv</span><span class="p">,</span> <span class="n">LBFGS</span><span class="p">())</span> <span class="c"># or ConjugateGradient()</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;minimum = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimum</span><span class="p">)</span><span class="s"> with argmin = </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">minimizer</span><span class="p">)</span><span class="s"> in &quot;</span><span class="o">*</span>
<span class="s">&quot;</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">iterations</span><span class="p">)</span><span class="s"> iterations&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>For derivative-free methods, you can change the algorithm – and have no need to provide a gradient</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="n">x_iv</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x_iv</span><span class="p">,</span> <span class="n">SimulatedAnnealing</span><span class="p">())</span> <span class="c"># or ParticleSwarm() or NelderMead()</span>
</pre></div>
</div>
<p>However, you will note that this did not converge, as stochastic methods typically require many more iterations as a tradeoff for their global-convergence properties.</p>
<p>See the <a class="reference external" href="http://julianlsolvers.github.io/Optim.jl/stable/examples/generated/maxlikenlm/">maximum likelihood</a>
example and the accompanying <a class="reference external" href="https://nbviewer.jupyter.org/github/JuliaNLSolvers/Optim.jl/blob/gh-pages/v0.15.3/examples/generated/maxlikenlm.ipynb">Jupyter notebook</a>.</p>
</div>
</div>
<div class="section" id="jump-jl">
<h3><span class="section-number">10.3.2. </span>JuMP.jl<a class="headerlink" href="#jump-jl" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://github.com/JuliaOpt/JuMP.jl">JuMP.jl</a> package is an ambitious implementation of a modelling language for optimization problems in Julia.</p>
<p>In that sense, it is more like an AMPL (or Pyomo) built on top of the Julia
language with macros, and able to use a variety of different commerical and open source solvers.</p>
<p>If you have a linear, quadratic, conic, mixed-integer linear, etc. problem then this will likely be the ideal “meta-package” for calling various solvers.</p>
<p>For nonlinear problems, the modelling language may make things difficult for complicated functions (as it is not designed to be used as a general-purpose nonlinear optimizer).</p>
<p>See the <a class="reference external" href="http://www.juliaopt.org/JuMP.jl/0.18/quickstart.html">quick start guide</a> for more details on all of the options.</p>
<p>The following is an example of calling a linear objective with a nonlinear constraint (provided by an external function).</p>
<p>Here <code class="docutils literal notranslate"><span class="pre">Ipopt</span></code> stands for <code class="docutils literal notranslate"><span class="pre">Interior</span> <span class="pre">Point</span> <span class="pre">OPTimizer</span></code>, a <a class="reference external" href="https://github.com/JuliaOpt/Ipopt.jl">nonlinear solver</a> in Julia</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">JuMP</span><span class="p">,</span> <span class="n">Ipopt</span>
<span class="c"># solve</span>
<span class="c"># max( x[1] + x[2] )</span>
<span class="c"># st sqrt(x[1]^2 + x[2]^2) &lt;= 1</span>

<span class="k">function</span> <span class="n">squareroot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c"># pretending we don&#39;t know sqrt()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="c"># Initial starting point for Newton’s method</span>
    <span class="k">while</span> <span class="n">abs</span><span class="p">(</span><span class="n">z</span><span class="o">*</span><span class="n">z</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">1e-13</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">-</span> <span class="p">(</span><span class="n">z</span><span class="o">*</span><span class="n">z</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="n">z</span><span class="p">)</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">z</span>
<span class="k">end</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">with_optimizer</span><span class="p">(</span><span class="n">Ipopt</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">))</span>
<span class="c"># need to register user defined functions for AD</span>
<span class="n">JuMP</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="o">:</span><span class="n">squareroot</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">squareroot</span><span class="p">,</span> <span class="n">autodiff</span><span class="o">=</span><span class="kc">true</span><span class="p">)</span>

<span class="nd">@variable</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="o">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">start</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c"># start is the initial condition</span>
<span class="nd">@objective</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">Max</span><span class="p">,</span> <span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nd">@NLconstraint</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">squareroot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="o">+</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span>
<span class="nd">@show</span> <span class="n">JuMP</span><span class="o">.</span><span class="n">optimize!</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p>And this is an example of a quadratic objective</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># solve</span>
<span class="c"># min (1-x)^2 + 100(y-x^2)^2)</span>
<span class="c"># st x + y &gt;= 10</span>

<span class="k">using</span> <span class="n">JuMP</span><span class="p">,</span><span class="n">Ipopt</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">with_optimizer</span><span class="p">(</span><span class="n">Ipopt</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">))</span> <span class="c"># settings for the solver</span>
<span class="nd">@variable</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="nd">@variable</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span>

<span class="nd">@NLobjective</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">Min</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">100</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">x</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span>

<span class="n">JuMP</span><span class="o">.</span><span class="n">optimize!</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;x = &quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&quot; y = &quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c"># adding a (linear) constraint</span>
<span class="nd">@constraint</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">JuMP</span><span class="o">.</span><span class="n">optimize!</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">println</span><span class="p">(</span><span class="s">&quot;x = &quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s">&quot; y = &quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="blackboxoptim-jl">
<h3><span class="section-number">10.3.3. </span>BlackBoxOptim.jl<a class="headerlink" href="#blackboxoptim-jl" title="Permalink to this headline">¶</a></h3>
<p>Another package for doing global optimization without derivatives is <a class="reference external" href="https://github.com/robertfeldt/BlackBoxOptim.jl">BlackBoxOptim.jl</a>.</p>
<p>To see an example from the documentation</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">BlackBoxOptim</span>

<span class="k">function</span> <span class="n">rosenbrock2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">^</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span>
<span class="k">end</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">bboptimize</span><span class="p">(</span><span class="n">rosenbrock2d</span><span class="p">;</span> <span class="n">SearchRange</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">),</span> <span class="n">NumDimensions</span> <span class="o">=</span> <span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
<p>An example for <a class="reference external" href="https://github.com/robertfeldt/BlackBoxOptim.jl/blob/master/examples/rosenbrock_parallel.jl">parallel execution</a> of the objective is provided.</p>
</div>
</div>
<div class="section" id="systems-of-equations-and-least-squares">
<h2><a class="toc-backref" href="#id5"><span class="section-number">10.4. </span>Systems of Equations and Least Squares</a><a class="headerlink" href="#systems-of-equations-and-least-squares" title="Permalink to this headline">¶</a></h2>
<div class="section" id="roots-jl">
<h3><span class="section-number">10.4.1. </span>Roots.jl<a class="headerlink" href="#roots-jl" title="Permalink to this headline">¶</a></h3>
<p>A root of a real function <span class="math notranslate nohighlight">\(f\)</span> on <span class="math notranslate nohighlight">\([a,b]\)</span> is an <span class="math notranslate nohighlight">\(x \in [a, b]\)</span> such that <span class="math notranslate nohighlight">\(f(x)=0\)</span>.</p>
<p>For example, if we plot the function</p>
<div class="math notranslate nohighlight" id="equation-root-f">
<span class="eqno">(10.1)<a class="headerlink" href="#equation-root-f" title="Permalink to this equation">¶</a></span>\[f(x) = \sin(4 (x - 1/4)) + x + x^{20} - 1\]</div>
<p>with <span class="math notranslate nohighlight">\(x \in [0,1]\)</span> we get</p>
<div class="figure align-default">
<img alt="../_images/sine-screenshot-2.png" src="../_images/sine-screenshot-2.png" />
</div>
<p>The unique root is approximately 0.408.</p>
<p>The <a class="reference external" href="https://github.com/JuliaLang/Roots.jl">Roots.jl</a> package offers <code class="docutils literal notranslate"><span class="pre">fzero()</span></code> to find roots</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">Roots</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">x</span><span class="o">^</span><span class="mi">20</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">fzero</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="nlsolve-jl">
<h3><span class="section-number">10.4.2. </span>NLsolve.jl<a class="headerlink" href="#nlsolve-jl" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://github.com/JuliaNLSolvers/NLsolve.jl/">NLsolve.jl</a> package provides functions to solve for multivariate systems of equations and fixed points.</p>
<p>From the documentation, to solve for a system of equations without providing a Jacobian</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">NLsolve</span>

<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">3</span><span class="o">-</span><span class="mi">7</span><span class="p">)</span><span class="o">+</span><span class="mi">18</span>
        <span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="c"># returns an array</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">nlsolve</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span> <span class="mf">0.1</span><span class="p">;</span> <span class="mf">1.2</span><span class="p">])</span>
</pre></div>
</div>
<p>In the above case, the algorithm used finite differences to calculate the Jacobian.</p>
<p>Alternatively, if <code class="docutils literal notranslate"><span class="pre">f(x)</span></code> is written generically, you can use auto-differentiation with a single setting.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">nlsolve</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span> <span class="mf">0.1</span><span class="p">;</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">autodiff</span><span class="o">=:</span><span class="n">forward</span><span class="p">)</span>

<span class="n">println</span><span class="p">(</span><span class="s">&quot;converged=</span><span class="si">$</span><span class="p">(</span><span class="n">NLsolve</span><span class="o">.</span><span class="n">converged</span><span class="p">(</span><span class="n">results</span><span class="p">))</span><span class="s"> at root=</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">zero</span><span class="p">)</span><span class="s"> in &quot;</span><span class="o">*</span>
<span class="s">&quot;</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">iterations</span><span class="p">)</span><span class="s"> iterations and </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">f_calls</span><span class="p">)</span><span class="s"> function calls&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Providing a function which operates inplace (i.e., modifies an argument) may help performance for large systems of equations (and hurt it for small ones).</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span> <span class="n">f!</span><span class="p">(</span><span class="n">F</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c"># modifies the first argument</span>
    <span class="n">F</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">^</span><span class="mi">3</span><span class="o">-</span><span class="mi">7</span><span class="p">)</span><span class="o">+</span><span class="mi">18</span>
    <span class="n">F</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">end</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">nlsolve</span><span class="p">(</span><span class="n">f!</span><span class="p">,</span> <span class="p">[</span> <span class="mf">0.1</span><span class="p">;</span> <span class="mf">1.2</span><span class="p">],</span> <span class="n">autodiff</span><span class="o">=:</span><span class="n">forward</span><span class="p">)</span>

<span class="n">println</span><span class="p">(</span><span class="s">&quot;converged=</span><span class="si">$</span><span class="p">(</span><span class="n">NLsolve</span><span class="o">.</span><span class="n">converged</span><span class="p">(</span><span class="n">results</span><span class="p">))</span><span class="s"> at root=</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">zero</span><span class="p">)</span><span class="s"> in &quot;</span><span class="o">*</span>
<span class="s">&quot;</span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">iterations</span><span class="p">)</span><span class="s"> iterations and </span><span class="si">$</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">f_calls</span><span class="p">)</span><span class="s"> function calls&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="leastsquaresoptim-jl">
<h2><a class="toc-backref" href="#id6"><span class="section-number">10.5. </span>LeastSquaresOptim.jl</a><a class="headerlink" href="#leastsquaresoptim-jl" title="Permalink to this headline">¶</a></h2>
<p>Many optimization problems can be solved using linear or nonlinear least squares.</p>
<p>Let <span class="math notranslate nohighlight">\(x \in R^N\)</span> and <span class="math notranslate nohighlight">\(F(x) : R^N \to R^M\)</span> with <span class="math notranslate nohighlight">\(M \geq N\)</span>, then the nonlinear least squares problem is</p>
<div class="math notranslate nohighlight">
\[
\min_x F(x)^T F(x)
\]</div>
<p>While <span class="math notranslate nohighlight">\(F(x)^T F(x) \to R\)</span>, and hence this problem could technically use any nonlinear optimizer, it is useful to exploit the structure of the problem.</p>
<p>In particular, the Jacobian of <span class="math notranslate nohighlight">\(F(x)\)</span>, can be used to approximate the Hessian of the objective.</p>
<p>As with most nonlinear optimization problems, the benefits will typically become evident only when analytical or automatic differentiation is possible.</p>
<p>If <span class="math notranslate nohighlight">\(M = N\)</span> and we know a root <span class="math notranslate nohighlight">\(F(x^*) = 0\)</span> to the system of equations exists, then NLS is the defacto method for solving large <strong>systems of equations</strong>.</p>
<p>An implementation of NLS is given in <a class="reference external" href="https://github.com/matthieugomez/LeastSquaresOptim.jl">LeastSquaresOptim.jl</a>.</p>
<p>From the documentation</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span> <span class="n">LeastSquaresOptim</span>
<span class="k">function</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)]</span>
<span class="k">end</span>
<span class="n">LeastSquaresOptim</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">rosenbrock</span><span class="p">,</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">Dogleg</span><span class="p">())</span>
</pre></div>
</div>
<p><strong>Note:</strong> Because there is a name clash between <code class="docutils literal notranslate"><span class="pre">Optim.jl</span></code> and this package, to use both we need to qualify the use of the <code class="docutils literal notranslate"><span class="pre">optimize</span></code> function (i.e. <code class="docutils literal notranslate"><span class="pre">LeastSquaresOptim.optimize</span></code>).</p>
<p>Here, by default it will use AD with <code class="docutils literal notranslate"><span class="pre">ForwardDiff.jl</span></code> to calculate the Jacobian,
but you could also provide your own calculation of the Jacobian (analytical or using finite differences) and/or calculate the function inplace.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span> <span class="n">rosenbrock_f!</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span>
<span class="k">end</span>
<span class="n">LeastSquaresOptim</span><span class="o">.</span><span class="n">optimize!</span><span class="p">(</span><span class="n">LeastSquaresProblem</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                                <span class="n">f!</span> <span class="o">=</span> <span class="n">rosenbrock_f!</span><span class="p">,</span> <span class="n">output_length</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>

<span class="c"># if you want to use gradient</span>
<span class="k">function</span> <span class="n">rosenbrock_g!</span><span class="p">(</span><span class="n">J</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">J</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">J</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">J</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">200</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">J</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">end</span>
<span class="n">LeastSquaresOptim</span><span class="o">.</span><span class="n">optimize!</span><span class="p">(</span><span class="n">LeastSquaresProblem</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
                                <span class="n">f!</span> <span class="o">=</span> <span class="n">rosenbrock_f!</span><span class="p">,</span> <span class="n">g!</span> <span class="o">=</span> <span class="n">rosenbrock_g!</span><span class="p">,</span> <span class="n">output_length</span> <span class="o">=</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="additional-notes">
<h2><a class="toc-backref" href="#id7"><span class="section-number">10.6. </span>Additional Notes</a><a class="headerlink" href="#additional-notes" title="Permalink to this headline">¶</a></h2>
<p>Watch <a class="reference external" href="https://www.youtube.com/watch?v=vAp6nUMrKYg&amp;feature=youtu.be">this video</a> from one of Julia’s creators on automatic differentiation.</p>
</div>
<div class="section" id="exercises">
<h2><a class="toc-backref" href="#id8"><span class="section-number">10.7. </span>Exercises</a><a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="exercise-1">
<h3><span class="section-number">10.7.1. </span>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¶</a></h3>
<p>Doing a simple implementation of forward-mode auto-differentiation is very easy in Julia since it is generic.  In this exercise, you
will fill in a few of the operations required for a simple AD implementation.</p>
<p>First, we need to provide a type to hold the dual.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">DualNumber</span><span class="p">{</span><span class="n">T</span><span class="p">}</span> <span class="o">&lt;:</span> <span class="kt">Real</span>
    <span class="n">val</span><span class="o">::</span><span class="n">T</span>
    <span class="n">ϵ</span><span class="o">::</span><span class="n">T</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Here we have made it a subtype of <code class="docutils literal notranslate"><span class="pre">Real</span></code> so that it can pass through functions expecting Reals.</p>
<p>We can add on a variety of chain rule definitions by importing in the appropriate functions and adding DualNumber versions.  For example</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">import</span> <span class="n">Base</span><span class="o">:</span> <span class="o">+</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="o">-</span><span class="p">,</span> <span class="o">^</span><span class="p">,</span> <span class="n">exp</span>
<span class="o">+</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="n">DualNumber</span><span class="p">,</span> <span class="n">y</span><span class="o">::</span><span class="n">DualNumber</span><span class="p">)</span> <span class="o">=</span> <span class="n">DualNumber</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">val</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ϵ</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">ϵ</span><span class="p">)</span>  <span class="c"># dual addition</span>
<span class="o">+</span><span class="p">(</span><span class="n">x</span><span class="o">::</span><span class="n">DualNumber</span><span class="p">,</span> <span class="n">a</span><span class="o">::</span><span class="kt">Number</span><span class="p">)</span> <span class="o">=</span> <span class="n">DualNumber</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="o">+</span> <span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ϵ</span><span class="p">)</span>  <span class="c"># i.e. scalar addition, not dual</span>
<span class="o">+</span><span class="p">(</span><span class="n">a</span><span class="o">::</span><span class="kt">Number</span><span class="p">,</span> <span class="n">x</span><span class="o">::</span><span class="n">DualNumber</span><span class="p">)</span> <span class="o">=</span> <span class="n">DualNumber</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">val</span> <span class="o">+</span> <span class="n">a</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">ϵ</span><span class="p">)</span>  <span class="c"># i.e. scalar addition, not dual</span>
</pre></div>
</div>
<p>With that, we can seed a dual number and find simple derivatives,</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">+</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">DualNumber</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>  <span class="c"># x -&gt; 2.0 + 1.0\epsilon</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">DualNumber</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>  <span class="c"># i.e. y = 3.0, no derivative</span>


<span class="c"># seeded calculates both teh function and the d/dx gradient!</span>
<span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>For this assignment:</p>
<ol class="simple">
<li><p>Add in AD rules for the other operations: <code class="docutils literal notranslate"><span class="pre">*,</span> <span class="pre">-,</span> <span class="pre">^,</span> <span class="pre">exp</span></code>.</p></li>
<li><p>Come up with some examples of univariate and multivariate functions combining those operations and use your AD implementation to find the derivatives.</p></li>
</ol>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./more_julia"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            
            <div class="sidebar bd-sidebar inactive" id="site-navigation">

                <div class="sidebar__header">


                    Contents

                </div>

                <nav class="sidebar__nav" id="sidebar-nav" aria-label="Main navigation">
                    <p class="caption">
 <span class="caption-text">
  Getting Started with Julia
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/getting_started.html">
   1. Setting up Your Julia Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/julia_environment.html">
   2. Interacting with Julia
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/julia_by_example.html">
   3. Introductory Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/julia_essentials.html">
   4. Julia Essentials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/fundamental_types.html">
   5. Arrays, Tuples, Ranges, and Other Fundamental Types
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/introduction_to_types.html">
   6. Introduction to Types and Generic Programming
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Packages and Software Engineering in Julia
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="generic_programming.html">
   7. Generic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="general_packages.html">
   8. General Purpose Packages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_statistical_packages.html">
   9. Data and Statistics Packages
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   10. Solvers, Optimizers, and Automatic Differentiation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tools_editors.html">
   11. Julia Tools and Editors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="version_control.html">
   12. Git, GitHub, and Version Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="testing.html">
   13. Packages, Testing, and Continuous Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="need_for_speed.html">
   14. The Need for Speed
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/linear_algebra.html">
   15. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/orth_proj.html">
   16. Orthogonal Projections and Their Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/lln_clt.html">
   17. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/linear_models.html">
   18. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/finite_markov.html">
   19. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/stationary_densities.html">
   20. Continuous State Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/kalman.html">
   22. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/numerical_linear_algebra.html">
   23. Numerical Linear Algebra and Factorizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../tools_and_techniques/iterative_methods_sparsity.html">
   24. Krylov Methods and Matrix Conditioning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/short_path.html">
   25. Shortest Paths
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/mccall_model.html">
   26. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/mccall_model_with_separation.html">
   27. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/wald_friedman.html">
   28. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/odu.html">
   29. Job Search III: Search with Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/career.html">
   30. Job Search IV: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/jv.html">
   31. Job Search V: On-the-Job Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/optgrowth.html">
   32. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/coleman_policy_iter.html">
   33. Optimal Growth II: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/egm_policy_iter.html">
   34. Optimal Growth III: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/lqcontrol.html">
   35. LQ Dynamic Programming Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/perm_income.html">
   36. Optimal Savings I: The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/perm_income_cons.html">
   37. Optimal Savings II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/smoothing.html">
   38. Consumption and Tax Smoothing with Complete and Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/ifp.html">
   39. Optimal Savings III: Occasionally Binding Constraints
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/robustness.html">
   40. Robustness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/discrete_dp.html">
   41. Discrete State Dynamic Programming
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Modeling in Continuous Time
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../continuous_time/seir_model.html">
   42. Modeling COVID 19 with Differential Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../continuous_time/covid_sde.html">
   43. Modeling Shocks in COVID 19 with Stochastic Differential Equations
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/schelling.html">
   44. Schelling’s Segregation Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/lake_model.html">
   45. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/rational_expectations.html">
   46. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/markov_perf.html">
   47. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/markov_asset.html">
   48. Asset Pricing I: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/lucas_model.html">
   49. Asset Pricing II: The Lucas Asset Pricing Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/harrison_kreps.html">
   50. Asset Pricing III:  Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/uncertainty_traps.html">
   51. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/aiyagari.html">
   52. The Aiyagari Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/arellano.html">
   53. Default Risk and Income Fluctuations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/matsuyama.html">
   54. Globalization and Cycles
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Time Series Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/arma.html">
   55. Covariance Stationary Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/estspec.html">
   56. Estimation of Spectra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/additive_functionals.html">
   57. Additive Functionals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/multiplicative_functionals.html">
   58. Multiplicative Functionals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/lu_tricks.html">
   59. Classical Control with Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/classical_filtering.html">
   60. Classical Filtering With Linear Algebra
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Dynamic Programming Squared
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/dyn_stack.html">
   61. Dynamic Stackelberg Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/lqramsey.html">
   62. Optimal Taxation in an LQ Economy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/opt_tax_recur.html">
   63. Optimal Taxation with State-Contingent Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/amss.html">
   64. Optimal Taxation without State-Contingent Debt
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_lectures.html">
   65. About these Lectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../troubleshooting.html">
   66. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   67. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../status.html">
   68. Lecture Status
  </a>
 </li>
</ul>

                </nav>

                <div class="sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="toolbar">

            <div class="toolbar__inner">

                <ul class="toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="../intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                    <!-- <li class="btn__search">
                        <form action="../search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="_notebooks/more_julia/optimization_solver_packages.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li data-tippy-content="Launch Notebook" id="launchButton"><a href="https://mybinder.org/v2/gh/QuantEcon/lecture-julia.notebooks/master?urlpath=tree/more_julia/optimization_solver_packages.ipynb" target="_blank"><i data-feather="play-circle"></i></a></li>
                        <li data-tippy-content="Download PDF" onClick="window.print()"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-julia.myst/tree/master/lectures/more_julia/optimization_solver_packages.md" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="" download><p style="color: white;">Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title">QuantEcon Notebook Launcher</p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input" onchange="onChangeListener()">
                
                    <option value="https://mybinder.org/v2/gh/QuantEcon/lecture-julia.notebooks/master?urlpath=tree/more_julia/optimization_solver_packages.ipynb">BinderHub</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <!-- <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" onchange="onChangeListener()">
                <i class="fas fa-check-circle"></i>
            </li> -->
            </ul>
        </div>

    </div> <!-- .wrapper-->
  </body>
</html>