{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4157adaa",
   "metadata": {},
   "source": [
    "(discrete_dp)=\n",
    "```{raw} html\n",
    "<div id=\"qe-notebook-header\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# {index}`Discrete State Dynamic Programming <single: Discrete State Dynamic Programming>`\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture we discuss a family of dynamic programming problems with the following features:\n",
    "\n",
    "1. a discrete state space and discrete choices (actions)\n",
    "1. an infinite horizon\n",
    "1. discounted rewards\n",
    "1. Markov state transitions\n",
    "\n",
    "We call such problems discrete dynamic programs, or discrete DPs.\n",
    "\n",
    "Discrete DPs are the workhorses in much of modern quantitative economics, including\n",
    "\n",
    "* monetary economics\n",
    "* search and labor economics\n",
    "* household savings and consumption theory\n",
    "* investment theory\n",
    "* asset pricing\n",
    "* industrial organization, etc.\n",
    "\n",
    "When a given model is not inherently discrete, it is common to replace it with a discretized version in order to use discrete DP techniques.\n",
    "\n",
    "This lecture covers\n",
    "\n",
    "* the theory of dynamic programming in a discrete setting, plus examples and\n",
    "  applications\n",
    "* a powerful set of routines for solving discrete DPs from the [QuantEcon code libary](http://quantecon.org/quantecon-jl)\n",
    "\n",
    "### How to Read this Lecture\n",
    "\n",
    "We use dynamic programming many applied lectures, such as\n",
    "\n",
    "* The {doc}`shortest path lecture <../dynamic_programming/short_path>`\n",
    "* The {doc}`McCall search model lecture <../dynamic_programming/mccall_model>`\n",
    "* The {doc}`optimal growth lecture <../dynamic_programming/optgrowth>`\n",
    "\n",
    "The objective of this lecture is to provide a more systematic and theoretical treatment, including algorithms and implementation, while focusing on the discrete case.\n",
    "\n",
    "### References\n",
    "\n",
    "For background reading on dynamic programming and additional applications, see, for example,\n",
    "\n",
    "* {cite}`Ljungqvist2012`\n",
    "* {cite}`HernandezLermaLasserre1996`, section 3.5\n",
    "* {cite}`puterman2005`\n",
    "* {cite}`StokeyLucas1989`\n",
    "* {cite}`Rust1996`\n",
    "* {cite}`MirandaFackler2002`\n",
    "* [EDTC](http://johnstachurski.net/edtc.html), chapter 5\n",
    "\n",
    "(discrete_dps)=\n",
    "## Discrete DPs\n",
    "\n",
    "Loosely speaking, a discrete DP is a maximization problem with an objective\n",
    "function of the form\n",
    "\n",
    "```{math}\n",
    ":label: dp_objective\n",
    "\n",
    "\\mathbb{E}\n",
    "\\sum_{t = 0}^{\\infty} \\beta^t r(s_t, a_t)\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "* $s_t$ is the state variable\n",
    "* $a_t$ is the action\n",
    "* $\\beta$ is a discount factor\n",
    "* $r(s_t, a_t)$ is interpreted as a current reward when the state is $s_t$ and the action chosen is $a_t$\n",
    "\n",
    "Each pair $(s_t, a_t)$ pins down transition probabilities $Q(s_t, a_t, s_{t+1})$ for the next period state $s_{t+1}$.\n",
    "\n",
    "Thus, actions influence not only current rewards but also the future time path of the state.\n",
    "\n",
    "The essence of dynamic programming problems is to trade off current rewards\n",
    "vs favorable positioning of the future state (modulo randomness).\n",
    "\n",
    "Examples:\n",
    "\n",
    "* consuming today vs saving and accumulating assets\n",
    "* accepting a job offer today vs seeking a better one in the future\n",
    "* exercising an option now vs waiting\n",
    "\n",
    "### Policies\n",
    "\n",
    "The most fruitful way to think about solutions to discrete DP problems is to compare *policies*.\n",
    "\n",
    "In general, a policy is a randomized map from past actions and states to\n",
    "current action.\n",
    "\n",
    "In the setting formalized below, it suffices to consider so-called *stationary Markov policies*, which consider only the current state.\n",
    "\n",
    "In particular, a stationary Markov policy is a map $\\sigma$ from states to actions\n",
    "\n",
    "* $a_t = \\sigma(s_t)$ indicates that $a_t$ is the action to be taken in state $s_t$\n",
    "\n",
    "It is known that, for any arbitrary policy, there exists a stationary Markov policy that dominates it at least weakly.\n",
    "\n",
    "* See section 5.5 of {cite}`puterman2005` for discussion and proofs.\n",
    "\n",
    "In what follows, stationary Markov policies are referred to simply as policies.\n",
    "\n",
    "The aim is to find an optimal policy, in the sense of one that maximizes {eq}`dp_objective`.\n",
    "\n",
    "Let's now step through these ideas more carefully.\n",
    "\n",
    "### Formal definition\n",
    "\n",
    "Formally, a discrete dynamic program consists of the following components:\n",
    "\n",
    "1. A finite set of *states* $S = \\{0, \\ldots, n-1\\}$\n",
    "1. A finite set of *feasible actions* $A(s)$ for each state $s \\in S$, and a corresponding set of *feasible state-action pairs*\n",
    "   \n",
    "   $$\n",
    "   \\mathit{SA} := \\{(s, a) \\mid s \\in S, \\; a \\in A(s)\\}\n",
    "   $$\n",
    "   \n",
    "1. A *reward function* $r\\colon \\mathit{SA} \\to \\mathbb{R}$\n",
    "1. A *transition probability function* $Q\\colon \\mathit{SA} \\to \\Delta(S)$, where $\\Delta(S)$ is the set of probability distributions over $S$\n",
    "1. A *discount factor* $\\beta \\in [0, 1)$\n",
    "\n",
    "We also use the notation $A := \\bigcup_{s \\in S} A(s) = \\{0, \\ldots, m-1\\}$ and call this set the *action space*.\n",
    "\n",
    "A *policy* is a function $\\sigma\\colon S \\to A$.\n",
    "\n",
    "A policy is called *feasible* if it satisfies $\\sigma(s) \\in A(s)$ for all $s \\in S$.\n",
    "\n",
    "Denote the set of all feasible policies by $\\Sigma$.\n",
    "\n",
    "If a decision maker uses  a policy $\\sigma \\in \\Sigma$, then\n",
    "\n",
    "* the current reward at time $t$ is $r(s_t, \\sigma(s_t))$\n",
    "* the probability that $s_{t+1} = s'$ is $Q(s_t, \\sigma(s_t), s')$\n",
    "\n",
    "For each $\\sigma \\in \\Sigma$, define\n",
    "\n",
    "* $r_{\\sigma}$ by $r_{\\sigma}(s) := r(s, \\sigma(s))$)\n",
    "* $Q_{\\sigma}$ by $Q_{\\sigma}(s, s') := Q(s, \\sigma(s), s')$\n",
    "\n",
    "Notice that $Q_\\sigma$ is a {ref}`stochastic matrix <finite_dp_stoch_mat>` on $S$.\n",
    "\n",
    "It gives transition probabilities of the *controlled chain* when we follow policy $\\sigma$.\n",
    "\n",
    "If we think of $r_\\sigma$ as a column vector, then so is $Q_\\sigma^t r_\\sigma$, and the $s$-th row of the latter has the interpretation\n",
    "\n",
    "```{math}\n",
    ":label: ddp_expec\n",
    "\n",
    "(Q_\\sigma^t r_\\sigma)(s) = \\mathbb E [ r(s_t, \\sigma(s_t)) \\mid s_0 = s ]\n",
    "\\quad \\text{when } \\{s_t\\} \\sim Q_\\sigma\n",
    "```\n",
    "\n",
    "Comments\n",
    "\n",
    "* $\\{s_t\\} \\sim Q_\\sigma$ means that the state is generated by stochastic matrix $Q_\\sigma$\n",
    "* See {ref}`this discussion <finite_mc_expec>` on computing expectations of Markov chains for an explanation of the expression in {eq}`ddp_expec`\n",
    "\n",
    "Notice that we're not really distinguishing between functions from $S$ to $\\mathbb R$ and vectors in $\\mathbb R^n$.\n",
    "\n",
    "This is natural because they are in one to one correspondence.\n",
    "\n",
    "### Value and Optimality\n",
    "\n",
    "Let $v_{\\sigma}(s)$ denote the discounted sum of expected reward flows from policy $\\sigma$\n",
    "when the initial state is $s$.\n",
    "\n",
    "To calculate this quantity we pass the expectation through the sum in\n",
    "{eq}`dp_objective` and use {eq}`ddp_expec` to get\n",
    "\n",
    "$$\n",
    "v_{\\sigma}(s) = \\sum_{t=0}^{\\infty} \\beta^t (Q_{\\sigma}^t r_{\\sigma})(s)\n",
    "\\qquad (s \\in S)\n",
    "$$\n",
    "\n",
    "This function is called the *policy value function* for the policy $\\sigma$.\n",
    "\n",
    "The *optimal value function*, or simply *value function*, is the function $v^*\\colon S \\to \\mathbb{R}$ defined by\n",
    "\n",
    "$$\n",
    "v^*(s) = \\max_{\\sigma \\in \\Sigma} v_{\\sigma}(s)\n",
    "\\qquad (s \\in S)\n",
    "$$\n",
    "\n",
    "(We can use max rather than sup here because the domain is a finite set)\n",
    "\n",
    "A policy $\\sigma \\in \\Sigma$ is called *optimal* if $v_{\\sigma}(s) = v^*(s)$ for all $s \\in S$.\n",
    "\n",
    "Given any $w \\colon S \\to \\mathbb R$, a policy $\\sigma \\in \\Sigma$ is called $w$-greedy if\n",
    "\n",
    "$$\n",
    "\\sigma(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n",
    "\\left\\{\n",
    "    r(s, a) +\n",
    "    \\beta \\sum_{s' \\in S} w(s') Q(s, a, s')\n",
    "\\right\\}\n",
    "\\qquad (s \\in S)\n",
    "$$\n",
    "\n",
    "As discussed in detail below, optimal policies are precisely those that are $v^*$-greedy.\n",
    "\n",
    "### Two Operators\n",
    "\n",
    "It is useful to define the following operators:\n",
    "\n",
    "- The *Bellman operator* $T\\colon \\mathbb{R}^S \\to \\mathbb{R}^S$\n",
    "  is defined by\n",
    "\n",
    "$$\n",
    "(T v)(s) = \\max_{a \\in A(s)}\n",
    "\\left\\{\n",
    "    r(s, a) + \\beta \\sum_{s' \\in S} v(s') Q(s, a, s')\n",
    "\\right\\}\n",
    "\\qquad (s \\in S)\n",
    "$$\n",
    "\n",
    "- For any policy function $\\sigma \\in \\Sigma$, the operator $T_{\\sigma}\\colon \\mathbb{R}^S \\to \\mathbb{R}^S$ is defined by\n",
    "\n",
    "$$\n",
    "(T_{\\sigma} v)(s) = r(s, \\sigma(s)) +\n",
    "    \\beta \\sum_{s' \\in S} v(s') Q(s, \\sigma(s), s')\n",
    "\\qquad (s \\in S)\n",
    "$$\n",
    "\n",
    "This can be written more succinctly in operator notation as\n",
    "\n",
    "$$\n",
    "T_{\\sigma} v = r_{\\sigma} + \\beta Q_{\\sigma} v\n",
    "$$\n",
    "\n",
    "The two operators are both monotone\n",
    "\n",
    "* $v \\leq w$  implies $Tv \\leq Tw$ pointwise on $S$, and\n",
    "  similarly for $T_\\sigma$\n",
    "\n",
    "They are also contraction mappings with modulus $\\beta$\n",
    "\n",
    "* $\\lVert Tv - Tw \\rVert \\leq \\beta \\lVert v - w \\rVert$ and similarly for $T_\\sigma$, where $\\lVert \\cdot\\rVert$ is the max norm\n",
    "\n",
    "For any policy $\\sigma$, its value $v_{\\sigma}$ is the unique fixed point of $T_{\\sigma}$.\n",
    "\n",
    "For proofs of these results and those in the next section, see, for example, [EDTC](http://johnstachurski.net/edtc.html), chapter 10.\n",
    "\n",
    "### The Bellman Equation and the Principle of Optimality\n",
    "\n",
    "The main principle of the theory of dynamic programming is that\n",
    "\n",
    "- the optimal value function $v^*$ is a unique solution to the *Bellman equation*,\n",
    "  \n",
    "  $$\n",
    "  v(s) = \\max_{a \\in A(s)} \\left\\{ r(s, a) + \\beta \\sum_{s' \\in S} v(s') Q(s, a, s') \\right\\} \\qquad (s \\in S),\n",
    "  $$\n",
    "  or in other words, $v^*$ is the unique fixed point of $T$, and\n",
    "- $\\sigma^*$ is an optimal policy function if and only if it is $v^*$-greedy\n",
    "\n",
    "By the definition of greedy policies given above, this means that\n",
    "\n",
    "$$\n",
    "\\sigma^*(s) \\in \\operatorname*{arg\\,max}_{a \\in A(s)}\n",
    "    \\left\\{\n",
    "    r(s, a) + \\beta \\sum_{s' \\in S} v^*(s') Q(s, \\sigma(s), s')\n",
    "    \\right\\}\n",
    "\\qquad (s \\in S)\n",
    "$$\n",
    "\n",
    "## Solving Discrete DPs\n",
    "\n",
    "Now that the theory has been set out, let's turn to solution methods.\n",
    "\n",
    "Code for solving discrete DPs is available in [ddp.jl](https://github.com/QuantEcon/QuantEcon.jl/blob/master/src/markov/ddp.jl) from the [QuantEcon.jl](http://quantecon.org/quantecon-jl) code library.\n",
    "\n",
    "It implements the three most important solution methods for discrete dynamic programs, namely\n",
    "\n",
    "- value function iteration\n",
    "- policy function iteration\n",
    "- modified policy function iteration\n",
    "\n",
    "Let's briefly review these algorithms and their implementation.\n",
    "\n",
    "### Value Function Iteration\n",
    "\n",
    "Perhaps the most familiar method for solving all manner of dynamic programs is value function iteration.\n",
    "\n",
    "This algorithm uses the fact that the Bellman operator $T$ is a contraction mapping with fixed point $v^*$.\n",
    "\n",
    "Hence, iterative application of $T$ to any initial function $v^0 \\colon S \\to \\mathbb R$ converges to $v^*$.\n",
    "\n",
    "The details of the algorithm can be found in {ref}`the appendix <ddp_algorithms>`.\n",
    "\n",
    "### Policy Function Iteration\n",
    "\n",
    "This routine, also known as Howard's policy improvement algorithm, exploits more closely the particular structure of a discrete DP problem.\n",
    "\n",
    "Each iteration consists of\n",
    "\n",
    "1. A policy evaluation step that computes the value $v_{\\sigma}$ of a policy $\\sigma$ by solving the linear equation $v = T_{\\sigma} v$.\n",
    "1. A policy improvement step that computes a $v_{\\sigma}$-greedy policy.\n",
    "\n",
    "In the current setting policy iteration computes an exact optimal policy in finitely many iterations.\n",
    "\n",
    "* See theorem 10.2.6 of [EDTC](http://johnstachurski.net/edtc.html) for a proof\n",
    "\n",
    "The details of the algorithm can be found in {ref}`the appendix <ddp_algorithms>`.\n",
    "\n",
    "### Modified Policy Function Iteration\n",
    "\n",
    "Modified policy iteration replaces the policy evaluation step in policy iteration with \"partial policy evaluation\".\n",
    "\n",
    "The latter computes an approximation to the value of a policy $\\sigma$ by iterating $T_{\\sigma}$ for a specified number of times.\n",
    "\n",
    "This approach can be useful when the state space is very large and the linear system in the policy evaluation step of policy iteration is correspondingly difficult to solve.\n",
    "\n",
    "The details of the algorithm can be found in {ref}`the appendix <ddp_algorithms>`.\n",
    "\n",
    "(ddp_eg_gm)=\n",
    "## Example: A Growth Model\n",
    "\n",
    "Let's consider a simple consumption-saving model.\n",
    "\n",
    "A single household either consumes or stores its own output of a single consumption good.\n",
    "\n",
    "The household starts each period with current stock $s$.\n",
    "\n",
    "Next, the household chooses a quantity $a$ to store and consumes $c = s - a$\n",
    "\n",
    "* Storage is limited by a global upper bound $M$\n",
    "* Flow utility is $u(c) = c^{\\alpha}$\n",
    "\n",
    "Output is drawn from a discrete uniform distribution on $\\{0, \\ldots, B\\}$.\n",
    "\n",
    "The next period stock is therefore\n",
    "\n",
    "$$\n",
    "s' = a + U\n",
    "\\quad \\text{where} \\quad\n",
    "U \\sim U[0, \\ldots, B]\n",
    "$$\n",
    "\n",
    "The discount factor is $\\beta \\in [0, 1)$.\n",
    "\n",
    "### Discrete DP Representation\n",
    "\n",
    "We want to represent this model in the format of a discrete dynamic program.\n",
    "\n",
    "To this end, we take\n",
    "\n",
    "* the state variable to be the stock $s$\n",
    "* the state space to be $S = \\{0, \\ldots, M + B\\}$\n",
    "    * hence $n = M + B + 1$\n",
    "* the action to be the storage quantity $a$\n",
    "* the set of feasible actions at $s$ to be $A(s) = \\{0, \\ldots, \\min\\{s, M\\}\\}$\n",
    "    * hence $A = \\{0, \\ldots, M\\}$ and $m = M + 1$\n",
    "* the reward function to be $r(s, a) = u(s - a)$\n",
    "* the transition probabilities to be\n",
    "\n",
    "```{math}\n",
    ":label: ddp_def_ogq\n",
    "\n",
    "Q(s, a, s')\n",
    ":=\n",
    "\\begin{cases}\n",
    "    \\frac{1}{B + 1} & \\text{if } a \\leq s' \\leq a + B\n",
    "    \\\\\n",
    "     0 & \\text{ otherwise}\n",
    "\\end{cases}\n",
    "```\n",
    "\n",
    "### Defining a DiscreteDP Instance\n",
    "\n",
    "This information will be used to create an instance of DiscreteDP by passing\n",
    "the following information\n",
    "\n",
    "1. An $n \\times m$ reward array $R$\n",
    "1. An $n \\times m \\times n$ transition probability array $Q$\n",
    "1. A discount factor $\\beta$\n",
    "\n",
    "For $R$ we set $R[s, a] = u(s - a)$ if $a \\leq s$ and $-\\infty$ otherwise.\n",
    "\n",
    "For $Q$ we follow the rule in {eq}`ddp_def_ogq`.\n",
    "\n",
    "Note:\n",
    "\n",
    "* The feasibility constraint is embedded into $R$ by setting $R[s, a] = -\\infty$ for $a \\notin A(s)$.\n",
    "* Probability distributions for $(s, a)$ with $a \\notin A(s)$ can be arbitrary.\n",
    "\n",
    "The following code sets up these objects for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b87a31b",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics, BenchmarkTools, Plots, QuantEcon\n",
    "using SparseArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940555d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "using Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d955e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools, Plots, QuantEcon, Parameters\n",
    "gr(fmt = :png);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleOG = @with_kw (B = 10, M = 5, α = 0.5, β = 0.9)\n",
    "\n",
    "function transition_matrices(g)\n",
    "    @unpack B, M, α, β = g\n",
    "    u(c) = c^α\n",
    "    n = B + M + 1\n",
    "    m = M + 1\n",
    "\n",
    "    R = zeros(n, m)\n",
    "    Q = zeros(n, m, n)\n",
    "\n",
    "    for a in 0:M\n",
    "        Q[:, a + 1, (a:(a + B)) .+ 1] .= 1 / (B + 1)\n",
    "        for s in 0:(B + M)\n",
    "            R[s + 1, a + 1] = (a≤s ? u(s - a) : -Inf)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (Q = Q, R = R)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94ce43",
   "metadata": {},
   "source": [
    "Let's run this code and create an instance of `SimpleOG`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = SimpleOG();\n",
    "Q, R = transition_matrices(g);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668c37d",
   "metadata": {},
   "source": [
    "In case the preceding code was too concise, we can see a more verbose form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe826c60",
   "metadata": {
    "tags": [
     "output_scroll"
    ]
   },
   "outputs": [],
   "source": [
    "function verbose_matrices(g)\n",
    "    @unpack B, M, α, β = g\n",
    "    u(c) = c^α\n",
    "\n",
    "    #Matrix dimensions. The +1 is due to the 0 state.\n",
    "    n = B + M + 1\n",
    "    m = M + 1\n",
    "\n",
    "    R = fill(-Inf, n, m) #Start assuming nothing is feasible\n",
    "    Q = zeros(n,m,n) #Assume 0 by default\n",
    "\n",
    "    #Create the R matrix\n",
    "    #Note: indexing into matrix complicated since Julia starts indexing at 1 instead of 0\n",
    "    #but the state s and choice a can be 0\n",
    "    for a in 0:M\n",
    "         for s in 0:(B + M)\n",
    "            if a <= s #i.e. if feasible\n",
    "                R[s + 1, a + 1] = u(s - a)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #Create the Q multi-array\n",
    "    for s in 0:(B+M) #For each state\n",
    "        for a in 0:M #For each action\n",
    "            for sp in 0:(B+M) #For each state next period\n",
    "                if( sp >= a && sp <= a + B) # The support of all realizations\n",
    "                    Q[s + 1, a + 1, sp + 1] = 1 / (B + 1) # Same prob of all\n",
    "                end\n",
    "            end\n",
    "            @assert sum(Q[s + 1, a + 1, :]) ≈ 1 #Optional check that matrix is stochastic\n",
    "         end\n",
    "    end\n",
    "    return (Q = Q, R = R)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303b57d",
   "metadata": {},
   "source": [
    "Instances of `DiscreteDP` are created using the signature `DiscreteDP(R, Q, β)`.\n",
    "\n",
    "Let's create an instance using the objects stored in `g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a3bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp = DiscreteDP(R, Q, g.β);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2760cb0f",
   "metadata": {},
   "source": [
    "Now that we have an instance `ddp` of `DiscreteDP` we can solve it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = solve(ddp, PFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfadbc7",
   "metadata": {},
   "source": [
    "Let's see what we've got here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(typeof(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482ab3b",
   "metadata": {},
   "source": [
    "The most important attributes are `v`, the value function, and `σ`, the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e8711",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Value Function Tests\" begin\n",
    "    @test results.v[2] ≈ 20.017402216959912\n",
    "    @test results.v[4] ≈ 20.749453024528794\n",
    "    @test results.v[end] ≈ 23.277617618874903 # Also an implicit length check\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c438bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sigma .- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0350498",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Optimal Policy Tests\" begin\n",
    "    @test results.sigma .- 1 == [0, 0, 0, 0, 1, 1, 1, 2, 2, 3, 3, 4, 5, 5, 5, 5]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69facbf",
   "metadata": {},
   "source": [
    "Here 1 is subtracted from results.sigma because we added 1 to each state and action to create valid indices.\n",
    "\n",
    "Since we've used policy iteration, these results will be exact unless we hit the iteration bound `max_iter`.\n",
    "\n",
    "Let's make sure this didn't happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7c1bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228bf7d",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Iteration Tests\" begin\n",
    "    @test results.num_iter ≤ 3 # Make sure we didn't take more cycles, compared to v0.6\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b5afc1",
   "metadata": {},
   "source": [
    "In this case we converged in only 3 iterations.\n",
    "\n",
    "Another interesting object is `results.mc`, which is the controlled chain defined by $Q_{\\sigma^*}$, where $\\sigma^*$ is the optimal policy.\n",
    "\n",
    "In other words, it gives the dynamics of the state when the agent follows the optimal policy.\n",
    "\n",
    "Since this object is an instance of MarkovChain from  [QuantEcon.jl](http://quantecon.org/quantecon-jl) (see {doc}`this lecture <../tools_and_techniques/finite_markov>` for more discussion), we\n",
    "can easily simulate it, compute its stationary distribution and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ccac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_distributions(results.mc)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf06b5",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Stationary Distributions Test\" begin\n",
    "    @test stationary_distributions(results.mc)[1][10] ≈ 0.09090909090909091\n",
    "    @test stationary_distributions(results.mc)[1][14] ≈ 0.033169533169533166\n",
    "    # Only one element of this `mc` field.\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5dcd82",
   "metadata": {},
   "source": [
    "Here's the same information in a bar graph\n",
    "\n",
    "```{figure} /_static/figures/finite_dp_simple_og.png\n",
    ":width: 80%\n",
    "```\n",
    "\n",
    "What happens if the agent is more patient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c107a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_2 = SimpleOG(β=0.99);\n",
    "Q_2, R_2 = transition_matrices(g_2);\n",
    "\n",
    "ddp_2 = DiscreteDP(R_2, Q_2, g_2.β)\n",
    "\n",
    "results_2 = solve(ddp_2, PFI)\n",
    "\n",
    "std_2 = stationary_distributions(results_2.mc)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac347d47",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Patience Shock Tests\" begin\n",
    "    @test std_2[3] ≈ 0.03147788040836169\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23778ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar(std_2, label = \"stationary dist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb7eaa",
   "metadata": {},
   "source": [
    "We can see the rightward shift in probability mass.\n",
    "\n",
    "### State-Action Pair Formulation\n",
    "\n",
    "The `DiscreteDP` type in fact provides a second interface to setting up an instance.\n",
    "\n",
    "One of the advantages of this alternative set up is that it permits use of a sparse matrix for `Q`.\n",
    "\n",
    "(An example of using sparse matrices is given in the exercises below)\n",
    "\n",
    "The call signature of the second formulation is `DiscreteDP(R, Q, β, s_indices, a_indices)` where\n",
    "\n",
    "* `s_indices` and `a_indices` are arrays of equal length `L` enumerating all feasible state-action pairs\n",
    "* `R` is an array of length `L` giving corresponding rewards\n",
    "* `Q` is an `L x n` transition probability array\n",
    "\n",
    "Here's how we could set up these objects for the preceding example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 10\n",
    "M = 5\n",
    "α = 0.5\n",
    "β = 0.9\n",
    "u(c) = c^α\n",
    "n = B + M + 1\n",
    "m = M + 1\n",
    "\n",
    "s_indices = Int64[]\n",
    "a_indices = Int64[]\n",
    "Q = zeros(0, n)\n",
    "R = zeros(0)\n",
    "\n",
    "b = 1 / (B + 1)\n",
    "\n",
    "for s in 0:(M + B)\n",
    "    for a in 0:min(M, s)\n",
    "        s_indices = [s_indices; s + 1]\n",
    "        a_indices = [a_indices; a + 1]\n",
    "        q = zeros(1, n)\n",
    "        q[(a + 1):((a + B) + 1)] .= b\n",
    "        Q = [Q; q]\n",
    "        R = [R; u(s-a)]\n",
    "    end\n",
    "end\n",
    "\n",
    "ddp = DiscreteDP(R, Q, β, s_indices, a_indices);\n",
    "results = solve(ddp, PFI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed020d7",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"State-Action Pair Tests\" begin\n",
    "    @test results.v[4] ≈ 20.749453024528794 # Some checks on the returned solutions.\n",
    "    @test results.sigma == [1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 6, 6, 6, 6]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db95a64",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In the stochastic optimal growth lecture {doc}`dynamic programming lecture <../dynamic_programming/optgrowth>`, we solve a\n",
    "{ref}`benchmark model <benchmark_growth_mod>` that has an analytical solution to check we could replicate it numerically.\n",
    "\n",
    "The exercise is to replicate this solution using `DiscreteDP`.\n",
    "\n",
    "## Solutions\n",
    "\n",
    "These were written jointly by Max Huber and Daisuke Oyama.\n",
    "\n",
    "### Setup\n",
    "\n",
    "Details of the model can be found in [the\n",
    "lecture](https://julia.quantecon.org/dynamic_programming/optgrowth.html). As in the lecture,\n",
    "we let $f(k) = k^{\\alpha}$ with $\\alpha = 0.65$,\n",
    "$u(c) = \\log c$, and $\\beta = 0.95$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdb8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "α = 0.65\n",
    "f(k) = k.^α\n",
    "u_log(x) = log(x)\n",
    "β = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875fb94",
   "metadata": {},
   "source": [
    "Here we want to solve a finite state version of the continuous state\n",
    "model above. We discretize the state space into a grid of size\n",
    "`grid_size = 500`, from $10^{-6}$ to `grid_max=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8767920",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_max = 2\n",
    "grid_size = 500\n",
    "grid = range(1e-6, grid_max, length = grid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb986136",
   "metadata": {},
   "source": [
    "We choose the action to be the amount of capital to save for the next\n",
    "period (the state is the capital stock at the beginning of the period).\n",
    "Thus the state indices and the action indices are both `1`, ...,\n",
    "`grid_size`. Action (indexed by) `a` is feasible at state (indexed\n",
    "by) `s` if and only if `grid[a] < f([grid[s])` (zero consumption is\n",
    "not allowed because of the log utility).\n",
    "\n",
    "Thus the Bellman equation is:\n",
    "\n",
    "$$\n",
    "v(k) = \\max_{0 < k' < f(k)} u(f(k) - k') + \\beta v(k'),\n",
    "$$\n",
    "\n",
    "where $k^{\\prime}$ is the capital stock in the next period.\n",
    "\n",
    "The transition probability array `Q` will be highly sparse (in fact it\n",
    "is degenerate as the model is deterministic), so we formulate the\n",
    "problem with state-action pairs, to represent `Q` in sparse matrix\n",
    "format.\n",
    "\n",
    "We first construct indices for state-action pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8af91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = f.(grid) .- grid'\n",
    "coord = repeat(collect(1:grid_size), 1, grid_size) #coordinate matrix\n",
    "s_indices = coord[C .> 0]\n",
    "a_indices = transpose(coord)[C .> 0]\n",
    "L = length(a_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed687cdb",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"SAP Tests 2\" begin\n",
    "    @test L == 118841\n",
    "    @test a_indices[14] == 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b5716",
   "metadata": {},
   "source": [
    "Now let's set up $R$ and $Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ef05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = u_log.(C[C.>0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc218a0",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"R Tests\" begin\n",
    "    @test R[4] ≈ -2.873514275079717\n",
    "    @test length(R) == 118841\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "\n",
    "Q = spzeros(L, grid_size) # Formerly spzeros\n",
    "\n",
    "for i in 1:L\n",
    "    Q[i, a_indices[i]] = 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688bb1db",
   "metadata": {},
   "source": [
    "We're now in a position to create an instance of `DiscreteDP`\n",
    "corresponding to the growth model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352beb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp = DiscreteDP(R, Q, β, s_indices, a_indices);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c477e62",
   "metadata": {},
   "source": [
    "### Solving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = solve(ddp, PFI)\n",
    "v, σ, num_iter = results.v, results.sigma, results.num_iter\n",
    "num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d01c97",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Results Test\" begin\n",
    "    @test v[4] ≈ -42.301381867365954\n",
    "    @test σ[4] == 10\n",
    "    @test num_iter ≤ 10\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff2827",
   "metadata": {},
   "source": [
    "Let us compare the solution of the discrete model with the exact\n",
    "solution of the original continuous model. Here's the exact solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = f(grid) - grid[σ]\n",
    "\n",
    "ab = α * β\n",
    "c1 = (log(1 - α * β) + log(α * β) * α * β / (1 - α * β)) / (1 - β)\n",
    "c2 = α / (1 - α * β)\n",
    "\n",
    "v_star(k) = c1 + c2 * log(k)\n",
    "c_star(k) = (1 - α * β) * k.^α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5baf09",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Comparison Tests\" begin\n",
    "    @test c2 ≈ 1.699346405228758\n",
    "    @test c_star(c2) ≈ 0.5399016884304844\n",
    "    @test ab ≈ 0.6174999999999999\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e6ad2b",
   "metadata": {},
   "source": [
    "Let's plot the value functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31425369",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(grid, [v v_star.(grid)], ylim = (-40, -32), lw = 2, label = [\"discrete\" \"continuous\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f5547",
   "metadata": {},
   "source": [
    "They are barely distinguishable (although you can see the difference if\n",
    "you zoom).\n",
    "\n",
    "Now let's look at the discrete and exact policy functions for\n",
    "consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c16bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(grid, [c c_star.(grid)], lw = 2, label = [\"discrete\" \"continuous\"], legend = :topleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f839489",
   "metadata": {},
   "source": [
    "These functions are again close, although some difference is visible and\n",
    "becomes more obvious as you zoom. Here are some statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd56037",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(abs(x - v_star(y)) for (x, y) in zip(v, grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0c17a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Error Tests\" begin\n",
    "    @test maximum(abs(x - v_star(y)) for (x, y) in zip(v, grid)) ≈ 121.49819147053378\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8112b",
   "metadata": {},
   "source": [
    "This is a big error, but most of the error occurs at the lowest\n",
    "gridpoint. Otherwise the fit is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad927c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(abs(v[idx] - v_star(grid[idx])) for idx in 2:lastindex(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24edf94e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Truncated Error Tests\" begin\n",
    "    @test maximum(abs(v[idx] - v_star(grid[idx])) for idx in 2:lastindex(v)) ≈ 0.012681735127500815\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba679c3",
   "metadata": {},
   "source": [
    "The value function is monotone, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b29efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(x -> x ≥ 0, diff(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a7e71e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Monotonicity Test\" begin\n",
    "    @test all(x -> x ≥ 0, diff(v))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca46125",
   "metadata": {},
   "source": [
    "### Comparison of the solution methods\n",
    "\n",
    "Let's try different solution methods. The results below show that policy\n",
    "function iteration and modified policy function iteration are much\n",
    "faster that value function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark results = solve(ddp, PFI)\n",
    "results = solve(ddp, PFI);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b02de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark res1 = solve(ddp, VFI, max_iter = 500, epsilon = 1e-4)\n",
    "res1 = solve(ddp, VFI, max_iter = 500, epsilon = 1e-4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1.num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ == res1.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec28843",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Equivalence Test\" begin\n",
    "    @test σ == res1.sigma\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1296cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark res2 = solve(ddp, MPFI, max_iter = 500, epsilon = 1e-4)\n",
    "res2 = solve(ddp, MPFI, max_iter = 500, epsilon = 1e-4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd7791",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2.num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "σ == res2.sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a2753",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Other Equivalence Test\" begin\n",
    "    @test σ == res2.sigma\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625486b3",
   "metadata": {},
   "source": [
    "### Replication of the figures\n",
    "\n",
    "Let's visualize convergence of value function iteration, as in the\n",
    "lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 5log.(grid) .- 25  # Initial condition\n",
    "n = 50\n",
    "\n",
    "ws = []\n",
    "colors = []\n",
    "w = w_init\n",
    "for i in 0:n-1\n",
    "    w = bellman_operator(ddp, w)\n",
    "    push!(ws, w)\n",
    "    push!(colors, RGBA(0, 0, 0, i/n))\n",
    "end\n",
    "\n",
    "plot(grid,\n",
    "     w_init,\n",
    "     ylims = (-40, -20),\n",
    "     lw = 2,\n",
    "     xlims = extrema(grid),\n",
    "     label = \"initial condition\")\n",
    "\n",
    "plot!(grid, ws,  label = \"\", color = reshape(colors, 1, length(colors)), lw = 2)\n",
    "plot!(grid, v_star.(grid), label = \"true value function\", color = :red, lw = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0fb76",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Plots Test\" begin\n",
    "    @test ws[4][5] ≈ -37.93858578025213\n",
    "    @test v_star.(grid)[4] ≈ -42.29801689484901\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1c0596",
   "metadata": {},
   "source": [
    "We next plot the consumption policies along the value iteration. First\n",
    "we write a function to generate the and record the policies at given\n",
    "stages of iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19ef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_policies(n_vals...)\n",
    "    c_policies = []\n",
    "    w = w_init\n",
    "    for n in 1:maximum(n_vals)\n",
    "        w = bellman_operator(ddp, w)\n",
    "        if n in n_vals\n",
    "            σ = compute_greedy(ddp, w)\n",
    "            c_policy = f(grid) - grid[σ]\n",
    "            push!(c_policies, c_policy)\n",
    "        end\n",
    "    end\n",
    "    return c_policies\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b3688",
   "metadata": {},
   "source": [
    "Now let's generate the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_c = c_star.(grid)\n",
    "c_policies = compute_policies(2, 4, 6)\n",
    "plot_vecs = [c_policies[1] c_policies[2] c_policies[3] true_c true_c true_c]\n",
    "l1 = \"approximate optimal policy\"\n",
    "l2 = \"optimal consumption policy\"\n",
    "labels = [l1 l1 l1 l2 l2 l2]\n",
    "plot(grid,\n",
    "     plot_vecs,\n",
    "     xlim = (0, 2),\n",
    "     ylim = (0, 1),\n",
    "     layout = (3, 1),\n",
    "     lw = 2,\n",
    "     label = labels,\n",
    "     size = (600, 800),\n",
    "     title = [\"2 iterations\" \"4 iterations\" \"6 iterations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca9a21",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"New Tests\" begin\n",
    "    @test true_c[5] ≈ 0.026055057901168556\n",
    "    @test c_policies[1][5] ≈ 0.016012616069698123\n",
    "    @test c_policies[2][5] ≈ 0.02402864412581035\n",
    "    @test c_policies[3][5] ≈ 0.02402864412581035\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb2b43",
   "metadata": {},
   "source": [
    "### Dynamics of the capital stock\n",
    "\n",
    "Finally, let us work on [Exercise\n",
    "2](https://julia.quantecon.org/dynamic_programming/optgrowth.html#Exercise-1), where we plot\n",
    "the trajectories of the capital stock for three different discount\n",
    "factors, $0.9$, $0.94$, and $0.98$, with initial\n",
    "condition $k_0 = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801fdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factors = (0.9, 0.94, 0.98)\n",
    "k_init = 0.1\n",
    "\n",
    "k_init_ind = findfirst(collect(grid) .≥ k_init)\n",
    "\n",
    "sample_size = 25\n",
    "\n",
    "ddp0 = DiscreteDP(R, Q, β, s_indices, a_indices)\n",
    "k_paths = []\n",
    "labels = []\n",
    "\n",
    "for β in discount_factors\n",
    "    ddp0.beta = β\n",
    "    res0 = solve(ddp0, PFI)\n",
    "    k_path_ind = simulate(res0.mc, sample_size, init=k_init_ind)\n",
    "    k_path = grid[k_path_ind.+1]\n",
    "    push!(k_paths, k_path)\n",
    "    push!(labels, \"β = $β\")\n",
    "end\n",
    "\n",
    "plot(k_paths,\n",
    "     xlabel = \"time\",\n",
    "     ylabel = \"capital\",\n",
    "     ylim = (0.1, 0.3),\n",
    "     lw = 2,\n",
    "     markershape = :circle,\n",
    "     label = reshape(labels, 1, length(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b58db7",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Final Tests\" begin\n",
    "    @test k_init_ind == 26\n",
    "    @test k_paths[3][2] ≈ 0.14829751903807614\n",
    "    @test k_paths[2][5] ≈ 0.21242574348697396\n",
    "    @test k_paths[1][7] ≈ 0.20841772945891784\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdeff5",
   "metadata": {},
   "source": [
    "(ddp_algorithms)=\n",
    "## Appendix: Algorithms\n",
    "\n",
    "This appendix covers the details of the solution algorithms implemented for `DiscreteDP`.\n",
    "\n",
    "We will make use of the following notions of approximate optimality:\n",
    "\n",
    "* For $\\varepsilon > 0$, $v$ is called an  $\\varepsilon$-approximation of $v^*$ if $\\lVert v - v^*\\rVert < \\varepsilon$\n",
    "* A policy $\\sigma \\in \\Sigma$ is called $\\varepsilon$-optimal if $v_{\\sigma}$ is an $\\varepsilon$-approximation of $v^*$\n",
    "\n",
    "### Value Iteration\n",
    "\n",
    "The `DiscreteDP` value iteration method implements value function iteration as\n",
    "follows\n",
    "\n",
    "1. Choose any $v^0 \\in \\mathbb{R}^n$, and specify $\\varepsilon > 0$; set $i = 0$.\n",
    "1. Compute $v^{i+1} = T v^i$.\n",
    "1. If $\\lVert v^{i+1} - v^i\\rVert <  [(1 - \\beta) / (2\\beta)] \\varepsilon$,\n",
    "   then go to step 4; otherwise, set $i = i + 1$ and go to step 2.\n",
    "1. Compute a $v^{i+1}$-greedy policy $\\sigma$, and return $v^{i+1}$ and $\\sigma$.\n",
    "\n",
    "Given $\\varepsilon > 0$, the value iteration algorithm\n",
    "\n",
    "* terminates in a finite number of iterations\n",
    "* returns an $\\varepsilon/2$-approximation of the optimal value function and an $\\varepsilon$-optimal policy function (unless `iter_max` is reached)\n",
    "\n",
    "(While not explicit, in the actual implementation each algorithm is\n",
    "terminated if the number of iterations reaches `iter_max`)\n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "The `DiscreteDP` policy iteration method runs as follows\n",
    "\n",
    "1. Choose any $v^0 \\in \\mathbb{R}^n$ and compute a $v^0$-greedy policy $\\sigma^0$; set $i = 0$.\n",
    "1. Compute the value $v_{\\sigma^i}$ by solving\n",
    "   the equation $v = T_{\\sigma^i} v$.\n",
    "1. Compute a $v_{\\sigma^i}$-greedy policy\n",
    "   $\\sigma^{i+1}$; let $\\sigma^{i+1} = \\sigma^i$ if\n",
    "   possible.\n",
    "1. If $\\sigma^{i+1} = \\sigma^i$, then return $v_{\\sigma^i}$\n",
    "   and $\\sigma^{i+1}$; otherwise, set $i = i + 1$ and go to\n",
    "   step 2.\n",
    "\n",
    "The policy iteration algorithm terminates in a finite number of\n",
    "iterations.\n",
    "\n",
    "It returns an optimal value function and an optimal policy function (unless `iter_max` is reached).\n",
    "\n",
    "### Modified Policy Iteration\n",
    "\n",
    "The `DiscreteDP` modified policy iteration method runs as follows:\n",
    "\n",
    "1. Choose any $v^0 \\in \\mathbb{R}^n$, and specify $\\varepsilon > 0$ and $k \\geq 0$; set $i = 0$.\n",
    "1. Compute a $v^i$-greedy policy $\\sigma^{i+1}$; let $\\sigma^{i+1} = \\sigma^i$ if possible (for $i \\geq 1$).\n",
    "1. Compute $u = T v^i$ ($= T_{\\sigma^{i+1}} v^i$). If $\\mathrm{span}(u - v^i) < [(1 - \\beta) / \\beta] \\varepsilon$, then go to step 5; otherwise go to step 4.\n",
    "    * Span is defined by $\\mathrm{span}(z) = \\max(z) - \\min(z)$\n",
    "1. Compute $v^{i+1} = (T_{\\sigma^{i+1}})^k u$ ($= (T_{\\sigma^{i+1}})^{k+1} v^i$); set $i = i + 1$ and go to step 2.\n",
    "1. Return $v = u + [\\beta / (1 - \\beta)] [(\\min(u - v^i) + \\max(u - v^i)) / 2] \\mathbf{1}$ and $\\sigma_{i+1}$.\n",
    "\n",
    "Given $\\varepsilon > 0$, provided that $v^0$ is such that\n",
    "$T v^0 \\geq v^0$, the modified policy iteration algorithm\n",
    "terminates in a finite number of iterations.\n",
    "\n",
    "It returns an $\\varepsilon/2$-approximation of the optimal value function and an $\\varepsilon$-optimal policy function (unless `iter_max` is reached).\n",
    "\n",
    "See also the documentation for `DiscreteDP`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
