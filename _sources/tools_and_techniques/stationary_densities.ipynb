{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6307a958",
   "metadata": {},
   "source": [
    "(statd)=\n",
    "```{raw} html\n",
    "<div id=\"qe-notebook-header\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# {index}`Continuous State Markov Chains <single: Continuous State Markov Chains>`\n",
    "\n",
    "```{index} single: Markov Chains; Continuous State\n",
    "```\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture, we will study the case of continuous (i.e., uncountable) state Markov chains.\n",
    "\n",
    "A {doc}`previous lecture <../introduction_dynamics/finite_markov>` will cover finite Markov chains, a relatively elementary class of stochastic dynamic models.\n",
    "\n",
    "Most stochastic dynamic models studied by economists either fit directly into this class or can be represented as continuous state Markov chains after minor modifications.\n",
    "\n",
    "In this lecture, our focus will be on continuous Markov models that\n",
    "\n",
    "* evolve in discrete time\n",
    "* are often nonlinear\n",
    "\n",
    "The fact that we accommodate nonlinear models here is significant, because\n",
    "linear stochastic models have their own highly developed tool set.\n",
    "\n",
    "The question that interests us most is: Given a particular stochastic dynamic\n",
    "model, how will the state of the system evolve over time?\n",
    "\n",
    "In particular,\n",
    "\n",
    "* What happens to the distribution of the state variables?\n",
    "* Is there anything we can say about the \"average behavior\" of these variables?\n",
    "* Is there a notion of \"steady state\" or \"long run equilibrium\" that's applicable to the model?\n",
    "    * If so, how can we compute it?\n",
    "\n",
    "Answering these questions will lead us to explore topics such as simulation, distribution dynamics, stability, and ergodicity.\n",
    "\n",
    "```{note}\n",
    "For some people, the term \"Markov chain\" always refers to a process with a\n",
    "finite or discrete state space.  We follow the mainstream\n",
    "mathematical literature (e.g., {cite}`MeynTweedie2009`) in using the term to refer to any discrete **time**\n",
    "Markov process.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319731a3",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics, Distributions, LaTeXStrings, Plots, StatsPlots,\n",
    "      KernelDensity, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ebdd4",
   "metadata": {},
   "source": [
    "(statd_density_case)=\n",
    "## The Density Case\n",
    "\n",
    "You are probably aware that some distributions can be represented by densities\n",
    "and some cannot.\n",
    "\n",
    "(For example, distributions on the real numbers $\\mathbb R$ that put positive probability\n",
    "on individual points have no density representation)\n",
    "\n",
    "We are going to start our analysis by looking at Markov chains where the one step transition probabilities have density representations.\n",
    "\n",
    "The benefit is that the density case offers a very direct parallel to the finite case in terms of notation and intuition.\n",
    "\n",
    "Once we've built some intuition we'll cover the general case.\n",
    "\n",
    "### Definitions and Basic Properties\n",
    "\n",
    "We can start by considering a finite state space $S$. In this setting, the dynamics of the model are described by a stochastic matrix --- a nonnegative square matrix $P = P[i, j]$ such that each row $P[i, \\cdot]$ sums to one.\n",
    "\n",
    "The interpretation of $P$ is that $P[i, j]$ represents the\n",
    "probability of transitioning from state $i$ to state $j$ in one\n",
    "unit of time.\n",
    "\n",
    "In symbols,\n",
    "\n",
    "$$\n",
    "\\mathbb P \\{ X_{t+1} = j \\,|\\, X_t = i \\} = P[i, j]\n",
    "$$\n",
    "\n",
    "Equivalently,\n",
    "\n",
    "* $P$ can be thought of as a family of distributions $P[i, \\cdot]$, one for each $i \\in S$\n",
    "* $P[i, \\cdot]$ is the distribution of $X_{t+1}$ given $X_t = i$\n",
    "\n",
    "(As you probably recall, when using Julia arrays, $P[i, \\cdot]$ is expressed as `P[i,:]`)\n",
    "\n",
    "In this section, we'll allow $S$ to be a subset of $\\mathbb R$, such as\n",
    "\n",
    "* $\\mathbb R$ itself\n",
    "* the positive reals $(0, \\infty)$\n",
    "* a bounded interval $(a, b)$\n",
    "\n",
    "The family of discrete distributions $P[i, \\cdot]$ will be replaced by a family of densities $p(x, \\cdot)$, one for each $x \\in S$.\n",
    "\n",
    "Analogous to the finite state case, $p(x, \\cdot)$ is to be understood as the distribution (density) of $X_{t+1}$ given $X_t = x$.\n",
    "\n",
    "More formally, a *stochastic kernel on* $S$ is a function $p \\colon S \\times S \\to \\mathbb R$ with the property that\n",
    "\n",
    "1. $p(x, y) \\geq 0$ for all $x, y \\in S$\n",
    "1. $\\int p(x, y) dy = 1$ for all $x \\in S$\n",
    "\n",
    "(Integrals are over the whole space unless otherwise specified)\n",
    "\n",
    "For example, let $S = \\mathbb R$ and consider the particular stochastic\n",
    "kernel $p_w$ defined by\n",
    "\n",
    "```{math}\n",
    ":label: statd_rwsk\n",
    "\n",
    "p_w(x, y) := \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left\\{ - \\frac{(y - x)^2}{2} \\right\\}\n",
    "```\n",
    "\n",
    "What kind of model does $p_w$ represent?\n",
    "\n",
    "The answer is, the (normally distributed) random walk\n",
    "\n",
    "```{math}\n",
    ":label: statd_rw\n",
    "\n",
    "X_{t+1} = X_t + \\xi_{t+1}\n",
    "\\quad \\text{where} \\quad\n",
    "\\{ \\xi_t \\} \\stackrel {\\textrm{ IID }} {\\sim} N(0, 1)\n",
    "```\n",
    "\n",
    "To see this, let's find the stochastic kernel $p$ corresponding to {eq}`statd_rw`.\n",
    "\n",
    "Recall that $p(x, \\cdot)$ represents the distribution of $X_{t+1}$ given $X_t = x$.\n",
    "\n",
    "Letting $X_t = x$ in {eq}`statd_rw` and considering the distribution of $X_{t+1}$, we see that $p(x, \\cdot) = N(x, 1)$.\n",
    "\n",
    "In other words, $p$ is exactly $p_w$, as defined in {eq}`statd_rwsk`.\n",
    "\n",
    "### Connection to Stochastic Difference Equations\n",
    "\n",
    "In the previous section, we made the connection between stochastic difference\n",
    "equation {eq}`statd_rw` and stochastic kernel {eq}`statd_rwsk`.\n",
    "\n",
    "In economics and time series analysis we meet stochastic difference equations of all different shapes and sizes.\n",
    "\n",
    "It will be useful for us if we have some systematic methods for converting stochastic difference equations into stochastic kernels.\n",
    "\n",
    "To this end, consider the generic (scalar) stochastic difference equation given by\n",
    "\n",
    "```{math}\n",
    ":label: statd_srs\n",
    "\n",
    "X_{t+1} = \\mu(X_t) + \\sigma(X_t) \\, \\xi_{t+1}\n",
    "```\n",
    "\n",
    "Here we assume that\n",
    "\n",
    "* $\\{ \\xi_t \\} \\stackrel {\\textrm{ IID }} {\\sim} \\phi$, where $\\phi$ is a given density on $\\mathbb R$\n",
    "* $\\mu$ and $\\sigma$ are given functions on $S$, with $\\sigma(x) > 0$ for all $x$\n",
    "\n",
    "**Example 1:** The random walk {eq}`statd_rw` is a special case of {eq}`statd_srs`, with $\\mu(x) = x$ and $\\sigma(x) = 1$.\n",
    "\n",
    "**Example 2:** Consider the [ARCH model](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity)\n",
    "\n",
    "$$\n",
    "X_{t+1} = \\alpha X_t + \\sigma_t \\,  \\xi_{t+1},\n",
    "\\qquad \\sigma^2_t = \\beta + \\gamma X_t^2,\n",
    "\\qquad \\beta, \\gamma > 0\n",
    "$$\n",
    "\n",
    "Alternatively, we can write the model as\n",
    "\n",
    "```{math}\n",
    ":label: statd_arch\n",
    "\n",
    "X_{t+1} = \\alpha X_t + (\\beta + \\gamma X_t^2)^{1/2} \\xi_{t+1}\n",
    "```\n",
    "\n",
    "This is a special case of {eq}`statd_srs` with $\\mu(x) = \\alpha x$ and $\\sigma(x) = (\\beta + \\gamma x^2)^{1/2}$.\n",
    "\n",
    "(solow_swan)=\n",
    "**Example 3:** With stochastic production and a constant savings rate, the one-sector neoclassical growth model leads to a law of motion for capital per worker such as\n",
    "\n",
    "```{math}\n",
    ":label: statd_ss\n",
    "\n",
    "k_{t+1} = s  A_{t+1} f(k_t) + (1 - \\delta) k_t\n",
    "```\n",
    "\n",
    "Here\n",
    "\n",
    "* $s$ is the rate of savings\n",
    "* $A_{t+1}$ is a production shock\n",
    "    * The $t+1$ subscript indicates that $A_{t+1}$ is not visible at time $t$\n",
    "* $\\delta$ is a depreciation rate\n",
    "* $f \\colon \\mathbb R_+ \\to \\mathbb R_+$ is a production function satisfying $f(k) > 0$ whenever $k > 0$\n",
    "\n",
    "(The fixed savings rate can be rationalized as the optimal policy for a particular set of technologies and preferences (see {cite}`Ljungqvist2012`, section\n",
    "3.1.2), although we omit the details here)\n",
    "\n",
    "Equation {eq}`statd_ss` is a special case of {eq}`statd_srs` with $\\mu(x) = (1 - \\delta)x$ and $\\sigma(x) = s f(x)$.\n",
    "\n",
    "Now let's obtain the stochastic kernel corresponding to the generic model {eq}`statd_srs`.\n",
    "\n",
    "To find it, note first that if $U$ is a random variable with\n",
    "density $f_U$, and $V = a + b U$ for some constants $a,b$\n",
    "with $b > 0$, then the density of $V$ is given by\n",
    "\n",
    "```{math}\n",
    ":label: statd_dv\n",
    "\n",
    "f_V(v)\n",
    "= \\frac{1}{b}\n",
    "f_U \\left( \\frac{v - a}{b} \\right)\n",
    "```\n",
    "\n",
    "(The proof is {ref}`below <statd_appendix>`.  For a multidimensional version\n",
    "see [EDTC](http://johnstachurski.net/edtc.html), theorem 8.1.3)\n",
    "\n",
    "Taking {eq}`statd_dv` as given for the moment, we can\n",
    "obtain the stochastic kernel $p$ for {eq}`statd_srs` by recalling that\n",
    "$p(x, \\cdot)$ is the conditional density of $X_{t+1}$ given\n",
    "$X_t = x$.\n",
    "\n",
    "In the present case, this is equivalent to stating that $p(x, \\cdot)$ is the density of $Y := \\mu(x) + \\sigma(x) \\, \\xi_{t+1}$ when $\\xi_{t+1} \\sim \\phi$.\n",
    "\n",
    "Hence, by {eq}`statd_dv`,\n",
    "\n",
    "```{math}\n",
    ":label: statd_srssk\n",
    "\n",
    "p(x, y)\n",
    "= \\frac{1}{\\sigma(x)}\n",
    "\\phi \\left( \\frac{y - \\mu(x)}{\\sigma(x)} \\right)\n",
    "```\n",
    "\n",
    "For example, the growth model in {eq}`statd_ss` has stochastic kernel\n",
    "\n",
    "```{math}\n",
    ":label: statd_sssk\n",
    "\n",
    "p(x, y)\n",
    "= \\frac{1}{sf(x)}\n",
    "\\phi \\left( \\frac{y - (1 - \\delta) x}{s f(x)} \\right)\n",
    "```\n",
    "\n",
    "where $\\phi$ is the density of $A_{t+1}$.\n",
    "\n",
    "(Regarding the state space $S$ for this model, a natural choice is $(0, \\infty)$ --- in which case\n",
    "$\\sigma(x) = s f(x)$ is strictly positive for all $s$ as required)\n",
    "\n",
    "### Distribution Dynamics\n",
    "\n",
    "Later in {ref}`another section <mc_md>` of our lecture on **finite** Markov chains, we\n",
    "will explore the following question: If\n",
    "\n",
    "1. $\\{X_t\\}$ is a Markov chain with stochastic matrix $P$\n",
    "1. the distribution of $X_t$ is known to be $\\psi_t$\n",
    "\n",
    "then what is the distribution of $X_{t+1}$?\n",
    "\n",
    "Letting $\\psi_{t+1}$ denote the distribution of $X_{t+1}$, we will show that\n",
    "\n",
    "$$\n",
    "\\psi_{t+1}[j] = \\sum_{i \\in S} P[i,j] \\psi_t[i]\n",
    "$$\n",
    "\n",
    "This intuitive equality states that the probability of being at $j$\n",
    "tomorrow is the probability of visiting $i$ today and then going on to\n",
    "$j$, summed over all possible $i$.\n",
    "\n",
    "In the density case, we just replace the sum with an integral and probability\n",
    "mass functions with densities, yielding\n",
    "\n",
    "```{math}\n",
    ":label: statd_fdd\n",
    "\n",
    "\\psi_{t+1}(y) = \\int p(x,y) \\psi_t(x) \\, dx,\n",
    "\\qquad \\forall y \\in S\n",
    "```\n",
    "\n",
    "It is convenient to think of this updating process in terms of an operator.\n",
    "\n",
    "(An operator is just a function, but the term is usually reserved for a function that sends functions into functions)\n",
    "\n",
    "Let $\\mathscr D$ be the set of all densities on $S$, and let\n",
    "$P$ be the operator from $\\mathscr D$ to itself that takes density\n",
    "$\\psi$ and sends it into new density $\\psi P$, where the latter is\n",
    "defined by\n",
    "\n",
    "```{math}\n",
    ":label: def_dmo\n",
    "\n",
    "(\\psi P)(y) = \\int p(x,y) \\psi(x) dx\n",
    "```\n",
    "\n",
    "This operator is usually called the *Markov operator* corresponding to $p$\n",
    "\n",
    "```{note}\n",
    "Unlike most operators, we write $P$ to the right of its argument,\n",
    "instead of to the left (i.e., $\\psi P$ instead of $P \\psi$).\n",
    "This is a common convention, with the intention being to maintain the\n",
    "parallel with the finite case --- see {ref}`here <mc_fddv>`.\n",
    "```\n",
    "\n",
    "With this notation, we can write {eq}`statd_fdd` more succinctly as $\\psi_{t+1}(y) = (\\psi_t P)(y)$ for all $y$, or, dropping the $y$ and letting \"$=$\" indicate equality of functions,\n",
    "\n",
    "```{math}\n",
    ":label: statd_p\n",
    "\n",
    "\\psi_{t+1} = \\psi_t P\n",
    "```\n",
    "\n",
    "Equation {eq}`statd_p` tells us that if we specify a distribution for $\\psi_0$, then the entire sequence\n",
    "of future distributions can be obtained by iterating with $P$.\n",
    "\n",
    "It's interesting to note that {eq}`statd_p` is a deterministic difference equation.\n",
    "\n",
    "Thus, by converting a stochastic difference equation such as\n",
    "{eq}`statd_srs` into a stochastic kernel $p$ and hence an operator\n",
    "$P$, we convert a stochastic difference equation into a deterministic\n",
    "one (albeit in a much higher dimensional space).\n",
    "\n",
    "```{note}\n",
    "Some people might be aware that discrete Markov chains are in fact\n",
    "a special case of the continuous Markov chains we have just described.  The reason is\n",
    "that probability mass functions are densities with respect to\n",
    "the [counting measure](https://en.wikipedia.org/wiki/Counting_measure).\n",
    "```\n",
    "\n",
    "### Computation\n",
    "\n",
    "To learn about the dynamics of a given process, it's useful to compute and study the sequences of densities generated by the model.\n",
    "\n",
    "One way to do this is to try to implement the iteration described by {eq}`def_dmo` and {eq}`statd_p` using numerical integration.\n",
    "\n",
    "However, to produce $\\psi P$ from $\\psi$ via {eq}`def_dmo`, you\n",
    "would need to integrate at every $y$, and there is a continuum of such\n",
    "$y$.\n",
    "\n",
    "Another possibility is to discretize the model, but this introduces errors of unknown size.\n",
    "\n",
    "A nicer alternative in the present setting is to combine simulation with an elegant estimator called the *look ahead* estimator.\n",
    "\n",
    "Let's go over the ideas with reference to the growth model {ref}`discussed above <solow_swan>`, the dynamics of which we repeat here for convenience:\n",
    "\n",
    "```{math}\n",
    ":label: statd_ss2\n",
    "\n",
    "k_{t+1} = s  A_{t+1} f(k_t) + (1 - \\delta) k_t\n",
    "```\n",
    "\n",
    "Our aim is to compute the sequence $\\{ \\psi_t \\}$ associated with this model and fixed initial condition $\\psi_0$.\n",
    "\n",
    "To approximate $\\psi_t$ by simulation, recall that, by definition, $\\psi_t$ is the density of $k_t$ given $k_0 \\sim \\psi_0$.\n",
    "\n",
    "If we wish to generate observations of this random variable,  all we need to do is\n",
    "\n",
    "1. draw $k_0$ from the specified initial condition $\\psi_0$\n",
    "1. draw the shocks $A_1, \\ldots, A_t$ from their specified density $\\phi$\n",
    "1. compute $k_t$ iteratively via {eq}`statd_ss2`\n",
    "\n",
    "If we repeat this $n$ times, we get $n$ independent observations $k_t^1, \\ldots, k_t^n$.\n",
    "\n",
    "With these draws in hand, the next step is to generate some kind of representation of their distribution $\\psi_t$.\n",
    "\n",
    "A naive approach would be to use a histogram, or perhaps a [smoothed histogram](https://en.wikipedia.org/wiki/Kernel_density_estimation) using  the `kde` function from [KernelDensity.jl](https://github.com/JuliaStats/KernelDensity.jl).\n",
    "\n",
    "However, in the present setting there is a much better way to do this, based on the look-ahead estimator.\n",
    "\n",
    "With this estimator, to construct an estimate of $\\psi_t$, we\n",
    "actually generate $n$ observations of $k_{t-1}$, rather than $k_t$.\n",
    "\n",
    "Now we take these $n$ observations $k_{t-1}^1, \\ldots,\n",
    "k_{t-1}^n$ and form the estimate\n",
    "\n",
    "```{math}\n",
    ":label: statd_lae1\n",
    "\n",
    "\\psi_t^n(y) = \\frac{1}{n} \\sum_{i=1}^n p(k_{t-1}^i, y)\n",
    "```\n",
    "\n",
    "where $p$ is the growth model stochastic kernel in {eq}`statd_sssk`.\n",
    "\n",
    "What is the justification for this slightly surprising estimator?\n",
    "\n",
    "The idea is that, by the strong {ref}`law of large numbers <lln_ksl>`,\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n p(k_{t-1}^i, y)\n",
    "\\to\n",
    "\\mathbb E p(k_{t-1}^i, y)\n",
    "= \\int p(x, y) \\psi_{t-1}(x) \\, dx\n",
    "= \\psi_t(y)\n",
    "$$\n",
    "\n",
    "with probability one as $n \\to \\infty$.\n",
    "\n",
    "Here the first equality is by the definition of $\\psi_{t-1}$, and the\n",
    "second is by {eq}`statd_fdd`.\n",
    "\n",
    "We have just shown that our estimator $\\psi_t^n(y)$ in {eq}`statd_lae1`\n",
    "converges almost surely to $\\psi_t(y)$, which is just what we want to compute.\n",
    "\n",
    "```{only} html\n",
    "In fact much stronger convergence results are true (see, for example, <a href=/_static/pdfs/ECTA6180.pdf download>this paper</a>).\n",
    "```\n",
    "\n",
    "```{only} latex\n",
    "In fact much stronger convergence results are true (see, for example, [this paper](https://lectures.quantecon.org/_downloads/ECTA6180.pdf)).\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "We'll implement a function `lae_est(p, X, ygrid)` that returns the right-hand side of {eq}`statd_lae1`, averaging the kernel $p$ over simulated draws `X` on a grid `ygrid` (see [here](https://github.com/QuantEcon/QuantEcon.jl/blob/master/src/lae.jl) for the original implementation)\n",
    "\n",
    "### Example\n",
    "\n",
    "The following code is example of usage for the stochastic growth model {ref}`described above <solow_swan>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ecc65e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "using Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42) # For deterministic results.\n",
    "\n",
    "s = 0.2\n",
    "delta = 0.1\n",
    "a_sigma = 0.4                    # A = exp(B) where B ~ N(0, a_sigma)\n",
    "alpha = 0.4                      # We set f(k) = k**alpha\n",
    "psi_0 = Beta(5.0, 5.0)         # Initial distribution\n",
    "phi = LogNormal(0.0, a_sigma)\n",
    "\n",
    "function p(x, y)\n",
    "    # Stochastic kernel for the growth model with Cobb-Douglas production.\n",
    "    # Both x and y must be strictly positive.\n",
    "\n",
    "    d = s * x .^ alpha\n",
    "\n",
    "    pdf_arg = clamp.((y .- (1 - delta) .* x) ./ d, eps(), Inf)\n",
    "    return pdf.(phi, pdf_arg) ./ d\n",
    "end\n",
    "\n",
    "function lae_est(p, X, ygrid)\n",
    "    Xmat = reshape(X, :, 1)\n",
    "    Ymat = reshape(ygrid, 1, :)\n",
    "    psi_vals = mean(p.(Xmat, Ymat), dims = 1)\n",
    "    return dropdims(psi_vals, dims = 1)\n",
    "end\n",
    "\n",
    "n = 10000  # Number of observations at each date t\n",
    "T = 30     # Compute density of k_t at 1,...,T+1\n",
    "\n",
    "# Generate matrix s.t. t-th column is n observations of k_t\n",
    "k = zeros(n, T)\n",
    "A = rand!(phi, zeros(n, T))\n",
    "\n",
    "# Draw first column from initial distribution\n",
    "k[:, 1] = rand(psi_0, n) ./ 2  # divide by 2 to match scale = 0.5 in py version\n",
    "for t in 1:(T - 1)\n",
    "    k[:, t + 1] = s * A[:, t] .* k[:, t] .^ alpha + (1 - delta) .* k[:, t]\n",
    "end\n",
    "\n",
    "# Store draws for each date t\n",
    "laes = [k[:, t] for t in T:-1:1]\n",
    "\n",
    "# Plot\n",
    "ygrid = range(0.01, 4, length = 200)\n",
    "laes_plot = []\n",
    "colors = []\n",
    "for i in 1:T\n",
    "    lae = laes[i]\n",
    "    push!(laes_plot, lae_est(p, lae, ygrid))\n",
    "    push!(colors, RGBA(0, 0, 0, 1 - (i - 1) / T))\n",
    "end\n",
    "plot(ygrid, laes_plot, color = reshape(colors, 1, length(colors)), lw = 2,\n",
    "     xlabel = \"capital\", legend = :none)\n",
    "t = L\"Density of $k_1$ (lighter) to $k_T$ (darker) for $T=%$T$\"\n",
    "plot!(title = t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b299f4",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"First Figure Tests\" begin\n",
    "    @test laes[2][4] ≈ 2.8665915765284966\n",
    "    @test length(ygrid) == 200 && ygrid[1] ≈ 0.01 && ygrid[end] ≈ 4.0\n",
    "    @test mean(laes[1]) ≈ 2.819871989291165\n",
    "    @test mean(laes[30]) ≈ 0.2512486313471454\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c0d94",
   "metadata": {},
   "source": [
    "The figure shows part of the density sequence $\\{\\psi_t\\}$, with each\n",
    "density computed via the look ahead estimator.\n",
    "\n",
    "Notice that the sequence of densities shown in the figure seems to be\n",
    "converging --- more on this in just a moment.\n",
    "\n",
    "Another quick comment is that each of these distributions could be interpreted\n",
    "as a cross sectional distribution (see {ref}`this discussion <mc_eg1-1>`).\n",
    "\n",
    "## Beyond Densities\n",
    "\n",
    "Up until now, we have focused exclusively on continuous state Markov chains\n",
    "where all conditional distributions $p(x, \\cdot)$ are densities.\n",
    "\n",
    "As discussed above, not all distributions can be represented as densities.\n",
    "\n",
    "If the conditional distribution of $X_{t+1}$ given $X_t = x$\n",
    "**cannot** be represented as a density for some $x \\in S$, then we need a slightly\n",
    "different theory.\n",
    "\n",
    "The ultimate option is to switch from densities to [probability measures](https://en.wikipedia.org/wiki/Probability_measure), but not all readers will\n",
    "be familiar with measure theory.\n",
    "\n",
    "We can, however, construct a fairly general theory using distribution functions.\n",
    "\n",
    "### Example and Definitions\n",
    "\n",
    "To illustrate the issues, recall that Hopenhayn and Rogerson {cite}`HopenhaynRogerson1993` study a model of firm dynamics where individual firm productivity follows the exogenous process\n",
    "\n",
    "$$\n",
    "X_{t+1} = a + \\rho X_t + \\xi_{t+1},\n",
    "\\quad \\text{where} \\quad\n",
    "\\{ \\xi_t \\} \\stackrel {\\textrm{ IID }} {\\sim} N(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "As is, this fits into the density case we treated above.\n",
    "\n",
    "However, the authors wanted this process to take values in $[0, 1]$, so they added boundaries at the end points 0 and 1.\n",
    "\n",
    "One way to write this is\n",
    "\n",
    "$$\n",
    "X_{t+1} = h(a + \\rho X_t + \\xi_{t+1})\n",
    "\\quad \\text{where} \\quad\n",
    "h(x) := x \\, \\mathbf 1\\{0 \\leq x \\leq 1\\} + \\mathbf 1 \\{ x > 1\\}\n",
    "$$\n",
    "\n",
    "If you think about it, you will see that for any given $x \\in [0, 1]$,\n",
    "the conditional distribution of $X_{t+1}$ given $X_t = x$\n",
    "puts positive probability mass on 0 and 1.\n",
    "\n",
    "Hence it cannot be represented as a density.\n",
    "\n",
    "What we can do instead is use cumulative distribution functions (cdfs).\n",
    "\n",
    "To this end, set\n",
    "\n",
    "$$\n",
    "G(x, y) := \\mathbb P \\{ h(a + \\rho x + \\xi_{t+1}) \\leq y \\}\n",
    "\\qquad (0 \\leq x, y \\leq 1)\n",
    "$$\n",
    "\n",
    "This family of cdfs $G(x, \\cdot)$ plays a role analogous to the stochastic kernel in the density case.\n",
    "\n",
    "The distribution dynamics in {eq}`statd_fdd` are then replaced by\n",
    "\n",
    "```{math}\n",
    ":label: statd_fddc\n",
    "\n",
    "F_{t+1}(y) = \\int G(x,y) F_t(dx)\n",
    "```\n",
    "\n",
    "Here $F_t$ and $F_{t+1}$ are cdfs representing the distribution of the current state and next period state.\n",
    "\n",
    "The intuition behind {eq}`statd_fddc` is essentially the same as for {eq}`statd_fdd`.\n",
    "\n",
    "### Computation\n",
    "\n",
    "If you wish to compute these cdfs, you cannot use the look-ahead estimator as before.\n",
    "\n",
    "Indeed, you should not use any density estimator, since the objects you are\n",
    "estimating/computing are not densities.\n",
    "\n",
    "One good option is simulation as before, combined with the [empirical distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function).\n",
    "\n",
    "## Stability\n",
    "\n",
    "In this section, we will explore three concepts: stationarity, stability, and ergodicity. Our focus will be specifically on the density case (as in {ref}`this section <statd_density_case>`), where the stochastic kernel is a family of densities. The general case is relatively similar --- references are given below.\n",
    "\n",
    "\n",
    "### Theoretical Results\n",
    "\n",
    "Given a stochastic kernel $p$ and corresponding Markov operator as\n",
    "defined in {eq}`def_dmo`, a density $\\psi^*$ on $S$ is called\n",
    "*stationary* for $P$ if it is a fixed point of the operator $P$.\n",
    "\n",
    "In other words,\n",
    "\n",
    "```{math}\n",
    ":label: statd_dsd\n",
    "\n",
    "\\psi^*(y) = \\int p(x,y) \\psi^*(x) \\, dx,\n",
    "\\qquad \\forall y \\in S\n",
    "```\n",
    "\n",
    "As with the finite case, if $\\psi^*$ is stationary for $P$, and\n",
    "the distribution of $X_0$ is $\\psi^*$, then, in view of\n",
    "{eq}`statd_p`, $X_t$ will have this same distribution for all $t$.\n",
    "\n",
    "Hence $\\psi^*$ is the stochastic equivalent of a steady state.\n",
    "\n",
    "In the finite case, we will learn that at least one stationary distribution exists, although there may be many.\n",
    "\n",
    "When the state space is infinite, the situation is more complicated.\n",
    "\n",
    "Even existence can fail very easily.\n",
    "\n",
    "For example, the random walk model has no stationary density (see, e.g., [EDTC](http://johnstachurski.net/edtc.html), p. 210).\n",
    "\n",
    "However, there are well-known conditions under which a stationary density $\\psi^*$ exists.\n",
    "\n",
    "With additional conditions, we can also get a unique stationary density ($\\psi \\in \\mathscr D \\text{ and } \\psi = \\psi P \\implies \\psi = \\psi^*$),  and also global convergence in the sense that\n",
    "\n",
    "```{math}\n",
    ":label: statd_dca\n",
    "\n",
    "\\forall \\, \\psi \\in \\mathscr D, \\quad \\psi P^t \\to \\psi^*\n",
    "    \\quad \\text{as} \\quad t \\to \\infty\n",
    "```\n",
    "\n",
    "This combination of existence, uniqueness and global convergence in the sense\n",
    "of {eq}`statd_dca` is often referred to as *global stability*.\n",
    "\n",
    "Under very similar conditions, we get *ergodicity*, which means that\n",
    "\n",
    "```{math}\n",
    ":label: statd_lln\n",
    "\n",
    "\\frac{1}{n} \\sum_{t = 1}^n h(X_t)  \\to \\int h(x) \\psi^*(x) dx\n",
    "    \\quad \\text{as } n \\to \\infty\n",
    "```\n",
    "\n",
    "for any ([measurable](https://en.wikipedia.org/wiki/Measurable_function)) function $h \\colon S \\to \\mathbb R$  such that the right-hand side is finite.\n",
    "\n",
    "Note that the convergence in {eq}`statd_lln` does not depend on the distribution (or value) of $X_0$.\n",
    "\n",
    "This is actually very important for simulation --- it means we can learn about $\\psi^*$ (i.e., approximate the right hand side of {eq}`statd_lln` via the left hand side) without requiring any special knowledge about what to do with $X_0$.\n",
    "\n",
    "So what are these conditions we require to get global stability and ergodicity?\n",
    "\n",
    "In essence, it must be the case that\n",
    "\n",
    "1. Probability mass does not drift off to the \"edges\" of the state space\n",
    "1. Sufficient \"mixing\" obtains\n",
    "\n",
    "For one such set of conditions see theorem 8.2.14 of [EDTC](http://johnstachurski.net/edtc.html).\n",
    "\n",
    "In addition\n",
    "\n",
    "* {cite}`StokeyLucas1989`  contains a classic (but slightly outdated) treatment of these topics.\n",
    "* From the mathematical literature, {cite}`LasotaMackey1994`  and {cite}`MeynTweedie2009` give outstanding in depth treatments.\n",
    "* Section 8.1.2 of [EDTC](http://johnstachurski.net/edtc.html) provides detailed intuition, and section 8.3 gives additional references.\n",
    "* [EDTC](http://johnstachurski.net/edtc.html), section 11.3.4\n",
    "  provides a specific treatment for the growth model we considered in this\n",
    "  lecture.\n",
    "\n",
    "### An Example of Stability\n",
    "\n",
    "As stated above, the {ref}`growth model treated here <solow_swan>` is stable under mild conditions\n",
    "on the primitives.\n",
    "\n",
    "* See [EDTC](http://johnstachurski.net/edtc.html), section 11.3.4 for more details.\n",
    "\n",
    "We can see this stability in action --- in particular, the convergence in {eq}`statd_dca` --- by simulating the path of densities from various initial conditions.\n",
    "\n",
    "Here is such a figure\n",
    "\n",
    "(statd_egs)=\n",
    "```{figure} /_static/figures/solution_statd_ex2.png\n",
    ":width: 85%\n",
    "```\n",
    "\n",
    "All sequences are converging towards the same limit, regardless of their initial condition.\n",
    "\n",
    "The details regarding initial conditions and so on are given in {ref}`this exercise <statd_ex2>`, where you are asked to replicate the figure.\n",
    "\n",
    "### Computing Stationary Densities\n",
    "\n",
    "In the preceding figure, each sequence of densities is converging towards the unique stationary density $\\psi^*$.\n",
    "\n",
    "Even from this figure we can get a fair idea what $\\psi^*$ looks like, and where its mass is located.\n",
    "\n",
    "However, there is a much more direct way to estimate the stationary density,\n",
    "and it involves only a slight modification of the look ahead estimator.\n",
    "\n",
    "Let's say that we have a model of the form {eq}`statd_srs` that is stable and\n",
    "ergodic.\n",
    "\n",
    "Let $p$ be the corresponding stochastic kernel, as given in {eq}`statd_srssk`.\n",
    "\n",
    "To approximate the stationary density $\\psi^*$, we can simply generate a\n",
    "long time series $X_0, X_1, \\ldots, X_n$ and estimate $\\psi^*$ via\n",
    "\n",
    "```{math}\n",
    ":label: statd_lae2\n",
    "\n",
    "\\psi_n^*(y) = \\frac{1}{n} \\sum_{t=1}^n p(X_t, y)\n",
    "```\n",
    "\n",
    "This is essentially the same as the look ahead estimator {eq}`statd_lae1`,\n",
    "except that now the observations we generate are a single time series, rather\n",
    "than a cross section.\n",
    "\n",
    "The justification for {eq}`statd_lae2` is that, with probability one as $n \\to \\infty$,\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{t=1}^n p(X_t, y)\n",
    "\\to\n",
    "\\int p(x, y) \\psi^*(x) \\, dx\n",
    "= \\psi^*(y)\n",
    "$$\n",
    "\n",
    "where the convergence is by {eq}`statd_lln` and the equality on the right is by\n",
    "{eq}`statd_dsd`.\n",
    "\n",
    "The right hand side is exactly what we want to compute.\n",
    "\n",
    "On top of this asymptotic result, it turns out that the rate of convergence\n",
    "for the look ahead estimator is very good.\n",
    "\n",
    "The first exercise helps illustrate this point.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "(statd_ex1)=\n",
    "### Exercise 1\n",
    "\n",
    "Consider the simple threshold autoregressive model\n",
    "\n",
    "```{math}\n",
    ":label: statd_tar\n",
    "\n",
    "X_{t+1} = \\theta |X_t| + (1- \\theta^2)^{1/2} \\xi_{t+1}\n",
    "\\qquad \\text{where} \\quad\n",
    "\\{ \\xi_t \\} \\stackrel {\\textrm{ IID }} {\\sim} N(0, 1)\n",
    "```\n",
    "\n",
    "This is one of those rare nonlinear stochastic models where an analytical\n",
    "expression for the stationary density is available.\n",
    "\n",
    "In particular, provided that $|\\theta| < 1$, there is a unique\n",
    "stationary density $\\psi^*$ given by\n",
    "\n",
    "```{math}\n",
    ":label: statd_tar_ts\n",
    "\n",
    "\\psi^*(y) = 2 \\, \\phi(y) \\, \\Phi\n",
    "\\left[\n",
    "    \\frac{\\theta y}{(1 - \\theta^2)^{1/2}}\n",
    "\\right]\n",
    "```\n",
    "\n",
    "Here $\\phi$ is the standard normal density and $\\Phi$ is the standard normal cdf.\n",
    "\n",
    "As an exercise, compute the look ahead estimate of $\\psi^*$, as defined\n",
    "in {eq}`statd_lae2`, and compare it with $\\psi^*$  in {eq}`statd_tar_ts` to see whether they\n",
    "are indeed close for large $n$.\n",
    "\n",
    "In doing so, set $\\theta = 0.8$ and $n = 500$.\n",
    "\n",
    "The next figure shows the result of such a computation\n",
    "\n",
    "```{figure} /_static/figures/solution_statd_ex1.png\n",
    ":width: 75%\n",
    "```\n",
    "\n",
    "The additional density (black line) is a [nonparametric kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation), added to the solution for illustration.\n",
    "\n",
    "(You can try to replicate it before looking at the solution if you want to)\n",
    "\n",
    "As you can see, the look ahead estimator is a much tighter fit than the kernel\n",
    "density estimator.\n",
    "\n",
    "If you repeat the simulation you will see that this is consistently the case.\n",
    "\n",
    "(statd_ex2)=\n",
    "### Exercise 2\n",
    "\n",
    "Replicate the figure on global convergence {ref}`shown above <statd_egs>`.\n",
    "\n",
    "The densities come from the stochastic growth model treated {ref}`at the start of the lecture <solow_swan>`.\n",
    "\n",
    "Begin with the code found in [stochasticgrowth.py](https://github.com/QuantEcon/QuantEcon.lectures.code/blob/master/stationary_densities/stochasticgrowth.jl).\n",
    "\n",
    "Use the same parameters.\n",
    "\n",
    "For the four initial distributions, use the beta distribution and shift the random draws as shown below\n",
    "\n",
    "```{code-block} julia\n",
    "psi_0 = Beta(5.0, 5.0)  # Initial distribution\n",
    "n = 1000\n",
    "# .... more setup\n",
    "\n",
    "for i in 1:4\n",
    "    # .... some code\n",
    "    rand_draws = (rand(psi_0, n) .+ 2.5i) ./ 2\n",
    "```\n",
    "\n",
    "(statd_ex3)=\n",
    "### Exercise 3\n",
    "\n",
    "A common way to compare distributions visually is with [boxplots](https://en.wikipedia.org/wiki/Box_plot).\n",
    "\n",
    "To illustrate, let's generate three artificial data sets and compare them with a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580d546",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "Random.seed!(42); # For determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55555771",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "x = randn(n)        # N(0, 1)\n",
    "x = exp.(x)         # Map x to lognormal\n",
    "y = randn(n) .+ 2.0  # N(2, 1)\n",
    "z = randn(n) .+ 4.0  # N(4, 1)\n",
    "data = vcat(x, y, z)\n",
    "l = [L\"X\" L\"Y\" L\"Z\"]\n",
    "xlabels = reshape(repeat(l, n), 3n, 1)\n",
    "\n",
    "boxplot(xlabels, data, label = \"\", ylims = (-2, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48628174",
   "metadata": {},
   "source": [
    "The three data sets are\n",
    "\n",
    "$$\n",
    "\\{ X_1, \\ldots, X_n \\} \\sim LN(0, 1), \\;\\;\n",
    "\\{ Y_1, \\ldots, Y_n \\} \\sim N(2, 1), \\;\\;\n",
    "\\text{ and } \\;\n",
    "\\{ Z_1, \\ldots, Z_n \\} \\sim N(4, 1), \\;\n",
    "$$\n",
    "\n",
    "The figure looks as follows.\n",
    "\n",
    "Each data set is represented by a box, where the top and bottom of the box are the third and first quartiles of the data, and the red line in the center is the median.\n",
    "\n",
    "The boxes give some indication as to\n",
    "\n",
    "* the location of probability mass for each sample\n",
    "* whether the distribution is right-skewed (as is the lognormal distribution), etc\n",
    "\n",
    "Now let's put these ideas to use in a simulation.\n",
    "\n",
    "Consider the threshold autoregressive model in {eq}`statd_tar`.\n",
    "\n",
    "We know that the distribution of $X_t$ will converge to {eq}`statd_tar_ts` whenever $|\\theta| < 1$.\n",
    "\n",
    "Let's observe this convergence from different initial conditions using\n",
    "boxplots.\n",
    "\n",
    "In particular, the exercise is to generate J boxplot figures, one for each initial condition $X_0$ in\n",
    "\n",
    "```{code-block} julia\n",
    "initial_conditions = range(8, 0, length = J)\n",
    "```\n",
    "\n",
    "For each $X_0$ in this set,\n",
    "\n",
    "1. Generate $k$ time series of length $n$, each starting at $X_0$ and obeying {eq}`statd_tar`.\n",
    "2. Create a boxplot representing $n$ distributions, where the $t$-th distribution shows the $k$ observations of $X_t$.\n",
    "\n",
    "Use $\\theta = 0.9, n = 20, k = 5000, J = 8$.\n",
    "\n",
    "## Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a3b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelDensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12481a75",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Look ahead estimation of a TAR stationary density, where the TAR model\n",
    "is\n",
    "\n",
    "$$\n",
    "X_{t+1} = \\theta |X_t| + (1 - \\theta^2)^{1/2} \\xi_{t+1}\n",
    "$$\n",
    "\n",
    "and $\\xi_t \\sim N(0,1)$. Try running at n = 10, 100, 1000, 10000\n",
    "to get an idea of the speed of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befb00a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "Random.seed!(42);  # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = Normal()\n",
    "n = 500\n",
    "theta = 0.8\n",
    "d = sqrt(1.0 - theta^2)\n",
    "delta = theta / d\n",
    "\n",
    "# true density of TAR model\n",
    "psi_star(y) = 2 .* pdf.(phi, y) .* cdf.(phi, delta * y)\n",
    "\n",
    "# Stochastic kernel for the TAR model.\n",
    "p_TAR(x, y) = pdf.(phi, (y .- theta .* abs.(x)) ./ d) ./ d\n",
    "\n",
    "Z = rand(phi, n)\n",
    "X = zeros(n)\n",
    "for t in 1:(n - 1)\n",
    "    X[t + 1] = theta * abs(X[t]) + d * Z[t]\n",
    "end\n",
    "\n",
    "psi_est(a) = lae_est(p_TAR, X, a)\n",
    "k_est = kde(X)\n",
    "\n",
    "ys = range(-3, 3, length = 200)\n",
    "plot(ys, psi_star(ys), color = :blue, lw = 2, alpha = 0.6, label = \"true\")\n",
    "plot!(ys, psi_est(ys), color = :green, lw = 2, alpha = 0.6,\n",
    "      label = \"look ahead estimate\")\n",
    "plot!(k_est.x, k_est.density, color = :black, lw = 2, alpha = 0.6,\n",
    "      label = \"kernel based estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7658f55",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Solution 1 Tests\" begin\n",
    "    @test length(ys) == 200 && ys[1] ≈ -3.0 && ys[end] ≈ 3.0\n",
    "    @test X[7] ≈ 0.760849576105531\n",
    "    @test Z[3] ≈ -0.31498797116895605\n",
    "    @test mean(X) ≈ 0.5983214298825352\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6d3f7",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Here's one program that does the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08891e0b",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "Random.seed!(42);  # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.2\n",
    "delta = 0.1\n",
    "a_sigma = 0.4  # A = exp(B) where B ~ N(0, a_sigma)\n",
    "alpha = 0.4    # We set f(k) = k**alpha\n",
    "psi_0 = Beta(5.0, 5.0)  # Initial distribution\n",
    "phi = LogNormal(0.0, a_sigma)\n",
    "\n",
    "function p_growth(x, y)\n",
    "    # Stochastic kernel for the growth model with Cobb-Douglas production.\n",
    "    # Both x and y must be strictly positive.\n",
    "\n",
    "    d = s * x .^ alpha\n",
    "\n",
    "    pdf_arg = clamp.((y .- (1 - delta) .* x) ./ d, eps(), Inf)\n",
    "    return pdf.(phi, pdf_arg) ./ d\n",
    "end\n",
    "\n",
    "n = 1000  # Number of observations at each date t\n",
    "T = 40    # Compute density of k_t at 1,...,T+1\n",
    "\n",
    "xmax = 6.5\n",
    "ygrid = range(0.01, xmax, length = 150)\n",
    "laes_plot = zeros(length(ygrid), 4T)\n",
    "colors = zeros(RGBA,1,4*T);\n",
    "for i in 1:4\n",
    "    k = zeros(n, T)\n",
    "    A = rand!(phi, zeros(n, T))\n",
    "\n",
    "    # Draw first column from initial distribution\n",
    "    # match scale = 0.5 and loc = 2i in julia version\n",
    "    k[:, 1] = (rand(psi_0, n) .+ 2.5i) ./ 2\n",
    "    for t in 1:(T - 1)\n",
    "        k[:, t + 1] = s * A[:, t] .* k[:, t] .^ alpha + (1 - delta) .* k[:, t]\n",
    "    end\n",
    "\n",
    "    # Store draws for each date t\n",
    "    laes = [k[:, t] for t in T:-1:1]\n",
    "    ind = i\n",
    "    for j in 1:T\n",
    "        psi = laes[j]\n",
    "        laes_plot[:, ind] = lae_est(p_growth, psi, ygrid)\n",
    "        colors[ind]=RGBA(0, 0, 0, 1 - (j - 1) / T)\n",
    "        ind = ind + 4\n",
    "    end\n",
    "end\n",
    "\n",
    "plot(ygrid, laes_plot, layout = (2, 2), color = colors,\n",
    "     legend = :none, xlabel = \"capital\", xlims = (0, xmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58663016",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Solution 2 Tests\" begin\n",
    "    @test laes[3][4] ≈ 3.4544045546418998\n",
    "    @test length(ygrid) == 150 && ygrid[end] ≈ 6.5 && ygrid[1] ≈ 0.01\n",
    "    @test laes_plot[75, 80] ≈ 0.17910393584617842\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade001a2",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Here's a possible solution.\n",
    "\n",
    "Note the way we use vectorized code to simulate the $k$ time\n",
    "series for one boxplot all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1862a616",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "Random.seed!(42);  # reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ae170",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "k = 5000\n",
    "J = 6\n",
    "\n",
    "theta = 0.9\n",
    "d = sqrt(1 - theta^2)\n",
    "delta = theta / d\n",
    "\n",
    "initial_conditions = range(8, 0, length = J)\n",
    "\n",
    "Z = randn(k, n, J)\n",
    "titles = []\n",
    "data = []\n",
    "x_labels = []\n",
    "for j in 1:J\n",
    "    title = L\"time series from $t = %$(initial_conditions[j])$\"\n",
    "    push!(titles, title)\n",
    "\n",
    "    X = zeros(k, n)\n",
    "    X[:, 1] .= initial_conditions[j]\n",
    "    labels = []\n",
    "    labels = vcat(labels, ones(k, 1))\n",
    "    for t in 2:n\n",
    "        X[:, t] = theta .* abs.(X[:, t - 1]) .+ d .* Z[:, t, j]\n",
    "        labels = vcat(labels, t * ones(k, 1))\n",
    "    end\n",
    "    X = reshape(X, n * k, 1)\n",
    "    push!(data, X)\n",
    "    push!(x_labels, labels)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca498e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "for i in 1:J\n",
    "    push!(plots, boxplot(vec(x_labels[i]), vec(data[i]), title = titles[i]))\n",
    "end\n",
    "plot(plots..., layout = (J, 1), legend = :none, size = (800, 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801a531",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "(statd_appendix)=\n",
    "Here's the proof of {eq}`statd_dv`.\n",
    "\n",
    "Let $F_U$ and $F_V$ be the cumulative distributions of $U$ and $V$ respectively.\n",
    "\n",
    "By the definition of $V$, we have $F_V(v) = \\mathbb P \\{ a + b U \\leq v \\} = \\mathbb P \\{ U \\leq (v - a) / b \\}$.\n",
    "\n",
    "In other words, $F_V(v) = F_U ( (v - a)/b )$.\n",
    "\n",
    "Differentiating with respect to $v$ yields {eq}`statd_dv`."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia"
  },
  "source_map": [
   10,
   65,
   71,
   436,
   443,
   501,
   511,
   828,
   835,
   846,
   890,
   892,
   906,
   913,
   943,
   953,
   959,
   966,
   1017,
   1026,
   1035,
   1042,
   1075,
   1081
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}