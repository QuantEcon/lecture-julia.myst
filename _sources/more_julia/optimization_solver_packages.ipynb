{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3851f2b6",
   "metadata": {},
   "source": [
    "(optimization_solver_packages)=\n",
    "```{raw} html\n",
    "<div id=\"qe-notebook-header\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# Optimization and Nonlinear Solvers\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lecture we introduce a few of the Julia libraries for solving optimization problems, systems of equations, and finding fixed points.\n",
    "\n",
    "See {doc}`auto-differentiation <../more_julia/auto_differentiation>` for more on calculating gradients and Jacobians for these types of algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaee34b",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics, BenchmarkTools\n",
    "using ForwardDiff, Optim, Roots, NLsolve\n",
    "using FixedPointAcceleration, NonlinearSolve\n",
    "using Optimization, OptimizationOptimJL, ForwardDiff, Enzyme\n",
    "using Optim: converged, maximum, maximizer, minimizer, iterations #some extra functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc54b7",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "There are a large number of packages intended to be used for optimization in Julia.\n",
    "\n",
    "Part of the reason for the diversity of options is that Julia makes it possible to efficiently implement a large number of variations on optimization routines.\n",
    "\n",
    "The other reason is that different types of optimization problems require different algorithms.\n",
    "\n",
    "### Optim.jl\n",
    "\n",
    "A good pure-Julia solution for the (unconstrained or box-bounded) optimization of\n",
    "univariate and multivariate functions is the [Optim.jl](https://github.com/JuliaNLSolvers/Optim.jl) package.\n",
    "\n",
    "By default, the algorithms in `Optim.jl` target minimization rather than\n",
    "maximization, so if a function is called `optimize` it will mean minimization.\n",
    "\n",
    "#### Univariate Functions on Bounded Intervals\n",
    "\n",
    "[Univariate optimization](http://julianlsolvers.github.io/Optim.jl/stable/user/minimization/#minimizing-a-univariate-function-on-a-bounded-interval)\n",
    "defaults to a robust hybrid optimization routine called [Brent's method](https://en.wikipedia.org/wiki/Brent%27s_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim: converged, maximum, maximizer, minimizer, iterations #some extra functions\n",
    "\n",
    "result = optimize(x -> x^2, -2.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de31e5",
   "metadata": {},
   "source": [
    "Always check if the results converged, and throw errors otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382de9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged(result) ||\n",
    "    error(\"Failed to converge in $(iterations(result)) iterations\")\n",
    "xmin = result.minimizer\n",
    "result.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236de916",
   "metadata": {},
   "source": [
    "The first line is a logical OR between `converged(result)` and `error(\"...\")`.\n",
    "\n",
    "If the convergence check passes, the logical sentence is true, and it will proceed to the next line; if not, it will throw the error.\n",
    "\n",
    "#### Unconstrained Multivariate Optimization\n",
    "\n",
    "There are a variety of [algorithms and options](http://julianlsolvers.github.io/Optim.jl/stable/user/minimization/#_top) for multivariate optimization.\n",
    "\n",
    "From the documentation, the simplest version is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367b8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv) # i.e. optimize(f, x_iv, NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d830e0",
   "metadata": {},
   "source": [
    "The default algorithm is `NelderMead`, which is derivative-free and hence requires many function evaluations.\n",
    "\n",
    "To change the algorithm type to [L-BFGS](http://julianlsolvers.github.io/Optim.jl/stable/algo/lbfgs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17d6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = optimize(f, x_iv, LBFGS())\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in \" *\n",
    "        \"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b8c85",
   "metadata": {},
   "source": [
    "Note that this has fewer iterations.\n",
    "\n",
    "As no derivative was given, it used [finite differences](https://en.wikipedia.org/wiki/Finite_difference) to approximate the gradient of `f(x)`.\n",
    "\n",
    "However, since most of the algorithms require derivatives, you will often want to use automatic differentiation or pass analytical gradients if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05993cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv, LBFGS(), autodiff = :forward) # i.e. use ForwardDiff.jl\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in \" *\n",
    "        \"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed6786",
   "metadata": {},
   "source": [
    "Note that we did not need to use `ForwardDiff.jl` directly, as long as our `f(x)` function was written to be generic (see the {doc}`generic programming lecture <../more_julia/generic_programming>` ).\n",
    "\n",
    "Alternatively, with an analytical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "function g!(G, x)\n",
    "    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "    G[2] = 200.0 * (x[2] - x[1]^2)\n",
    "end\n",
    "\n",
    "results = optimize(f, g!, x_iv, LBFGS()) # or ConjugateGradient()\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in \" *\n",
    "        \"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9205adc",
   "metadata": {},
   "source": [
    "For derivative-free methods, you can change the algorithm -- and have no need to provide a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b50f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv, SimulatedAnnealing()) # or ParticleSwarm() or NelderMead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43919e5d",
   "metadata": {},
   "source": [
    "However, you will note that this did not converge, as stochastic methods typically require many more iterations as a tradeoff for their global convergence properties.\n",
    "\n",
    "See the [maximum likelihood](http://julianlsolvers.github.io/Optim.jl/stable/examples/generated/maxlikenlm/)\n",
    "example and the accompanying [Jupyter notebook](https://nbviewer.jupyter.org/github/JuliaNLSolvers/Optim.jl/blob/gh-pages/v0.15.3/examples/generated/maxlikenlm.ipynb).\n",
    "\n",
    "### JuMP.jl\n",
    "\n",
    "The [JuMP.jl](https://github.com/JuliaOpt/JuMP.jl) package is an ambitious implementation of a modelling language for optimization problems in Julia.\n",
    "\n",
    "In that sense, it is more like an AMPL (or Pyomo) built on top of the Julia\n",
    "language with macros, and able to use a variety of different commercial and open-source solvers.\n",
    "\n",
    "If you have a linear, quadratic, conic, mixed-integer linear, etc. problem then this will likely be the ideal \"meta-package\" for calling various solvers.\n",
    "\n",
    "For nonlinear problems, the modelling language may make things difficult for complicated functions (as it is not designed to be used as a general-purpose nonlinear optimizer).\n",
    "\n",
    "See the [quick start guide](http://www.juliaopt.org/JuMP.jl/0.18/quickstart.html) for more details on all of the options.\n",
    "\n",
    "The following is an example of calling a linear objective with a nonlinear constraint (provided by an external function).\n",
    "\n",
    "Here `Ipopt` stands for `Interior Point OPTimizer`, a [nonlinear solver](https://github.com/JuliaOpt/Ipopt.jl) in Julia\n",
    "\n",
    "```{code-block} julia\n",
    "using JuMP, Ipopt\n",
    "# solve\n",
    "# max( x[1] + x[2] )\n",
    "# st sqrt(x[1]^2 + x[2]^2) <= 1\n",
    "\n",
    "function squareroot(x) # pretending we don't know sqrt()\n",
    "    z = x # Initial starting point for Newtonâ€™s method\n",
    "    while abs(z * z - x) > 1e-13\n",
    "        z = z - (z * z - x) / (2z)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "m = Model(Ipopt.Optimizer)\n",
    "# need to register user defined functions for AD\n",
    "JuMP.register(m, :squareroot, 1, squareroot, autodiff = true)\n",
    "\n",
    "@variable(m, x[1:2], start=0.5) # start is the initial condition\n",
    "@objective(m, Max, sum(x))\n",
    "@NLconstraint(m, squareroot(x[1]^2 + x[2]^2)<=1)\n",
    "@show JuMP.optimize!(m)\n",
    "```\n",
    "\n",
    "And this is an example of a quadratic objective\n",
    "\n",
    "```{code-block} julia\n",
    "# solve\n",
    "# min (1-x)^2 + (100(y-x^2)^2)\n",
    "# st x + y >= 10\n",
    "\n",
    "using JuMP, Ipopt\n",
    "m = Model(Ipopt.Optimizer) # settings for the solver\n",
    "@variable(m, x, start=0.0)\n",
    "@variable(m, y, start=0.0)\n",
    "\n",
    "@NLobjective(m, Min, (1 - x)^2+100(y - x^2)^2)\n",
    "\n",
    "JuMP.optimize!(m)\n",
    "println(\"x = \", value(x), \" y = \", value(y))\n",
    "\n",
    "# adding a (linear) constraint\n",
    "@constraint(m, x + y==10)\n",
    "JuMP.optimize!(m)\n",
    "println(\"x = \", value(x), \" y = \", value(y))\n",
    "```\n",
    "\n",
    "### BlackBoxOptim.jl\n",
    "\n",
    "Another package for doing global optimization without derivatives is [BlackBoxOptim.jl](https://github.com/robertfeldt/BlackBoxOptim.jl).\n",
    "\n",
    "\n",
    "An example for [parallel execution](https://github.com/robertfeldt/BlackBoxOptim.jl/blob/master/examples/rosenbrock_parallel.jl) of the objective is provided.\n",
    "\n",
    "## Optimization.jl Meta-Package\n",
    "The [Optimization.jl](https://github.com/SciML/Optimization.jl) package provides a common interface to a variety of optimization packages in Julia.  As part of the [SciML](https://sciml.ai/) ecosystem, it is designed to work seamlessly with other SciML tools, and to provide differentiable optimization routines that can be used in conjunction with automatic differentiation packages.\n",
    "\n",
    "Algorithms require loading additional packages for one of the [supported optimizers](https://docs.sciml.ai/Optimization/stable/#Overview-of-the-solver-packages-in-alphabetical-order), such as `OptimizationOptimJL.jl`, which wraps the `Optim.jl` package.\n",
    "\n",
    "From the [documentation](https://docs.sciml.ai/Optimization/stable/getting_started/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2\n",
    "u0 = zeros(2)\n",
    "p = [1.0, 100.0]\n",
    "prob = OptimizationProblem(rosenbrock, u0, p)\n",
    "sol = solve(prob, NelderMead())\n",
    "sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7eb32f",
   "metadata": {},
   "source": [
    "The separation of the argument, `u`, and the parameters, `p`, is common in SciML and provides methods to cleanly handle parameterized problems.\n",
    "\n",
    "Function wrappers also provide easy integration with automatic differentiation such as `ForwardDiff.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb150914",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_fd = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())\n",
    "prob = OptimizationProblem(f_fd, u0, p)\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae932bc0",
   "metadata": {},
   "source": [
    "Or with `Enzyme.jl`, which has slower compilation times but provides another AD backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_enzyme = OptimizationFunction(rosenbrock, Optimization.AutoEnzyme())\n",
    "prob = OptimizationProblem(f_enzyme, u0, p)\n",
    "sol = solve(prob, BFGS())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0dac3",
   "metadata": {},
   "source": [
    "Finally, [the documentation](https://docs.sciml.ai/Optimization/stable/) for a variety of other examples and features, such as constrained optimization.\n",
    "\n",
    "\n",
    "## NonlinearSolve.jl Meta-Package\n",
    "Within the [SciML](https://sciml.ai/) ecosystem, the [NonlinearSolve.jl](https://github.com/SciML/NonlinearSolve.jl) package provides a unified interface for solving nonlinear systems of equations. It builds on top of other SciML packages and offers advanced features such as automatic differentiation, various solver algorithms, and support for large-scale problems.  Furthermore, as with `Optimization.jl`, it has convenient integration with automatic differentiation (as well as implementing AD for the solver itself, with respect to parameters).\n",
    "\n",
    "In general, we suggest using this meta-package where possible as it provides a well-maintained interface amenable to switching out solvers in the future. \n",
    "\n",
    "### Basic Examples\n",
    "\n",
    "Here we adapt examples directly from the [documentation](https://docs.sciml.ai/NonlinearSolve/stable/).\n",
    "\n",
    "First we can solve a system of equations as a closure over a constant `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2.0\n",
    "f(u, p) = u .* u .- c # p ignored\n",
    "u0 = [1.0, 1.0]\n",
    "prob = NonlinearProblem(f, u0) # defaults to p = nothing\n",
    "sol = solve(prob, NewtonRaphson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c5dad",
   "metadata": {},
   "source": [
    "In this case, the `p` argument (which can hold parameters) is ignored, but needs to be present in SciML problems.\n",
    "\n",
    "The `NewtonRaphson()` method is a built-in solver, and not an external package.  We can access the results through `sol.u` and see other information such as the return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@show sol.u\n",
    "@show sol.retcode\n",
    "sol.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad94c246",
   "metadata": {},
   "source": [
    "We can see further details on the algorithm itself, and see that it uses `ForwardDiff.jl` by default since `NewtonRaphson()` requires a Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.alg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c3d2c",
   "metadata": {},
   "source": [
    "You can see the performance of this algorithm in this context, which uses a large number of allocations relative to the simplicity of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark solve($prob, NewtonRaphson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729563e9",
   "metadata": {},
   "source": [
    "### Using Parameters and Inplace Functions\n",
    "SciML interfaces prefer to cleanly separate a function argument and a parameter argument, which makes it easier to handle parameterized problems and ensure flexible code.\n",
    "\n",
    "To use this, we rewrite our example above to use the previously ignored parameter `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeeea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(u, p) = u .* u .- p\n",
    "u0 = [1.0, 1.0]\n",
    "p = 2.0\n",
    "prob = NonlinearProblem(f, u0, p) # pass the `p`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738186f2",
   "metadata": {},
   "source": [
    "Note that the `prob` shows In-place: true.\n",
    "\n",
    "Benchmarking this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime solve($prob, NewtonRaphson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d353b",
   "metadata": {},
   "source": [
    "This may or may not have better performance than the previous version which used the closure.\n",
    "\n",
    "Regardless, it will still have many allocations, which can be a significant fraction of the runtime for problems small or large.\n",
    "\n",
    "To support this, the SciML ecosystem has a bias towards in-place functions.  Here we add another version of the function `f!(du, u, p)` to be in-place, modifying the first argument, which the solver detects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bd378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function f!(du, u, p)\n",
    "    du .= u .* u .- p\n",
    "end\n",
    "prob = NonlinearProblem(f!, u0, p)\n",
    "@btime solve($prob, NewtonRaphson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f736edbf",
   "metadata": {},
   "source": [
    "Note the decrease in allocations, and possibly time depending on your system.\n",
    "\n",
    "### Patterns for Parameters\n",
    "\n",
    "The `p` argument can be anything, including named tuples, vectors, or a single value as above.\n",
    "\n",
    "A common pattern appropriate for more complicated code is to use a named tuple packing and unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function f_nt(u, p)\n",
    "    (; c) = p # unpack any arguments in the named tuple\n",
    "    return u .* u .- c\n",
    "end\n",
    "\n",
    "p_nt = (c = 2.0,) # named tuple\n",
    "prob_nt = NonlinearProblem(f_nt, u0, p_nt)\n",
    "@btime solve($prob_nt, NewtonRaphson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae0c7b7",
   "metadata": {},
   "source": [
    "### Small Systems\n",
    "For small systems, the solver is able to use [StaticArrays.jl](https://github.com/JuliaArrays/StaticArrays.jl) which can further improve performance.  These fixed-size arrays require using the out-of-place formulation, but otherwise require no special handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using StaticArrays\n",
    "f_SA(u, p) = SA[u[1] * u[1] - p, u[2] * u[2] - p] # rewrote without broadcasting\n",
    "u0_static = SA[1.0, 1.0] # static array\n",
    "prob = NonlinearProblem(f_SA, u0_static, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8340504",
   "metadata": {},
   "source": [
    "Note that the problem shows that this is `In-place: false` and operates on the `SVector{2, Float64}` type.\n",
    "\n",
    "Next we can benchmark this version, where we can use a simpler non-allocating solver designed for small systems, `SimpleNewtonRaphson()`.  See [the docs](https://docs.sciml.ai/NonlinearSolve/stable/native/simplenonlinearsolve/) for more details on solvers optimized for non-allocating algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d294666",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime solve($prob, SimpleNewtonRaphson())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec76fc",
   "metadata": {},
   "source": [
    "Depending on your system, you may find this is 100 to 1000x faster than the original version using closures and allocations.\n",
    "\n",
    "```{note}\n",
    "In principle it should be equivalent to use our previous `f` instead of `f_SA`, but depending on package and compiler versions it may not achieve the full 0 allocations.  Regardless, try the generic function first before porting over to a new function.  The `SimpleNewtonRaphson()` function is also specialized to avoid any internal allocations for small systems, but you will even see some benefits with `NewtonRaphson()`.\n",
    "```\n",
    "\n",
    "### Using Other Solvers\n",
    "While it has some pre-built solvers, as we used above, it is primarily intended as a meta-package.  To use one of the many [external solver packages](https://docs.sciml.ai/NonlinearSolve/stable/solvers/nonlinear_system_solvers/) you simply need to ensure the package is in your Project file and then include it.\n",
    "\n",
    "For example, we can use the Anderson Acceleration in [FixedPointAcceleration.jl's wrapper](https://docs.sciml.ai/NonlinearSolve/stable/api/fixedpointacceleration/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c664cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using FixedPointAcceleration\n",
    "prob = NonlinearProblem(f, u0, p)\n",
    "@btime solve($prob, FixedPointAccelerationJL())\n",
    "@btime solve($prob, FixedPointAccelerationJL(; algorithm = :Simple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832edb68",
   "metadata": {},
   "source": [
    "The default algorithm in this case is `:Anderson`, but we can also use `:Simple` fixed-point iteration.\n",
    "\n",
    "In this case, the fixed-point iteration algorithm is slower than the gradient-based Newton-Raphson, but shows the flexibility in cases where gradients are not available.\n",
    "\n",
    "```{note}\n",
    "While there is a [NLsolve.jl wrapper](https://docs.sciml.ai/NonlinearSolve/stable/api/nlsolve/), it seems to have a less robust implementation of Anderson than some of the other packages.\n",
    "```\n",
    "\n",
    "### Bracketing Solvers and Rootfinding Problems\n",
    "\n",
    "The `NonlinearProblem` type is intended for algorithms which use an initial condition (and may or may not use derivatives).\n",
    "\n",
    "For some one-dimensional problems it is more convenient to use one of the [bracketing methods](https://docs.sciml.ai/NonlinearSolve/stable/solvers/bracketing_solvers/#bracketing)\n",
    "\n",
    "For example, from the [documentation](https://docs.sciml.ai/NonlinearSolve/stable/tutorials/getting_started/#Problem-Type-2:-Solving-Interval-Rootfinding-Problems-with-Bracketing-Methods),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d97e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_bracketed(u, p) = u * u - p\n",
    "uspan = (1.0, 2.0) # brackets\n",
    "p = 2.0\n",
    "prob = IntervalNonlinearProblem(f_bracketed, uspan, p)\n",
    "sol = solve(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3d62f",
   "metadata": {},
   "source": [
    "In general, this should be preferred to using the `Roots.jl` package below, as it may provide a more consistent interface.\n",
    "\n",
    "\n",
    "## Systems of Equations and Fixed Points\n",
    "\n",
    "Julia has a variety of packages you can directly use for solving systems of equations and finding fixed points.  Many of these packages can also be used as backends for the `NonlinearSolve.jl` meta-package described above.\n",
    "\n",
    "\n",
    "### Roots.jl\n",
    "\n",
    "A root of a real function $f$ on $[a,b]$ is an $x \\in [a, b]$ such that $f(x)=0$.\n",
    "\n",
    "For example, if we plot the function\n",
    "\n",
    "```{math}\n",
    ":label: root_f\n",
    "\n",
    "f(x) = \\sin(4 (x - 1/4)) + x + x^{20} - 1\n",
    "```\n",
    "\n",
    "with $x \\in [0,1]$ we get\n",
    "\n",
    "```{figure} /_static/figures/sine-screenshot-2.png\n",
    "\n",
    "```\n",
    "\n",
    "The unique root is approximately 0.408.\n",
    "\n",
    "The [Roots.jl](https://github.com/JuliaLang/Roots.jl) package offers `fzero()` to find roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bf3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Roots\n",
    "f(x) = sin(4 * (x - 1 / 4)) + x + x^20 - 1\n",
    "fzero(f, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a4100",
   "metadata": {},
   "source": [
    "### NLsolve.jl\n",
    "\n",
    "The [NLsolve.jl](https://github.com/JuliaNLSolvers/NLsolve.jl/) package provides functions to solve for multivariate systems of equations and fixed points.\n",
    "\n",
    "From the documentation, to solve for a system of equations without providing a Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb208e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLsolve\n",
    "\n",
    "f(x) = [(x[1] + 3) * (x[2]^3 - 7) + 18\n",
    "        sin(x[2] * exp(x[1]) - 1)] # returns an array\n",
    "\n",
    "results = nlsolve(f, [0.1; 1.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea2571",
   "metadata": {},
   "source": [
    "In the above case, the algorithm used finite differences to calculate the Jacobian.\n",
    "\n",
    "Alternatively, if `f(x)` is written generically, you can use auto-differentiation with a single setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nlsolve(f, [0.1; 1.2], autodiff = :forward)\n",
    "\n",
    "println(\"converged=$(NLsolve.converged(results)) at root=$(results.zero) in \" *\n",
    "        \"$(results.iterations) iterations and $(results.f_calls) function calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1a7475",
   "metadata": {},
   "source": [
    "Providing a function which operates inplace (i.e., modifies an argument) may help performance for large systems of equations (and hurt it for small ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2a4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "function f!(F, x) # modifies the first argument\n",
    "    F[1] = (x[1] + 3) * (x[2]^3 - 7) + 18\n",
    "    F[2] = sin(x[2] * exp(x[1]) - 1)\n",
    "end\n",
    "\n",
    "results = nlsolve(f!, [0.1; 1.2], autodiff = :forward)\n",
    "\n",
    "println(\"converged=$(NLsolve.converged(results)) at root=$(results.zero) in \" *\n",
    "        \"$(results.iterations) iterations and $(results.f_calls) function calls\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia-1.12"
  },
  "source_map": [
   10,
   34,
   43,
   66,
   70,
   74,
   79,
   91,
   95,
   101,
   105,
   113,
   119,
   125,
   136,
   140,
   144,
   228,
   235,
   241,
   245,
   249,
   253,
   269,
   275,
   280,
   284,
   289,
   291,
   295,
   297,
   304,
   309,
   315,
   317,
   325,
   331,
   341,
   350,
   356,
   361,
   367,
   369,
   382,
   387,
   405,
   411,
   443,
   447,
   455,
   462,
   468,
   473,
   477
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}