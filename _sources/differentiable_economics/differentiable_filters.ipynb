{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c711b98a",
   "metadata": {},
   "source": [
    "(diff_filters)=\n",
    "```{raw} html\n",
    "<div id=\"qe-notebook-header\" style=\"text-align:right;\">\n",
    "        <a href=\"https://quantecon.org/\" title=\"quantecon.org\">\n",
    "                <img style=\"width:250px;display:inline;\" src=\"https://assets.quantecon.org/img/qe-menubar-logo.svg\" alt=\"QuantEcon\">\n",
    "        </a>\n",
    "</div>\n",
    "```\n",
    "\n",
    "# Differentiable Filters\n",
    "\n",
    "```{index} single: Differentiable Filters\n",
    "```\n",
    "\n",
    "```{contents} Contents\n",
    ":depth: 2\n",
    "```\n",
    "\n",
    "## Overview\n",
    "\n",
    "Many problems in economics involve state-space models with unobserved states and noisy observations. For example, when estimating dynamic models an econometrician might have an unobserved TFP process and only observe noisy functions of aggregates such as capital and consumption from national accounts. Or within a model, an agent might have an unobserved state such as a belief about the state of the world, or a hidden match quality they are trying to learn -- as in {cite}`Jovanovic1979matching`'s model of job matching and turnover, {cite}`Jovanovic1982`'s model of firm selection and industry dynamics, and {cite}`Moscarini2005`'s equilibrium search model with learning about match quality.\n",
    "\n",
    "The mathematical problem of estimating these unobserved states is called \"filtering\", where the {doc}`Kalman filter <../introduction_dynamics/kalman>` is the optimal filter given certain assumptions (e.g., a linear state-space model with Gaussian shocks).\n",
    "\n",
    "One immediate application is estimation: the Kalman filter also computes the likelihood of a linear state-space model given observed data. If we can differentiate through the filter, we unlock gradient-based methods for estimation -- maximum likelihood via algorithms such as L-BFGS, Bayesian inference via [Hamiltonian Monte Carlo (HMC)](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) or [variational inference](https://en.wikipedia.org/wiki/Variational_inference), and sensitivity analysis of filtered states with respect to model parameters. These techniques are used in {cite}`dssm`.\n",
    "\n",
    "This lecture builds on {doc}`differentiable dynamics <differentiable_dynamics>` (differentiable simulation) and the {doc}`Kalman filter <../introduction_dynamics/kalman>` (filter theory). See {doc}`auto-differentiation <../more_julia/auto_differentiation>` for background on Enzyme.\n",
    "\n",
    "Along the way, you will learn several patterns for writing high-performance numerical code that is both generic and AD-compatible:\n",
    "\n",
    "* The **bang-bang (`!!`) convention** -- a single algorithm that works with both heap-allocated mutable arrays (for large problems) and stack-allocated `StaticArrays` (for dramatic speedups on small problems)\n",
    "* **Prototype-based allocation** for writing generic code that infers array types from inputs\n",
    "* A **nonlinear state-space simulator** with callback-based transitions and observations, compatible with Enzyme\n",
    "* A **high-performance zero-allocation Kalman filter** with forward- and reverse-mode AD, enabling gradient-based estimation of all model parameters in a single sweep\n",
    "* Examples of auto-differentiation of more complicated functions, such as in-place solutions to linear systems (i.e., `ldiv!`) and matrix factorizations (i.e., `cholesky!`), which are common in many likelihoods\n",
    "\n",
    "```{caution}\n",
    "The code in this lecture is significantly more advanced than some of the other lectures, and requires some experience with both auto-differentiation concepts and a more detailed understanding of type-safety and memory management in Julia.\n",
    "\n",
    "Enzyme.jl is under active development and while state-of-the-art, it is often bleeding-edge. Some of the patterns shown here may change in future releases. See {doc}`auto-differentiation <../more_julia/auto_differentiation>` for the latest best practices.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bef5c",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "using LinearAlgebra, Random, Plots, Test, Enzyme, Statistics, RecursiveArrayTools\n",
    "using BenchmarkTools, EnzymeTestUtils, StaticArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be856c45",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "using Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420477c",
   "metadata": {},
   "source": [
    "## The Bang-Bang Pattern for Generic Arrays\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The best coding strategy for numerical algorithms depends on the size of the problem. Different array types and mutation patterns are appropriate at different scales.\n",
    "\n",
    "For large vectors and matrices, allocation of intermediate arrays can be a major bottleneck. In these cases, we want to write **in-place code** that mutates pre-allocated buffers to achieve zero allocations and maximum performance -- see {ref}`in-place-functions` for background. Writing zero-allocation code is also essential in practice for compatibility with Enzyme.\n",
    "\n",
    "For small fixed-size problems, `StaticArrays` (stack-allocated, no heap) can be dramatically faster than standard `Vector`/`Matrix`. The [rule of thumb](https://juliaarrays.github.io/StaticArrays.jl/stable/) is that StaticArrays are beneficial when the **total number of elements** is under about 100 -- so an `SVector{10}` or an `SMatrix{3,3}` are good candidates, while an `SMatrix{20,20}` (400 elements) is not. The biggest wins (10x--100x) come at very small sizes (under ~20 elements); beyond 100 elements, compilation costs explode and the stack/register advantages disappear.\n",
    "\n",
    "The challenge is that these two approaches seem incompatible: standard in-place operations (`mul!`, `ldiv!`, `copyto!`) only work with mutable arrays, while StaticArrays are immutable and require returning new values. Ideally, we want to write a **single algorithm** that works with both array types -- choosing the optimal strategy for each problem size without duplicating code.\n",
    "\n",
    "### The `!!` Convention\n",
    "\n",
    "One approach is a pattern called the \"bang-bang\" convention.\n",
    "\n",
    "A function `f!!` **always returns its result** and **tries to mutate in-place when possible**.\n",
    "\n",
    "It checks if a type can be modified directly with `ismutable(Y)` and then modifies in place if possible. For example, if mutable it might call `mul!(Y, A, B)` and return `Y`; if immutable it would simply return `A * B`.\n",
    "\n",
    "This makes the natural data structure **arrays of arrays** (e.g., `Vector{SVector{N}}`) rather than 2D matrices, since each element can be either mutable or immutable.\n",
    "\n",
    "### Key `!!` Utilities\n",
    "\n",
    "| Function | Mutable path | Immutable path | Use case |\n",
    "|----------|-------------|----------------|----------|\n",
    "| `mul!!(Y, A, B)` | `mul!(Y,A,B); return Y` | `return A*B` | Matrix/vector multiply |\n",
    "| `mul!!(Y, A, B, α, β)` | `mul!(Y,A,B,α,β); return Y` | `return α*(A*B) + β*Y` | Accumulate: `Y = αAB + βY` |\n",
    "| `muladd!!(Y, A, B)` | `mul!(Y,A,B,1.0,1.0); return Y` | `return Y + A*B` | `Y += A*B` |\n",
    "| `copyto!!(Y, X)` | `copyto!(Y,X); return Y` | `return X` | Copy data |\n",
    "| `assign!!(Y, X)` | loop `Y[i]=X[i]; return Y` | `return X` | Enzyme-safe copy (avoids `Base.copyto!`) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f85a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function mul!!(Y, A, B)\n",
    "    if ismutable(Y)\n",
    "        mul!(Y, A, B)\n",
    "        return Y\n",
    "    else\n",
    "        return A * B\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function mul!!(Y, A, B, α, β)\n",
    "    if ismutable(Y)\n",
    "        mul!(Y, A, B, α, β)\n",
    "        return Y\n",
    "    else\n",
    "        return α * (A * B) + β * Y\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function muladd!!(Y, A, B)\n",
    "    if ismutable(Y)\n",
    "        mul!(Y, A, B, 1.0, 1.0)\n",
    "        return Y\n",
    "    else\n",
    "        return Y + A * B\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f68f36",
   "metadata": {},
   "source": [
    "We also define no-op specializations for `nothing` arguments. This lets us write generic code that handles optional components -- for example, a model with or without observation noise -- using a single code path. If `H = nothing`, then `muladd!!(y, H, v)` simply returns `y` unchanged without any branching at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline muladd!!(Y, ::Nothing, B) = Y\n",
    "@inline muladd!!(Y, A, ::Nothing) = Y\n",
    "@inline muladd!!(Y, ::Nothing, ::Nothing) = Y\n",
    "\n",
    "@inline function copyto!!(Y, X)\n",
    "    if ismutable(Y)\n",
    "        copyto!(Y, X)\n",
    "        return Y\n",
    "    else\n",
    "        return X\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function assign!!(Y, X)\n",
    "    if ismutable(Y)\n",
    "        @inbounds for i in eachindex(X)\n",
    "            Y[i] = X[i]\n",
    "        end\n",
    "        return Y\n",
    "    else\n",
    "        return X\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759681ec",
   "metadata": {},
   "source": [
    "`assign!!` in this pattern helps to avoid `copyto!!` since Enzyme can have issues with runtime activity analysis errors when overwriting an initial condition in an output buffer.\n",
    "\n",
    "### Prototype-Based Allocation\n",
    "\n",
    "When writing generic code, we often need to allocate workspace arrays that match the type family of the inputs -- mutable `Vector`/`Matrix` or immutable `SVector`/`SMatrix`. Rather than maintaining separate allocation functions for each case, we use **prototype-based allocation**: pass an existing array as a template, and `alloc_like` creates a new zeroed array of the same type. The `SVector`/`SMatrix` overloads are necessary because Julia's `similar` returns a mutable `MVector`/`MMatrix` for static arrays, which would lose immutability and stack allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a78bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same shape as prototype\n",
    "@inline alloc_like(x::AbstractArray) = similar(x)\n",
    "@inline alloc_like(::SVector{N, T}) where {N, T} = zeros(SVector{N, T})\n",
    "@inline alloc_like(::SMatrix{N, M, T}) where {N, M, T} = zeros(SMatrix{N, M, T})\n",
    "\n",
    "# Different dimensions, same type family\n",
    "@inline alloc_like(x::AbstractArray, dims::Int...) = similar(x, dims...)\n",
    "@inline alloc_like(::SVector{<:Any, T}, n::Int) where {T} = zeros(SVector{n, T})\n",
    "@inline alloc_like(::SMatrix{<:Any, <:Any, T}, n::Int,\n",
    "                   m::Int) where {T} = zeros(SMatrix{n, m, T})\n",
    "@inline alloc_like(::SMatrix{<:Any, <:Any, T},\n",
    "                   n::Int) where {T} = zeros(SVector{n, T})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15f0c8",
   "metadata": {},
   "source": [
    "Similarly, `fill_zero!!` zeroes an array generically -- in-place for mutable, returning a new zeroed value for immutable. This is needed to reset workspace caches between AD calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0274c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline fill_zero!!(::SVector{N, T}) where {N, T} = zeros(SVector{N, T})\n",
    "@inline fill_zero!!(::SMatrix{N, M, T}) where {N, M,\n",
    "                                               T} = zeros(SMatrix{N, M, T})\n",
    "@inline function fill_zero!!(x::AbstractArray{T}) where {T}\n",
    "    fill!(x, zero(T))\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3785768",
   "metadata": {},
   "source": [
    "## High-Performance Nonlinear State-Space Simulation\n",
    "\n",
    "Here we will make a variation on the simulation in {doc}`differentiable dynamics <differentiable_dynamics>`, but with a more flexible interface that supports arbitrary nonlinear state transitions and observation functions via callbacks, and is compatible with Enzyme.jl for AD.\n",
    "\n",
    "### Arrays-of-Arrays Storage\n",
    "\n",
    "With the `!!` pattern, the natural data structure is `Vector{Vector{Float64}}` or `Vector{SVector{N,Float64}}`. This enables a single simulation function for both mutable and static arrays.\n",
    "\n",
    "### The `simulate_ssm!` Function\n",
    "\n",
    "The model is\n",
    "\n",
    "$$\n",
    "x_{t+1} = f(x_t, w_{t+1}, p, t), \\qquad y_t = g(x_t, p, t) + H v_t, \\quad v_t \\sim N(0, I)\n",
    "$$\n",
    "\n",
    "where $f$ and $g$ can be **arbitrary nonlinear functions**, but the observation noise is assumed **additive Gaussian** with $v_t \\sim N(0, I)$.\n",
    "\n",
    "To implement this for both static and preallocated arrays, the state transition callback `f!!(x_next, x, w, p, t)` implements the full $f(\\cdot)$ (including process noise), while the observation callback `g!!(y, x, p, t)` implements only the deterministic component $g(\\cdot)$. The simulator adds the Gaussian observation noise $H v_t$ separately via `muladd!!`. Both callbacks follow the `!!` convention -- they attempt to mutate the first argument in-place but always return the result. Passing `H = nothing` drops the observation noise entirely thanks to the no-op specializations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function simulate_ssm!(x, y, f!!, g!!, x_0, w, v, H, p)\n",
    "    T = length(w)\n",
    "\n",
    "    # Initialize first state\n",
    "    x[1] = assign!!(x[1], x_0)\n",
    "\n",
    "    @inbounds for t in 1:T\n",
    "        x[t + 1] = f!!(x[t + 1], x[t], w[t], p, t - 1) \n",
    "        y[t] = g!!(y[t], x[t], p, t - 1)\n",
    "        y[t] = muladd!!(y[t], H, v[t])\n",
    "    end\n",
    "\n",
    "    # Final observation\n",
    "    y[T + 1] = g!!(y[T + 1], x[T + 1], p, T)\n",
    "    y[T + 1] = muladd!!(y[T + 1], H, v[T + 1])\n",
    "\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d5080",
   "metadata": {},
   "source": [
    "### Linear State-Space Callbacks\n",
    "\n",
    "As a concrete example, we define callbacks for the linear state-space model. The state transition callback implements $f(x_t, w_{t+1}) = Ax_t + Cw_{t+1}$, while the observation callback implements only the noiseless part $g(x_t) = Gx_t$. The full observation is $y_t = g(x_t) + Hv_t = Gx_t + Hv_t$, where the $Hv_t$ term is added by `simulate_ssm!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d666ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function f_lss!!(x_p, x, w, p, t)\n",
    "    x_p = mul!!(x_p, p.A, x)\n",
    "    return muladd!!(x_p, p.C, w)\n",
    "end\n",
    "\n",
    "@inline function g_lss!!(y, x, p, t)\n",
    "    return mul!!(y, p.G, x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa0c44",
   "metadata": {},
   "source": [
    "### Example: Small Linear State-Space Model\n",
    "\n",
    "We set up a small $N=2$ model with two coupled states and two observables. The transition matrix $A$ has off-diagonal terms, so both states interact and move jointly. Both states receive independent shocks through $C$, and both are directly observed through $G = I$ with small measurement noise $H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "\n",
    "A = [0.9 0.1; -0.1 0.8]    # coupled states: cross-feedback\n",
    "C = [0.1 0.0; 0.0 0.1]     # independent shocks to each state\n",
    "G = [1.0 0.0; 0.0 1.0]     # both states observed directly\n",
    "H = [0.05 0.0; 0.0 0.05]   # small observation noise\n",
    "model = (; A, C, G, H)\n",
    "\n",
    "N = size(A, 1)     # state dimension\n",
    "M = size(G, 1)     # observation dimension\n",
    "K = size(C, 2)     # state noise dimension\n",
    "L = size(H, 2)     # observation noise dimension\n",
    "T = 50\n",
    "\n",
    "x_0 = zeros(N)\n",
    "\n",
    "# Generate noise sequences (arrays of arrays)\n",
    "w = [randn(K) for _ in 1:T]\n",
    "v = [randn(L) for _ in 1:(T + 1)]\n",
    "\n",
    "# Allocate output arrays using prototypes\n",
    "x = [alloc_like(x_0) for _ in 1:(T + 1)]\n",
    "y = [alloc_like(x_0, M) for _ in 1:(T + 1)]\n",
    "\n",
    "simulate_ssm!(x, y, f_lss!!, g_lss!!, x_0, w, v, H, model)\n",
    "\n",
    "time = 0:T\n",
    "plot(time, [x[t][1] for t in 1:(T + 1)], lw = 2, label = \"x₁\",\n",
    "     xlabel = \"t\", ylabel = \"state\", title = \"State Paths\")\n",
    "plot!(time, [x[t][2] for t in 1:(T + 1)], lw = 2, label = \"x₂\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6879c3c",
   "metadata": {},
   "source": [
    "### RecursiveArrayTools.jl\n",
    "\n",
    "With this pattern, accessing and slicing arrays of arrays can be burdensome.\n",
    "\n",
    "The `RecursiveArrayTools.jl` package helps by wrapping the arrays in a `VectorOfArray` type, which provides convenient indexing and slicing while preserving the underlying array types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e643a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = VectorOfArray(x)\n",
    "y_arr = VectorOfArray(y)\n",
    "\n",
    "@show x_arr[:,1], x_arr[1,1]\n",
    "@show x_arr[1, 5:10];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb9ffa",
   "metadata": {},
   "source": [
    "### Running with StaticArrays\n",
    "\n",
    "The same model wrapped in `SMatrix`/`SVector` produces identical results but runs on the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_s = SMatrix{N, N}(A)\n",
    "C_s = SMatrix{N, K}(C)\n",
    "G_s = SMatrix{M, N}(G)\n",
    "H_s = SMatrix{M, L}(H)\n",
    "model_s = (; A = A_s, C = C_s, G = G_s, H = H_s)\n",
    "\n",
    "x_0_s = SVector{N}(x_0)\n",
    "w_s = [SVector{K}(w[t]) for t in 1:T]\n",
    "v_s = [SVector{L}(v[t]) for t in 1:(T + 1)]\n",
    "x_s = [alloc_like(x_0_s) for _ in 1:(T + 1)]\n",
    "y_s = [alloc_like(x_0_s, M) for _ in 1:(T + 1)]\n",
    "\n",
    "simulate_ssm!(x_s, y_s, f_lss!!, g_lss!!, x_0_s, w_s, v_s, H_s, model_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a26218",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Mutable vs Static simulation consistency\" begin\n",
    "    @test all(isapprox(x_s[t], x[t]; rtol = 1e-12) for t in 1:(T + 1))\n",
    "    @test all(isapprox(y_s[t], y[t]; rtol = 1e-12) for t in 1:(T + 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e610d96",
   "metadata": {},
   "source": [
    "### Benchmarks\n",
    "\n",
    "Depending on your system, you will find a significant speedup for this small model when using `StaticArrays` -- often 10x or more, and neither version should allocate any memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_ssm!($x, $y, f_lss!!, g_lss!!, $x_0, $w, $v, $H, $model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d83b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime simulate_ssm!($x_s, $y_s, f_lss!!, g_lss!!, $x_0_s, $w_s, $v_s, $H_s,\n",
    "                     $model_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec3478",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Simulation zero allocations\" begin\n",
    "    simulate_ssm!(x, y, f_lss!!, g_lss!!, x_0, w, v, H, model)\n",
    "    @test (@allocated simulate_ssm!(x, y, f_lss!!, g_lss!!, x_0, w, v, H, model)) == 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289feb62",
   "metadata": {},
   "source": [
    "### Forward-Mode AD: Perturbing the Shocks\n",
    "\n",
    "For the linear model $x_{t+1} = Ax_t + Cw_{t+1}$ starting from $x_1 = x_0$, a unit perturbation $\\delta w_1 = e_k$ (the $k$-th standard basis vector) propagates through the dynamics as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_{t+1}}{\\partial w_{1,k}} = A^{t-1} C\\, e_k, \\qquad t \\geq 1\n",
    "$$\n",
    "\n",
    "This is the **impulse response function** -- it shows how a one-time shock decays through the system. Forward-mode AD computes exactly these derivatives: by seeding $dw_1 = e_k$ and propagating tangents forward, the output `dx[t]` gives $\\partial x_t / \\partial w_{1,k}$ at every horizon.\n",
    "\n",
    "Every argument to `autodiff` must be annotated to tell Enzyme how to handle it (see {ref}`enzyme-activity-rules`). Here `w` is `Duplicated` because we are differentiating with respect to it, and `dw` holds the seed tangent. The buffers `x` and `y` are also `Duplicated` because `simulate_ssm!` **mutates** them -- Enzyme needs shadow arrays `dx` and `dy` to propagate tangents through those writes, even though `x` and `y` are intermediate storage rather than parameters of interest. Arguments that are neither differentiated nor mutated (like the model coefficients) are wrapped in `Const`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ee15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = Enzyme.make_zero(x)\n",
    "dy = Enzyme.make_zero(y)\n",
    "dw = Enzyme.make_zero(w)\n",
    "dw[1][1] = 1.0  # unit perturbation to first shock element\n",
    "\n",
    "autodiff(Forward, simulate_ssm!,\n",
    "         Duplicated(x, dx), Duplicated(y, dy),\n",
    "         Const(f_lss!!), Const(g_lss!!), Const(x_0),\n",
    "         Duplicated(w, dw), Const(v), Const(H), Const(model))\n",
    "\n",
    "plot(time, [dx[t][1] for t in 1:(T + 1)], lw = 2, label = \"dx₁/dw₁₁\",\n",
    "     xlabel = \"t\", ylabel = \"sensitivity\",\n",
    "     title = \"Impulse Response via Forward AD\")\n",
    "plot!(time, [dx[t][2] for t in 1:(T + 1)], lw = 2, label = \"dx₂/dw₁₁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c22a0",
   "metadata": {},
   "source": [
    "### Reverse-Mode AD: Sensitivity of the Terminal Observation\n",
    "\n",
    "Reverse mode is best when computing the gradient of a **scalar** output with respect to **all** inputs in a single sweep, regardless of input dimension. Here the scalar is the first element of the final observation $y_{T+1,1}$ -- a \"terminal observable\" whose sensitivity tells us how each initial condition, shock, and model parameter affects this single outcome.\n",
    "\n",
    "We wrap the simulation in a self-contained function that allocates its own workspace, calls `simulate_ssm!`, and returns $y_{T+1,1}$. The callbacks `f!!` and `g!!` are passed as keyword arguments to keep them out of the differentiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5047dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "function terminal_observable(x_0, w, v, H, p; f!! = f_lss!!, g!! = g_lss!!)\n",
    "    T_s = length(w)\n",
    "    y_proto = alloc_like(x_0, size(H, 1))\n",
    "    x = [alloc_like(x_0) for _ in 1:(T_s + 1)]\n",
    "    y = [alloc_like(y_proto) for _ in 1:(T_s + 1)]\n",
    "    simulate_ssm!(x, y, f!!, g!!, x_0, w, v, H, p)\n",
    "    return y[end][1]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf25e0",
   "metadata": {},
   "source": [
    "Because `terminal_observable` returns an `Active` scalar, Enzyme's reverse mode will back-propagate through `simulate_ssm!` and accumulate gradients in the `Duplicated` shadow arrays we provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec0351",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_0 = Enzyme.make_zero(x_0)\n",
    "dw = Enzyme.make_zero(w)\n",
    "dmodel = Enzyme.make_zero(model)\n",
    "\n",
    "autodiff(Reverse, terminal_observable, Active,\n",
    "         Duplicated(x_0, dx_0), Duplicated(w, dw),\n",
    "         Const(v), Const(H), Duplicated(model, dmodel))\n",
    "\n",
    "println(\"∂y[T+1]₁/∂x_0:       \", dx_0)\n",
    "println(\"∂y[T+1]₁/∂w[1]:      \", dw[1])\n",
    "println(\"∂y[T+1]₁/∂A:         \", dmodel.A)\n",
    "println(\"∂y[T+1]₁/∂G:         \", dmodel.G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de56bd9",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Reverse AD terminal observable\" begin\n",
    "    @test all(isfinite, dx_0)\n",
    "    @test any(!iszero, dx_0)\n",
    "    @test all(isfinite, dw[1])\n",
    "    @test any(!iszero, dw[1])\n",
    "    @test all(isfinite, dmodel.A)\n",
    "    @test any(!iszero, dmodel.A)\n",
    "    @test all(isfinite, dmodel.G)\n",
    "    @test any(!iszero, dmodel.G)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e18059",
   "metadata": {},
   "source": [
    "## The Kalman Filter\n",
    "\n",
    "### Additional `!!` Utilities\n",
    "\n",
    "Before the filter, we need utilities for Cholesky factorization, linear solves, transposition, symmetrization, and log-determinants. Each serves a specific purpose in the filter:\n",
    "\n",
    "- **`cholesky!!(A, :U)`** -- Cholesky factorization of innovation covariance $S_t$\n",
    "- **`ldiv!!(y, F, x)`** and **`ldiv!!(F, x)`** -- solving $S_t^{-1} \\nu_t$ for the log-likelihood and Kalman gain (the 2-arg form avoids internal allocation)\n",
    "- **`transpose!!(Y, X)`** -- computing $K_t$ via $S_t K_t^{\\top} = (\\hat\\Sigma_t G^{\\top})^{\\top}$\n",
    "- **`symmetrize_upper!!(L, A, epsilon)`** -- enforcing exact symmetry before Cholesky (numerical drift in $\\hat\\Sigma_t$ can break it); the `epsilon` diagonal perturbation ensures positive definiteness\n",
    "- **`logdet_chol(F)`** -- allocation-free log-determinant from Cholesky factor\n",
    "\n",
    "| Function | Mutable path | Immutable path |\n",
    "|----------|-------------|----------------|\n",
    "| `cholesky!!(A, :U)` | `cholesky!(Symmetric(A,:U), NoPivot())` | `cholesky(Symmetric(A,:U))` |\n",
    "| `ldiv!!(y, F, x)` | `ldiv!(y,F,x); return y` | `return F \\ x` |\n",
    "| `ldiv!!(F, x)` | `ldiv!(F,x); return x` | `return F \\ x` |\n",
    "| `transpose!!(Y, X)` | `transpose!(Y,X); return Y` | `return transpose(X)` |\n",
    "| `symmetrize_upper!!(L, A, epsilon)` | element-wise loop, zero lower | `(A+A')/2 + epsilon*I` |\n",
    "| `logdet_chol(F)` | `2*sum(log(diag(F.U)))` | same |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function cholesky!!(A, uplo::Symbol = :U)\n",
    "    if ismutable(A)\n",
    "        return cholesky!(Symmetric(A, uplo), NoPivot(); check = false)\n",
    "    else\n",
    "        return cholesky(Symmetric(A, uplo))\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function ldiv!!(y, F, x)\n",
    "    if ismutable(y)\n",
    "        ldiv!(y, F, x)\n",
    "        return y\n",
    "    else\n",
    "        return F \\ x\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function ldiv!!(F, x)\n",
    "    if ismutable(x)\n",
    "        ldiv!(F, x)\n",
    "        return x\n",
    "    else\n",
    "        return F \\ x\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function transpose!!(Y, X)\n",
    "    if ismutable(Y)\n",
    "        transpose!(Y, X)\n",
    "        return Y\n",
    "    else\n",
    "        return transpose(X)\n",
    "    end\n",
    "end\n",
    "\n",
    "@inline function logdet_chol(F)\n",
    "    U = F.U\n",
    "    result = zero(eltype(U))\n",
    "    @inbounds for i in axes(U, 1)\n",
    "        result += log(U[i, i])\n",
    "    end\n",
    "    return 2 * result\n",
    "end\n",
    "\n",
    "@noinline function symmetrize_upper!!(L, A, epsilon = 0.0)\n",
    "    if ismutable(L)\n",
    "        @inbounds for j in axes(A, 2)\n",
    "            for i in 1:j\n",
    "                v = (A[i, j] + A[j, i]) * 0.5\n",
    "                L[i, j] = (i == j) ? v + epsilon : v\n",
    "            end\n",
    "            for i in (j + 1):size(A, 1)\n",
    "                L[i, j] = 0\n",
    "            end\n",
    "        end\n",
    "        return L\n",
    "    else\n",
    "        sym = (A + A') / 2\n",
    "        if epsilon != 0\n",
    "            return sym + epsilon * one(A)\n",
    "        else\n",
    "            return sym\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121c427",
   "metadata": {},
   "source": [
    "### State-Space Model\n",
    "\n",
    "The linear state-space model (see {doc}`linear models <../introduction_dynamics/linear_models>`) is\n",
    "\n",
    "$$\n",
    "x_{t+1} = A x_t + C w_{t+1}, \\quad w_t \\sim N(0, I)\n",
    "$$\n",
    "$$\n",
    "y_t = G x_t + H v_t, \\quad v_t \\sim N(0, I)\n",
    "$$\n",
    "\n",
    "### Kalman Filter Recursion\n",
    "\n",
    "The Kalman filter (see the {doc}`Kalman filter <../introduction_dynamics/kalman>` lecture) estimates the hidden state $x_t$ from noisy observations $\\{y_1, \\ldots, y_T\\}$. Written in predict-update form matching our code:\n",
    "\n",
    "**Predict:**\n",
    "\n",
    "$$\n",
    "\\hat\\mu_t = A \\mu_t, \\qquad \\hat\\Sigma_t = A \\Sigma_t A^{\\top} + CC^{\\top}\n",
    "$$\n",
    "\n",
    "**Innovation:**\n",
    "\n",
    "$$\n",
    "\\nu_t = y_t - G\\hat\\mu_t, \\qquad S_t = G\\hat\\Sigma_t G^{\\top} + HH^{\\top}\n",
    "$$\n",
    "\n",
    "**Update:**\n",
    "\n",
    "$$\n",
    "K_t = \\hat\\Sigma_t G^{\\top} S_t^{-1}, \\quad \\mu_{t+1} = \\hat\\mu_t + K_t\\nu_t, \\quad \\Sigma_{t+1} = \\hat\\Sigma_t - K_t G \\hat\\Sigma_t\n",
    "$$\n",
    "\n",
    "### Log-Likelihood\n",
    "\n",
    "Under the Gaussian assumption, the innovation $\\nu_t = y_t - G\\hat\\mu_t$ is distributed as\n",
    "\n",
    "$$\n",
    "\\nu_t \\sim N(0, S_t), \\qquad S_t = G\\hat\\Sigma_t G^{\\top} + HH^{\\top}\n",
    "$$\n",
    "\n",
    "so the log-density of $\\nu_t$ under the multivariate normal $N(0, S_t)$ is\n",
    "\n",
    "$$\n",
    "\\log p(\\nu_t) = -\\frac{1}{2}\\bigl(M \\log 2\\pi + \\log|S_t| + \\nu_t^{\\top} S_t^{-1} \\nu_t\\bigr)\n",
    "$$\n",
    "\n",
    "Summing over observations gives the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ell = \\sum_{t=1}^T \\log p(\\nu_t) = -\\frac{1}{2}\\sum_{t=1}^T \\bigl(M\\log 2\\pi + \\log|S_t| + \\nu_t^{\\top} S_t^{-1}\\nu_t\\bigr)\n",
    "$$\n",
    "\n",
    "In the code, we compute $\\log|S_t|$ from its Cholesky factor $S_t = U^{\\top}U$ via `logdet_chol(F)`, which sums $2\\sum_i \\log U_{ii}$ without any allocation. The quadratic form $\\nu_t^{\\top} S_t^{-1} \\nu_t$ is computed by first solving $S_t^{-1}\\nu_t$ with `ldiv!!`, then taking the inner product with `dot`.\n",
    "\n",
    "### Workspace Cache\n",
    "\n",
    "Zero-allocation code requires preallocating **all** intermediate buffers. This is especially important for AD since mutations must be tracked. Using `alloc_like`, a single allocation function handles both mutable and static arrays -- it infers all types and dimensions from the prototypes `mu_0`, `Sigma_0`, and `model.G`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e091ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "function alloc_kalman_cache(mu_0, Sigma_0, model, T)\n",
    "    N = size(Sigma_0, 1)\n",
    "    M = size(model.G, 1)\n",
    "    return (;\n",
    "            mu_pred = [alloc_like(mu_0) for _ in 1:T],\n",
    "            sigma_pred = [alloc_like(Sigma_0) for _ in 1:T],\n",
    "            A_sigma = [alloc_like(Sigma_0) for _ in 1:T],\n",
    "            sigma_Gt = [alloc_like(Sigma_0, N, M) for _ in 1:T],\n",
    "            innovation = [alloc_like(mu_0, M) for _ in 1:T],\n",
    "            innovation_cov = [alloc_like(Sigma_0, M, M) for _ in 1:T],\n",
    "            S_chol = [alloc_like(Sigma_0, M, M) for _ in 1:T],\n",
    "            innovation_solved = [alloc_like(mu_0, M) for _ in 1:T],\n",
    "            gain_rhs = [alloc_like(model.G) for _ in 1:T],\n",
    "            gain = [alloc_like(Sigma_0, N, M) for _ in 1:T],\n",
    "            gainG = [alloc_like(Sigma_0) for _ in 1:T],\n",
    "            KgSigma = [alloc_like(Sigma_0) for _ in 1:T],\n",
    "            mu_update = [alloc_like(mu_0) for _ in 1:T])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b00e1",
   "metadata": {},
   "source": [
    "Since the cache is reused across calls, it must be zeroed at the start of each filter run to prevent gradient accumulation during AD. Using `fill_zero!!`, the same loop handles both mutable and immutable arrays without branching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9731540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "function zero_kalman_cache!!(cache)\n",
    "    @inbounds for t in 1:length(cache.mu_pred)\n",
    "        cache.mu_pred[t] = fill_zero!!(cache.mu_pred[t])\n",
    "        cache.sigma_pred[t] = fill_zero!!(cache.sigma_pred[t])\n",
    "        cache.A_sigma[t] = fill_zero!!(cache.A_sigma[t])\n",
    "        cache.sigma_Gt[t] = fill_zero!!(cache.sigma_Gt[t])\n",
    "        cache.innovation[t] = fill_zero!!(cache.innovation[t])\n",
    "        cache.innovation_cov[t] = fill_zero!!(cache.innovation_cov[t])\n",
    "        cache.S_chol[t] = fill_zero!!(cache.S_chol[t])\n",
    "        cache.innovation_solved[t] = fill_zero!!(cache.innovation_solved[t])\n",
    "        cache.gain_rhs[t] = fill_zero!!(cache.gain_rhs[t])\n",
    "        cache.gain[t] = fill_zero!!(cache.gain[t])\n",
    "        cache.gainG[t] = fill_zero!!(cache.gainG[t])\n",
    "        cache.KgSigma[t] = fill_zero!!(cache.KgSigma[t])\n",
    "        cache.mu_update[t] = fill_zero!!(cache.mu_update[t])\n",
    "    end\n",
    "    return cache\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3d2ff",
   "metadata": {},
   "source": [
    "### The `kalman!` Function\n",
    "\n",
    "The full Kalman filter implementation. Inline comments reference the math equations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "function kalman!(mu, Sigma, y, mu_0, Sigma_0, model, cache;\n",
    "                 perturb_diagonal = 1e-8)\n",
    "    (; A, C, G, H) = model\n",
    "    T = length(y)\n",
    "\n",
    "    # Zero cache buffers for Enzyme AD compatibility\n",
    "    zero_kalman_cache!!(cache)\n",
    "\n",
    "    # Initialize: μ₁ = μ₀, Σ₁ = Σ₀\n",
    "    # Use assign!! (not copyto!!) to avoid aliasing between Sigma[1] and Sigma_0,\n",
    "    # which would prevent Enzyme from differentiating through Sigma_0.\n",
    "    mu[1] = assign!!(mu[1], mu_0)\n",
    "    Sigma[1] = assign!!(Sigma[1], Sigma_0)\n",
    "\n",
    "    loglik = zero(eltype(mu[1]))\n",
    "    is_mutable = ismutable(mu[1])\n",
    "\n",
    "    @inbounds for t in 1:T\n",
    "        μt = mu[t]\n",
    "        Σt = Sigma[t]\n",
    "\n",
    "        # Unpack cache buffers for this timestep\n",
    "        μp = cache.mu_pred[t]\n",
    "        Σp = cache.sigma_pred[t]\n",
    "        AΣ = cache.A_sigma[t]\n",
    "        ΣGt = cache.sigma_Gt[t]\n",
    "        ν = cache.innovation[t]\n",
    "        S = cache.innovation_cov[t]\n",
    "        S_chol_buf = cache.S_chol[t]\n",
    "        ν_solved = cache.innovation_solved[t]\n",
    "        rhs = cache.gain_rhs[t]\n",
    "        K = cache.gain[t]\n",
    "        KG = cache.gainG[t]\n",
    "        KGS = cache.KgSigma[t]\n",
    "        μu = cache.mu_update[t]\n",
    "\n",
    "        # Predict mean: μ̂_t = A μ_t\n",
    "        μp = mul!!(μp, A, μt)\n",
    "\n",
    "        # Predict covariance: Σ̂_t = A Σ_t A' + CC'\n",
    "        AΣ = mul!!(AΣ, A, Σt)\n",
    "        Σp = mul!!(Σp, AΣ, transpose(A))\n",
    "        Σp = muladd!!(Σp, C, transpose(C))\n",
    "\n",
    "        # Innovation: ν_t = y_t - G μ̂_t\n",
    "        ν = copyto!!(ν, y[t])\n",
    "        ν = mul!!(ν, G, μp, -1.0, 1.0)\n",
    "\n",
    "        # Innovation covariance: S_t = G Σ̂_t G' + HH'\n",
    "        ΣGt = mul!!(ΣGt, Σp, transpose(G))\n",
    "        S = mul!!(S, G, ΣGt)\n",
    "        S = muladd!!(S, H, transpose(H))\n",
    "\n",
    "        # Symmetrize and Cholesky factorize S_t\n",
    "        S_chol_buf = symmetrize_upper!!(S_chol_buf, S, perturb_diagonal)\n",
    "        F = cholesky!!(S_chol_buf, :U)\n",
    "\n",
    "        # Kalman gain: K_t = Σ̂_t G' S_t⁻¹\n",
    "        # Solve S_t K_t' = (Σ̂_t G')' for K_t\n",
    "        rhs = transpose!!(rhs, ΣGt)\n",
    "        rhs = ldiv!!(F, rhs)\n",
    "        K = transpose!!(K, rhs)\n",
    "\n",
    "        # Update mean: μ_{t+1} = μ̂_t + K_t ν_t\n",
    "        # Mutable: mu[t+1] aliases a heap array -- write elements directly.\n",
    "        # Immutable: μp and μu are new SVector values on the stack returned by\n",
    "        # the !! functions. Write them back to the cache so Enzyme's reverse\n",
    "        # pass can track them (otherwise the cache still holds stale zeros).\n",
    "        μu = mul!!(μu, K, ν)\n",
    "        if is_mutable\n",
    "            for i in eachindex(μp)\n",
    "                mu[t + 1][i] = μp[i] + μu[i]\n",
    "            end\n",
    "        else\n",
    "            cache.mu_pred[t] = μp\n",
    "            cache.mu_update[t] = μu\n",
    "            mu[t + 1] = μp + μu\n",
    "        end\n",
    "\n",
    "        # Update covariance: Σ_{t+1} = Σ̂_t - K_t G Σ̂_t\n",
    "        # Same mutable/immutable cache-writeback logic as mean update above.\n",
    "        KG = mul!!(KG, K, G)\n",
    "        KGS = mul!!(KGS, KG, Σp)\n",
    "        if is_mutable\n",
    "            for i in eachindex(Σp)\n",
    "                Sigma[t + 1][i] = Σp[i] - KGS[i]\n",
    "            end\n",
    "        else\n",
    "            cache.sigma_pred[t] = Σp\n",
    "            cache.KgSigma[t] = KGS\n",
    "            Sigma[t + 1] = Σp - KGS\n",
    "        end\n",
    "\n",
    "        # Log-likelihood contribution\n",
    "        ν_solved = ldiv!!(ν_solved, F, ν)\n",
    "        logdetS = logdet_chol(F)\n",
    "        M_obs = length(ν)\n",
    "        quad = dot(ν_solved, ν)\n",
    "        loglik -= 0.5 * (M_obs * log(2π) + logdetS + quad)\n",
    "    end\n",
    "\n",
    "    return loglik\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1bfb8",
   "metadata": {},
   "source": [
    "### Running the Filter\n",
    "\n",
    "We generate synthetic observations from the model and run `kalman!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae997f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic observations using simulate_ssm!\n",
    "Random.seed!(123)\n",
    "T_kf = 20\n",
    "mu_0 = zeros(N)\n",
    "Sigma_0 = Matrix{Float64}(I, N, N)\n",
    "\n",
    "x_0_kf = mu_0 + cholesky(Sigma_0).L * randn(N)\n",
    "w_kf = [randn(K) for _ in 1:T_kf]\n",
    "v_kf = [randn(L) for _ in 1:(T_kf + 1)]\n",
    "\n",
    "x_true = [alloc_like(mu_0) for _ in 1:(T_kf + 1)]\n",
    "y_sim = [alloc_like(mu_0, M) for _ in 1:(T_kf + 1)]\n",
    "simulate_ssm!(x_true, y_sim, f_lss!!, g_lss!!, x_0_kf, w_kf, v_kf, H, model)\n",
    "y_obs = y_sim[1:T_kf]\n",
    "\n",
    "# Run our kalman! filter\n",
    "mu_kf = [alloc_like(mu_0) for _ in 1:(T_kf + 1)]\n",
    "Sigma_kf = [alloc_like(Sigma_0) for _ in 1:(T_kf + 1)]\n",
    "cache_kf = alloc_kalman_cache(mu_0, Sigma_0, model, T_kf)\n",
    "\n",
    "loglik = kalman!(mu_kf, Sigma_kf, y_obs, mu_0, Sigma_0, model, cache_kf)\n",
    "println(\"Log-likelihood: \", loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e2090",
   "metadata": {},
   "source": [
    "### Plot: Filtered vs True States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7305a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_kf = 0:T_kf\n",
    "plot(time_kf, [x_true[t][1] for t in 1:(T_kf + 1)], lw = 2,\n",
    "     label = \"true state x₁\",\n",
    "     xlabel = \"t\", ylabel = \"state\", title = \"Filtered vs True States\",\n",
    "     ls = :dash)\n",
    "plot!(time_kf, [mu_kf[t][1] for t in 1:(T_kf + 1)], lw = 2,\n",
    "      label = \"filtered μ₁\")\n",
    "plot!(time_kf, [x_true[t][2] for t in 1:(T_kf + 1)], lw = 2,\n",
    "      label = \"true state x₂\",\n",
    "      ls = :dash)\n",
    "plot!(time_kf, [mu_kf[t][2] for t in 1:(T_kf + 1)], lw = 2,\n",
    "      label = \"filtered μ₂\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef842f",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "In this section we check important performance characteristics of our Kalman filter implementation, including type stability, zero allocations, and speed benchmarks. We also compare the mutable version against a static version using `StaticArrays` to see the tradeoffs in speed and flexibility.\n",
    "\n",
    "### Type Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60daafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inferred kalman!(mu_kf, Sigma_kf, y_obs, mu_0, Sigma_0, model, cache_kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402031ba",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Kalman zero allocations\" begin\n",
    "    kalman!(mu_kf, Sigma_kf, y_obs, mu_0, Sigma_0, model, cache_kf)\n",
    "    @test (@allocated kalman!(mu_kf, Sigma_kf, y_obs, mu_0, Sigma_0, model, cache_kf)) == 0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65e6e9",
   "metadata": {},
   "source": [
    "### Benchmarks: Small Static vs Mutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime kalman!($mu_kf, $Sigma_kf, $y_obs, $mu_0, $Sigma_0, $model, $cache_kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static version -- alloc_like infers SVector/SMatrix from prototypes\n",
    "mu_0_s = SVector{N}(mu_0)\n",
    "Sigma_0_s = SMatrix{N, N}(Sigma_0)\n",
    "y_obs_s = [SVector{M}(y_obs[t]) for t in 1:T_kf]\n",
    "mu_kf_s = [alloc_like(mu_0_s) for _ in 1:(T_kf + 1)]\n",
    "Sigma_kf_s = [alloc_like(Sigma_0_s) for _ in 1:(T_kf + 1)]\n",
    "cache_kf_s = alloc_kalman_cache(mu_0_s, Sigma_0_s, model_s, T_kf)\n",
    "\n",
    "@btime kalman!($mu_kf_s, $Sigma_kf_s, $y_obs_s, $mu_0_s, $Sigma_0_s, $model_s,\n",
    "               $cache_kf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adff130",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Static vs mutable Kalman consistency\" begin\n",
    "    loglik_mut = kalman!(mu_kf, Sigma_kf, y_obs, mu_0, Sigma_0, model, cache_kf)\n",
    "    loglik_sta = kalman!(mu_kf_s, Sigma_kf_s, y_obs_s, mu_0_s, Sigma_0_s, model_s, cache_kf_s)\n",
    "    @test loglik_mut ≈ loglik_sta rtol = 1e-10\n",
    "    for t in 1:(T_kf + 1)\n",
    "        @test mu_kf[t] ≈ Vector(mu_kf_s[t]) rtol = 1e-10\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10274012",
   "metadata": {},
   "source": [
    "### Benchmark: Larger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "N_big, M_big, K_big, L_big, T_big = 5, 2, 5, 2, 100\n",
    "\n",
    "A_big_raw = randn(N_big, N_big)\n",
    "A_big = 0.5 * A_big_raw / maximum(abs.(eigvals(A_big_raw)))\n",
    "C_big = 0.1 * randn(N_big, K_big)\n",
    "G_big = randn(M_big, N_big)\n",
    "H_big = 0.1 * randn(M_big, L_big)\n",
    "model_big = (; A = A_big, C = C_big, G = G_big, H = H_big)\n",
    "\n",
    "mu_0_big = zeros(N_big)\n",
    "Sigma_0_big = Matrix{Float64}(I, N_big, N_big)\n",
    "\n",
    "y_big = [randn(M_big) for _ in 1:T_big]\n",
    "mu_big = [alloc_like(mu_0_big) for _ in 1:(T_big + 1)]\n",
    "Sigma_big = [alloc_like(Sigma_0_big) for _ in 1:(T_big + 1)]\n",
    "cache_big = alloc_kalman_cache(mu_0_big, Sigma_0_big, model_big, T_big)\n",
    "\n",
    "@btime kalman!($mu_big, $Sigma_big, $y_big, $mu_0_big, $Sigma_0_big,\n",
    "               $model_big, $cache_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678edb9",
   "metadata": {},
   "source": [
    "## Differentiating the Kalman Filter\n",
    "\n",
    "### Forward Mode: Sensitivity of Prior Mean\n",
    "\n",
    "We perturb the first element of $\\mu_0$ by a unit tangent and observe how the filtered means respond over time. As with the simulation, every mutated buffer (`mu`, `Sigma`, `cache`, `y`) needs a shadow array for Enzyme to propagate tangents through the writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c1a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_fwd = [alloc_like(mu_0) for _ in 1:(T_kf + 1)]\n",
    "Sigma_fwd = [alloc_like(Sigma_0) for _ in 1:(T_kf + 1)]\n",
    "cache_fwd = alloc_kalman_cache(mu_0, Sigma_0, model, T_kf)\n",
    "dmu_fwd = Enzyme.make_zero(mu_fwd)\n",
    "dSigma_fwd = Enzyme.make_zero(Sigma_fwd)\n",
    "dcache_fwd = Enzyme.make_zero(cache_fwd)\n",
    "dy_fwd = Enzyme.make_zero(y_obs)\n",
    "\n",
    "dmu_0_fwd = zeros(N)\n",
    "dmu_0_fwd[1] = 1.0  # seed: perturb first state element\n",
    "\n",
    "autodiff(Forward, kalman!,\n",
    "         Duplicated(mu_fwd, dmu_fwd),\n",
    "         Duplicated(Sigma_fwd, dSigma_fwd),\n",
    "         Duplicated(y_obs, dy_fwd),\n",
    "         Duplicated(mu_0, dmu_0_fwd),\n",
    "         Const(Sigma_0),\n",
    "         Const(model),\n",
    "         Duplicated(cache_fwd, dcache_fwd))\n",
    "\n",
    "plot(time_kf, [dmu_fwd[t][1] for t in 1:(T_kf + 1)], lw = 2, label = \"dμ₁/dμ₀₁\",\n",
    "     xlabel = \"t\", ylabel = \"sensitivity\",\n",
    "     title = \"Sensitivity of Filtered Mean to Prior μ₀[1]\")\n",
    "plot!(time_kf, [dmu_fwd[t][2] for t in 1:(T_kf + 1)], lw = 2,\n",
    "      label = \"dμ₂/dμ₀₁\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76a59a8",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Forward AD Kalman\" begin\n",
    "    @test any(!iszero, dmu_fwd[2])\n",
    "    @test all(t -> all(isfinite, dmu_fwd[t]), 1:(T_kf + 1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20b5a3",
   "metadata": {},
   "source": [
    "### Reverse Mode: Gradient of Log-Likelihood\n",
    "\n",
    "Reverse mode gives gradients of the log-likelihood with respect to **all** model parameters and initial conditions in a single sweep, regardless of parameter dimension. This is what makes gradient-based MLE practical.\n",
    "\n",
    "As with `terminal_observable`, we wrap `kalman!` in a self-contained function that allocates its own workspace and returns the scalar log-likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d61892",
   "metadata": {},
   "outputs": [],
   "source": [
    "function kalman_loglik(y, mu_0, Sigma_0, model)\n",
    "    T_k = length(y)\n",
    "    mu = [alloc_like(mu_0) for _ in 1:(T_k + 1)]\n",
    "    Sigma = [alloc_like(Sigma_0) for _ in 1:(T_k + 1)]\n",
    "    cache = alloc_kalman_cache(mu_0, Sigma_0, model, T_k)\n",
    "    return kalman!(mu, Sigma, y, mu_0, Sigma_0, model, cache)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b0415",
   "metadata": {},
   "source": [
    "Because `kalman_loglik` returns an `Active` scalar, Enzyme back-propagates through `kalman!` and accumulates gradients in the `Duplicated` shadow arrays. We use the larger $N=5$ model for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758f7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(456)\n",
    "y_big_obs = [randn(M_big) for _ in 1:T_big]\n",
    "\n",
    "dmu_0_rev = Enzyme.make_zero(mu_0_big)\n",
    "dmodel_rev = Enzyme.make_zero(model_big)\n",
    "\n",
    "autodiff(Reverse, kalman_loglik, Active,\n",
    "         Const(y_big_obs), Duplicated(mu_0_big, dmu_0_rev),\n",
    "         Const(Sigma_0_big), Duplicated(model_big, dmodel_rev))\n",
    "\n",
    "println(\"∂ℓ/∂μ₀:             \", round.(dmu_0_rev; digits = 4))\n",
    "println(\"∂ℓ/∂A (first row):   \", round.(dmodel_rev.A[1, :]; digits = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d3a12e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "@testset \"Reverse AD Kalman\" begin\n",
    "    @test sum(abs, dmu_0_rev) > 0\n",
    "    @test !any(isnan, dmu_0_rev)\n",
    "    @test sum(abs, dmodel_rev.A) > 0\n",
    "    @test !any(isnan, dmodel_rev.A)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12011fbb",
   "metadata": {},
   "source": [
    "### AD Correctness: EnzymeTestUtils\n",
    "\n",
    "We validate forward and reverse modes against finite differences using `test_forward` and `test_reverse` on a small ($N=2$, $T=2$) model.\n",
    "\n",
    "Enzyme is in active development and bugs in AD can occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6921bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test, M_test, T_test = 2, 2, 2\n",
    "\n",
    "A_test = [0.8 0.1; -0.1 0.7]\n",
    "C_test = [0.1 0.0; 0.0 0.1]\n",
    "G_test = [1.0 0.0; 0.0 1.0]\n",
    "H_test = [0.1 0.0; 0.0 0.1]\n",
    "model_test = (; A = A_test, C = C_test, G = G_test, H = H_test)\n",
    "\n",
    "mu_0_test = zeros(N_test)\n",
    "Sigma_0_test = Matrix{Float64}(I, N_test, N_test)\n",
    "y_test = [[0.5, 0.3], [0.2, 0.1]]\n",
    "\n",
    "mu_et = [alloc_like(mu_0_test) for _ in 1:(T_test + 1)]\n",
    "Sigma_et = [alloc_like(Sigma_0_test) for _ in 1:(T_test + 1)]\n",
    "cache_et = alloc_kalman_cache(mu_0_test, Sigma_0_test, model_test, T_test)\n",
    "\n",
    "test_forward(kalman!, Const,\n",
    "             (mu_et, Duplicated),\n",
    "             (Sigma_et, Duplicated),\n",
    "             (y_test, Duplicated),\n",
    "             (mu_0_test, Duplicated),\n",
    "             (copy(Sigma_0_test), Const),\n",
    "             (model_test, Duplicated),\n",
    "             (cache_et, Duplicated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0874fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reverse(kalman!, Const,\n",
    "             (mu_et, Duplicated),\n",
    "             (Sigma_et, Duplicated),\n",
    "             (y_test, Duplicated),\n",
    "             (mu_0_test, Duplicated),\n",
    "             (copy(Sigma_0_test), Const),\n",
    "             (model_test, Duplicated),\n",
    "             (cache_et, Duplicated))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia-1.12"
  },
  "source_map": [
   10,
   54,
   62,
   67,
   103,
   130,
   134,
   158,
   166,
   179,
   183,
   191,
   213,
   232,
   238,
   247,
   253,
   284,
   292,
   298,
   304,
   320,
   328,
   334,
   338,
   343,
   351,
   365,
   380,
   388,
   397,
   401,
   416,
   430,
   453,
   519,
   580,
   599,
   603,
   622,
   628,
   732,
   738,
   761,
   765,
   778,
   786,
   790,
   798,
   802,
   806,
   819,
   831,
   835,
   856,
   864,
   892,
   900,
   908,
   916,
   920,
   935,
   945,
   953,
   980
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}