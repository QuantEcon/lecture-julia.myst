
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>19. Numerical Linear Algebra and Factorizations &#8212; Quantitative Economics with Julia</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js"></script>
    <link rel="canonical" href="https://julia.quantecon.org/tools_and_techniques/numerical_linear_algebra.html" />
    <link rel="shortcut icon" href="../_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="20. Krylov Methods and Matrix Conditioning" href="iterative_methods_sparsity.html" />
    <link rel="prev" title="18. Continuous State Markov Chains" href="stationary_densities.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-54984338-8', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/qe-logo-large.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Quantitative Economics with Julia</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started with Julia
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/getting_started.html">
   1. Setting up Your Julia Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/julia_by_example.html">
   2. Introductory Examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/julia_essentials.html">
   3. Julia Essentials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/fundamental_types.html">
   4. Arrays, Tuples, Ranges, and Other Fundamental Types
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started_julia/introduction_to_types.html">
   5. Introduction to Types and Generic Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Package Ecosystem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../more_julia/generic_programming.html">
   6. Generic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../more_julia/general_packages.html">
   7. General Purpose Packages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../more_julia/data_statistical_packages.html">
   8. Data and Statistics Packages
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../more_julia/optimization_solver_packages.html">
   9. Solvers, Optimizers, and Automatic Differentiation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Software Engineering
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../software_engineering/tools_editors.html">
   10. Visual Studio Code and Other Tools
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../software_engineering/version_control.html">
   11. GitHub, Version Control and Collaboration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../software_engineering/testing.html">
   12. Packages, Testing, and Continuous Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../software_engineering/need_for_speed.html">
   13. The Need for Speed
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="geom_series.html">
   14. Geometric Series for Elementary Economics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   15. Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="orth_proj.html">
   16. Orthogonal Projections and Their Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lln_clt.html">
   17. LLN and CLT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stationary_densities.html">
   18. Continuous State Markov Chains
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   19. Numerical Linear Algebra and Factorizations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="iterative_methods_sparsity.html">
   20. Krylov Methods and Matrix Conditioning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Dynamics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/scalar_dynam.html">
   21. Dynamics in One Dimension
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/ar1_processes.html">
   22. AR1 Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/finite_markov.html">
   23. Finite Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/linear_models.html">
   24. Linear State Space Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/wealth_dynamics.html">
   25. Wealth Distribution Dynamics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/kalman.html">
   26. A First Look at the Kalman Filter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../introduction_dynamics/short_path.html">
   27. Shortest Paths
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/mccall_model.html">
   28. Job Search I: The McCall Search Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/mccall_model_with_separation.html">
   29. Job Search II: Search and Separation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/wald_friedman.html">
   30. A Problem that Stumped Milton Friedman
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/odu.html">
   31. Job Search III: Search with Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/career.html">
   32. Job Search IV: Modeling Career Choice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/jv.html">
   33. Job Search V: On-the-Job Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/optgrowth.html">
   34. Optimal Growth I: The Stochastic Optimal Growth Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/coleman_policy_iter.html">
   35. Optimal Growth II: Time Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/egm_policy_iter.html">
   36. Optimal Growth III: The Endogenous Grid Method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/lqcontrol.html">
   37. LQ Dynamic Programming Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/perm_income.html">
   38. Optimal Savings I: The Permanent Income Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/perm_income_cons.html">
   39. Optimal Savings II: LQ Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/smoothing.html">
   40. Consumption and Tax Smoothing with Complete and Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/ifp.html">
   41. Optimal Savings III: Occasionally Binding Constraints
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/robustness.html">
   42. Robustness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming/discrete_dp.html">
   43. Discrete State Dynamic Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Modeling in Continuous Time
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../continuous_time/seir_model.html">
   44. Modeling COVID 19 with Differential Equations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../continuous_time/covid_sde.html">
   45. Modeling Shocks in COVID 19 with Stochastic Differential Equations
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/schelling.html">
   46. Schelling’s Segregation Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/lake_model.html">
   47. A Lake Model of Employment and Unemployment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/rational_expectations.html">
   48. Rational Expectations Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/markov_perf.html">
   49. Markov Perfect Equilibrium
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/markov_asset.html">
   50. Asset Pricing I: Finite State Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/lucas_model.html">
   51. Asset Pricing II: The Lucas Asset Pricing Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/harrison_kreps.html">
   52. Asset Pricing III:  Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/uncertainty_traps.html">
   53. Uncertainty Traps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/aiyagari.html">
   54. The Aiyagari Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/arellano.html">
   55. Default Risk and Income Fluctuations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../multi_agent_models/matsuyama.html">
   56. Globalization and Cycles
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Time Series Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/arma.html">
   57. Covariance Stationary Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/estspec.html">
   58. Estimation of Spectra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/additive_functionals.html">
   59. Additive Functionals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/multiplicative_functionals.html">
   60. Multiplicative Functionals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/lu_tricks.html">
   61. Classical Control with Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series_models/classical_filtering.html">
   62. Classical Filtering With Linear Algebra
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming Squared
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/dyn_stack.html">
   63. Dynamic Stackelberg Problems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/lqramsey.html">
   64. Optimal Taxation in an LQ Economy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/opt_tax_recur.html">
   65. Optimal Taxation with State-Contingent Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dynamic_programming_squared/amss.html">
   66. Optimal Taxation without State-Contingent Debt
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../about_lectures.html">
   67. About these Lectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../troubleshooting.html">
   68. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../zreferences.html">
   69. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../status.html">
   70. Execution Statistics
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/tools_and_techniques/numerical_linear_algebra.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   19.1. Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-complexity">
     19.1.1. Computational Complexity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#notation">
       19.1.1.1. Notation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rules-of-computational-complexity">
     19.1.2. Rules of Computational Complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#losing-structure">
     19.1.3. Losing Structure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-multiplication">
     19.1.4. Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#factorizations">
   19.2. Factorizations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverting-matrices">
     19.2.1. Inverting Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#triangular-matrices-and-back-forward-substitution">
     19.2.2. Triangular Matrices and Back/Forward Substitution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lu-decomposition">
     19.2.3. LU Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cholesky-decomposition">
     19.2.4. Cholesky Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#qr-decomposition">
     19.2.5. QR Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-decomposition">
     19.2.6. Spectral Decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continuous-time-markov-chains-ctmcs">
   19.3. Continuous-Time Markov Chains (CTMCs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-dimensions">
     19.3.1. Multiple Dimensions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#irreducibility">
     19.3.2. Irreducibility
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#banded-matrices">
   19.4. Banded Matrices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-details-and-performance">
   19.5. Implementation Details and Performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-difficulty">
     19.5.1. Implementation Difficulty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#row-and-column-major-ordering">
     19.5.2. Row- and Column-Major Ordering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-on-allocations-and-in-place-operations">
     19.5.3. Digression on Allocations and In-place Operations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   19.6. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     19.6.1. Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2a">
     19.6.2. Exercise 2a
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2b">
     19.6.3. Exercise 2b
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Numerical Linear Algebra and Factorizations</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   19.1. Overview
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#computational-complexity">
     19.1.1. Computational Complexity
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#notation">
       19.1.1.1. Notation
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rules-of-computational-complexity">
     19.1.2. Rules of Computational Complexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#losing-structure">
     19.1.3. Losing Structure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#matrix-multiplication">
     19.1.4. Matrix Multiplication
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#factorizations">
   19.2. Factorizations
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inverting-matrices">
     19.2.1. Inverting Matrices
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#triangular-matrices-and-back-forward-substitution">
     19.2.2. Triangular Matrices and Back/Forward Substitution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lu-decomposition">
     19.2.3. LU Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cholesky-decomposition">
     19.2.4. Cholesky Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#qr-decomposition">
     19.2.5. QR Decomposition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#spectral-decomposition">
     19.2.6. Spectral Decomposition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#continuous-time-markov-chains-ctmcs">
   19.3. Continuous-Time Markov Chains (CTMCs)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiple-dimensions">
     19.3.1. Multiple Dimensions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#irreducibility">
     19.3.2. Irreducibility
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#banded-matrices">
   19.4. Banded Matrices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation-details-and-performance">
   19.5. Implementation Details and Performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-difficulty">
     19.5.1. Implementation Difficulty
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#row-and-column-major-ordering">
     19.5.2. Row- and Column-Major Ordering
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-on-allocations-and-in-place-operations">
     19.5.3. Digression on Allocations and In-place Operations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   19.6. Exercises
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-1">
     19.6.1. Exercise 1
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2a">
     19.6.2. Exercise 2a
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#exercise-2b">
     19.6.3. Exercise 2b
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div id="qe-notebook-header" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div><div class="tex2jax_ignore mathjax_ignore section" id="numerical-linear-algebra-and-factorizations">
<h1><a class="toc-backref" href="#id1"><span class="section-number">19. </span>Numerical Linear Algebra and Factorizations</a><a class="headerlink" href="#numerical-linear-algebra-and-factorizations" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#numerical-linear-algebra-and-factorizations" id="id1">Numerical Linear Algebra and Factorizations</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id2">Overview</a></p></li>
<li><p><a class="reference internal" href="#factorizations" id="id3">Factorizations</a></p></li>
<li><p><a class="reference internal" href="#continuous-time-markov-chains-ctmcs" id="id4">Continuous-Time Markov Chains (CTMCs)</a></p></li>
<li><p><a class="reference internal" href="#banded-matrices" id="id5">Banded Matrices</a></p></li>
<li><p><a class="reference internal" href="#implementation-details-and-performance" id="id6">Implementation Details and Performance</a></p></li>
<li><p><a class="reference internal" href="#exercises" id="id7">Exercises</a></p></li>
</ul>
</li>
</ul>
</div>
<blockquote class="epigraph">
<div><p>You cannot learn too much linear algebra. – Benedict Gross</p>
</div></blockquote>
<div class="section" id="overview">
<h2><a class="toc-backref" href="#id2"><span class="section-number">19.1. </span>Overview</a><a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>In this lecture, we examine the structure of matrices and linear operators (e.g., dense, sparse, symmetric, tridiagonal, banded) and
discuss how the structure can be exploited to radically increase the performance of solving large problems.</p>
<p>We build on applications discussed in previous lectures: <a class="reference internal" href="linear_algebra.html"><span class="doc">linear algebra</span></a>, <a class="reference internal" href="orth_proj.html"><span class="doc">orthogonal projections</span></a>, and <a class="reference internal" href="../introduction_dynamics/finite_markov.html"><span class="doc">Markov chains</span></a>.</p>
<p>The methods in this section are called direct methods, and they are qualitatively similar to performing Gaussian elimination to factor matrices and solve systems of equations.  In <a class="reference internal" href="iterative_methods_sparsity.html"><span class="doc">iterative methods and sparsity</span></a> we examine a different approach, using iterative algorithms, where we can think of more general linear operators.</p>
<p>The list of specialized packages for these tasks is enormous and growing, but some of the important organizations to
look at are <a class="reference external" href="https://github.com/JuliaMatrices">JuliaMatrices</a> , <a class="reference external" href="https://github.com/JuliaSparse">JuliaSparse</a>, and <a class="reference external" href="https://github.com/JuliaMath">JuliaMath</a></p>
<p><em>NOTE</em>: As this section uses advanced Julia techniques, you may wish to review multiple-dispatch and generic programming in  <a class="reference internal" href="../getting_started_julia/introduction_to_types.html"><span class="doc">introduction to types</span></a>, and consider further study on <a class="reference internal" href="../more_julia/generic_programming.html"><span class="doc">generic programming</span></a>.</p>
<p>The theme of this lecture, and numerical linear algebra in general, comes down to three principles:</p>
<ol class="simple">
<li><p><strong>Identify structure</strong> (e.g., <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#Special-matrices-1">symmetric, sparse, diagonal</a>) matrices in order to use <strong>specialized algorithms.</strong></p></li>
<li><p><strong>Do not lose structure</strong> by applying the wrong numerical linear algebra operations at the wrong times (e.g., sparse matrix becoming dense)</p></li>
<li><p>Understand the <strong>computational complexity</strong> of each algorithm, given the structure of the inputs.</p></li>
</ol>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">LinearAlgebra</span><span class="p">,</span><span class="w"> </span><span class="n">Statistics</span><span class="p">,</span><span class="w"> </span><span class="n">BenchmarkTools</span><span class="p">,</span><span class="w"> </span><span class="n">SparseArrays</span><span class="p">,</span><span class="w"> </span><span class="n">Random</span><span class="p">,</span><span class="w"> </span><span class="n">Parameters</span>
<span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">42</span><span class="p">);</span><span class="w">  </span><span class="c"># seed random numbers for reproducibility</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="computational-complexity">
<h3><span class="section-number">19.1.1. </span>Computational Complexity<a class="headerlink" href="#computational-complexity" title="Permalink to this headline">¶</a></h3>
<p>Ask yourself whether the following is a <strong>computationally expensive</strong> operation as the matrix <strong>size increases</strong></p>
<ul class="simple">
<li><p>Multiplying two matrices?</p>
<ul>
<li><p><em>Answer</em>: It depends.  Multiplying two diagonal matrices is trivial.</p></li>
</ul>
</li>
<li><p>Solving a linear system of equations?</p>
<ul>
<li><p><em>Answer</em>: It depends.  If the matrix is the identity, the solution is the vector itself.</p></li>
</ul>
</li>
<li><p>Finding the eigenvalues of a matrix?</p>
<ul>
<li><p><em>Answer</em>: It depends.  The eigenvalues of a triangular matrix are the diagonal elements.</p></li>
</ul>
</li>
</ul>
<p>As the goal of this section is to move toward numerical methods with large systems, we need to understand how well algorithms scale with the size of matrices, vectors, etc.  This is known as <a class="reference external" href="https://en.wikipedia.org/wiki/Computational_complexity">computational complexity</a>.  As we saw in the answer to the questions above, the algorithm - and hence the computational complexity - changes based on matrix structure.</p>
<p>While this notion of complexity can work at various levels, such as the number of <a class="reference external" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Arithmetic_functions">significant digits</a> for basic mathematical operations, the amount of memory and storage required, or the amount of time, we will typically focus on the time complexity.</p>
<p>For time complexity, the size <span class="math notranslate nohighlight">\(N\)</span> is usually the dimensionality of the problem, although occasionally the key will be the number of non-zeros in the matrix or the width of bands.  For our applications, time complexity is best thought of as the number of floating point operations (e.g., addition, multiplication) required.</p>
<div class="section" id="notation">
<h4><span class="section-number">19.1.1.1. </span>Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h4>
<p>Complexity of algorithms is typically written in <a class="reference external" href="https://en.wikipedia.org/wiki/Big_O_notation">Big O</a> notation, which provides bounds on the scaling of the computational complexity with respect to the size of the inputs.</p>
<p>Formally, if the number of operations required for a problem size <span class="math notranslate nohighlight">\(N\)</span> is <span class="math notranslate nohighlight">\(f(N)\)</span>, we  can write this as <span class="math notranslate nohighlight">\(f(N) = O(g(N))\)</span> for some <span class="math notranslate nohighlight">\(g(N)\)</span> - typically a polynomial.</p>
<p>The interpretation is that there exist some constants <span class="math notranslate nohighlight">\(M\)</span> and <span class="math notranslate nohighlight">\(N_0\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f(N) \leq M g(N), \text{ for } N &gt; N_0
\]</div>
<p>For example, the complexity of finding an LU Decomposition of a dense matrix is <span class="math notranslate nohighlight">\(O(N^3)\)</span>, which should be read as there being a constant where
eventually the number of floating point operations required to decompose a matrix of size <span class="math notranslate nohighlight">\(N\times N\)</span> grows cubically.</p>
<p>Keep in mind that these are asymptotic results intended for understanding the scaling of the problem, and the constant can matter for a given
fixed size.</p>
<p>For example, the number of operations required for an <a class="reference external" href="https://en.wikipedia.org/wiki/LU_decomposition#Algorithms">LU decomposition</a> of a dense <span class="math notranslate nohighlight">\(N \times N\)</span> matrix is <span class="math notranslate nohighlight">\(f(N) = \frac{2}{3} N^3\)</span>, ignoring the <span class="math notranslate nohighlight">\(N^2\)</span> and lower terms.  Other methods of solving a linear system may have different constants of proportionality, even if they have the same scaling, <span class="math notranslate nohighlight">\(O(N^3)\)</span>.</p>
</div>
</div>
<div class="section" id="rules-of-computational-complexity">
<h3><span class="section-number">19.1.2. </span>Rules of Computational Complexity<a class="headerlink" href="#rules-of-computational-complexity" title="Permalink to this headline">¶</a></h3>
<p>You will sometimes need to think through how <a class="reference external" href="https://en.wikipedia.org/wiki/Big_O_notation#Properties">combining algorithms</a> changes complexity.  For example, if you use</p>
<ol class="simple">
<li><p>an <span class="math notranslate nohighlight">\(O(N^3)\)</span> operation <span class="math notranslate nohighlight">\(P\)</span> times, then it simply changes the constant. The complexity remains <span class="math notranslate nohighlight">\(O(N^3)\)</span>.</p></li>
<li><p>one <span class="math notranslate nohighlight">\(O(N^3)\)</span> operation and one <span class="math notranslate nohighlight">\(O(N^2)\)</span> operation, then you take the max.  The complexity remains <span class="math notranslate nohighlight">\(O(N^3)\)</span>.</p></li>
<li><p>a repetition of an <span class="math notranslate nohighlight">\(O(N)\)</span> operation that itself uses an <span class="math notranslate nohighlight">\(O(N)\)</span> operation, you take the product.  The complexity becomes <span class="math notranslate nohighlight">\(O(N^2)\)</span>.</p></li>
</ol>
<p>With this, we have an important word of caution: Dense-matrix multiplication is an <a class="reference external" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra">expensive operation</a> for unstructured matrices.  The naive version is <span class="math notranslate nohighlight">\(O(N^3)\)</span> while the fastest-known algorithms (e.g., Coppersmith-Winograd) are roughly <span class="math notranslate nohighlight">\(O(N^{2.37})\)</span>.  In practice, it is reasonable to crudely approximate with <span class="math notranslate nohighlight">\(O(N^3)\)</span> when doing an analysis, in part since the higher constant factors of the better scaling algorithms dominate the better complexity until matrices become very large.</p>
<p>Of course, modern libraries use highly tuned and numerically stable <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm">algorithms</a> to multiply matrices and exploit the computer architecture, memory cache, etc., but this simply lowers the constant of proportionality and they remain roughly approximated by  <span class="math notranslate nohighlight">\(O(N^3)\)</span>.</p>
<p>A consequence is that, since many algorithms require matrix-matrix multiplication, it is often not possible to go below that order without further matrix structure.</p>
<p>That is, changing the constant of proportionality for a given size can help, but in order to achieve better scaling you need to identify matrix structure (e.g., tridiagonal, sparse) and ensure that your operations do not lose it.</p>
</div>
<div class="section" id="losing-structure">
<h3><span class="section-number">19.1.3. </span>Losing Structure<a class="headerlink" href="#losing-structure" title="Permalink to this headline">¶</a></h3>
<p>As a first example of a structured matrix, consider a <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/SparseArrays/index.html">sparse array</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sprand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mf">0.45</span><span class="p">)</span><span class="w">  </span><span class="c"># random sparse 10x10, 45 percent filled with non-zeros</span>

<span class="nd">@show</span><span class="w"> </span><span class="n">nnz</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w">  </span><span class="c"># counts the number of non-zeros</span>
<span class="n">invA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparse</span><span class="p">(</span><span class="n">inv</span><span class="p">(</span><span class="kt">Array</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span><span class="w">  </span><span class="c"># Julia won&#39;t invert sparse, so convert to dense with Array.</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">nnz</span><span class="p">(</span><span class="n">invA</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nnz(A) = 46
nnz(invA) = 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100
</pre></div>
</div>
</div>
</div>
<p>This increase from less than 50 to 100 percent dense demonstrates that significant sparsity can be lost when computing an inverse.</p>
<p>The results can be even more extreme.  Consider a tridiagonal matrix of size <span class="math notranslate nohighlight">\(N \times N\)</span>
that might come out of a Markov chain or a discretization of a diffusion process,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">([</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="mf">0.2</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="mf">0.2</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5×5 Tridiagonal{Float64, Vector{Float64}}:
 0.8  0.2   ⋅    ⋅    ⋅ 
 0.1  0.8  0.1   ⋅    ⋅ 
  ⋅   0.1  0.8  0.1   ⋅ 
  ⋅    ⋅   0.1  0.8  0.1
  ⋅    ⋅    ⋅   0.2  0.8
</pre></div>
</div>
</div>
</div>
<p>The number of non-zeros here is approximately <span class="math notranslate nohighlight">\(3 N\)</span>, linear, which scales well for huge matrices into the millions or billions</p>
<p>But consider the inverse</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5×5 Matrix{Float64}:
  1.29099      -0.327957     0.0416667  -0.00537634   0.000672043
 -0.163978      1.31183     -0.166667    0.0215054   -0.00268817
  0.0208333    -0.166667     1.29167    -0.166667     0.0208333
 -0.00268817    0.0215054   -0.166667    1.31183     -0.163978
  0.000672043  -0.00537634   0.0416667  -0.327957     1.29099
</pre></div>
</div>
</div>
</div>
<p>Now, the matrix is fully dense and has <span class="math notranslate nohighlight">\(N^2\)</span> non-zeros.</p>
<p>This also applies to the <span class="math notranslate nohighlight">\(A' A\)</span> operation when forming the normal equations of linear least squares.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sprand</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">21</span><span class="p">,</span><span class="w"> </span><span class="mf">0.3</span><span class="p">)</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">nnz</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">/</span><span class="mi">20</span><span class="o">^</span><span class="mi">2</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">nnz</span><span class="p">(</span><span class="n">A</span><span class="o">&#39;*</span><span class="n">A</span><span class="p">)</span><span class="o">/</span><span class="mi">21</span><span class="o">^</span><span class="mi">2</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>nnz(A) / 20 ^ 2 = 0.34
nnz(A&#39; * A) / 21 ^ 2 = 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9229024943310657
</pre></div>
</div>
</div>
</div>
<p>We see that a 30 percent dense matrix becomes almost full dense after the product is taken.</p>
<p><em>Sparsity/Structure is not just for storage</em>:  Matrix size can sometimes become important (e.g., a 1 million by 1 million tridiagonal matrix needs to store 3 million numbers (i.e., about 6MB of memory), where a dense one requires 1 trillion (i.e., about 1TB of memory)).</p>
<p>But, as we will see, the main purpose of considering sparsity and matrix structure is that it enables specialized algorithms, which typically
have a lower computational order than unstructured dense, or even unstructured sparse, operations.</p>
<p>First, create a convenient function for benchmarking linear solvers</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">BenchmarkTools</span>
<span class="k">function</span><span class="w"> </span><span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="w">    </span><span class="n">println</span><span class="p">(</span><span class="s">&quot;A</span><span class="se">\\</span><span class="s">b for typeof(A) = </span><span class="si">$</span><span class="p">(</span><span class="n">string</span><span class="p">(</span><span class="n">typeof</span><span class="p">(</span><span class="n">A</span><span class="p">)))</span><span class="s">&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="nd">@btime</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="o">$</span><span class="n">b</span>
<span class="k">end</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>benchmark_solve (generic function with 1 method)
</pre></div>
</div>
</div>
</div>
<p>Then, take away structure to see the impact on performance,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">([</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="mf">0.2</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="mf">0.2</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);])</span>
<span class="n">A_sparse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparse</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w">  </span><span class="c"># sparse but losing tridiagonal structure</span>
<span class="n">A_dense</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">Array</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w">    </span><span class="c"># dropping the sparsity structure, dense 1000x1000</span>

<span class="c"># benchmark solution to system A x = b</span>
<span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A_sparse</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A_dense</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A\b for typeof(A) = Tridiagonal{Float64, Vector{Float64}}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  32.700 μs (8 allocations: 47.72 KiB)
A\b for typeof(A) = SparseMatrixCSC{Float64, Int64}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  737.191 μs (79 allocations: 1.03 MiB)
A\b for typeof(A) = Matrix{Float64}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  24.937 ms (4 allocations: 7.64 MiB)
</pre></div>
</div>
</div>
</div>
<p>This example shows what is at stake:  using a structured tridiagonal matrix may be 10-20 times faster than using a sparse matrix, which is 100 times faster than
using a dense matrix.</p>
<p>In fact, the difference becomes more extreme as the matrices grow.  Solving a tridiagonal system is <span class="math notranslate nohighlight">\(O(N)\)</span>, while that of a dense matrix without any structure is <span class="math notranslate nohighlight">\(O(N^3)\)</span>.  The complexity of a sparse solution is more complicated, and scales in part by the <code class="docutils literal notranslate"><span class="pre">nnz(N)</span></code>, i.e., the number of nonzeros.</p>
</div>
<div class="section" id="matrix-multiplication">
<h3><span class="section-number">19.1.4. </span>Matrix Multiplication<a class="headerlink" href="#matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<p>While we write matrix multiplications in our algebra with abundance, in practice the computational operation scales very poorly without any matrix structure.</p>
<p>Matrix multiplication is so important to modern computers that the constant of scaling is small using proper packages, but the order is still roughly <span class="math notranslate nohighlight">\(O(N^3)\)</span> in practice (although smaller in theory, as discussed above).</p>
<p>Sparse matrix multiplication, on the other hand, is <span class="math notranslate nohighlight">\(O(N M_A M_B)\)</span> where <span class="math notranslate nohighlight">\(M_A\)</span> is the number of nonzeros per row of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(M_B\)</span> is the number of non-zeros per column of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>By the rules of computational order, that means any algorithm requiring a matrix multiplication of dense matrices requires at least <span class="math notranslate nohighlight">\(O(N^3)\)</span> operation.</p>
<p>The other important question is what is the structure of the resulting matrix.  For example, multiplying an upper triangular matrix by a lower triangular matrix</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span>
<span class="n">U</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">UpperTriangular</span><span class="p">(</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5×5 UpperTriangular{Float64, Matrix{Float64}}:
 0.894532  0.620631  0.238642  0.493049  0.418376
  ⋅        0.691248  0.640302  0.843674  0.0508756
  ⋅         ⋅        0.187509  0.148339  0.732873
  ⋅         ⋅         ⋅        0.190347  0.502406
  ⋅         ⋅         ⋅         ⋅        0.664839
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">U</span><span class="o">&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5×5 LowerTriangular{Float64, Adjoint{Float64, Matrix{Float64}}}:
 0.894532   ⋅          ⋅         ⋅         ⋅ 
 0.620631  0.691248    ⋅         ⋅         ⋅ 
 0.238642  0.640302   0.187509   ⋅         ⋅ 
 0.493049  0.843674   0.148339  0.190347   ⋅ 
 0.418376  0.0508756  0.732873  0.502406  0.664839
</pre></div>
</div>
</div>
</div>
<p>But the product is fully dense (e.g., think of a Cholesky multiplied by itself to produce a covariance matrix)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">U</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5×5 Matrix{Float64}:
 0.800188  0.555175  0.213473  0.441048  0.374251
 0.555175  0.863008  0.590717  0.88919   0.294825
 0.213473  0.590717  0.502096  0.685683  0.269839
 0.441048  0.88919   0.685683  1.01312   0.453547
 0.374251  0.294825  0.269839  0.453547  1.40915
</pre></div>
</div>
</div>
</div>
<p>On the other hand, a tridiagonal matrix times a diagonal matrix is still tridiagonal - and can use specialized <span class="math notranslate nohighlight">\(O(N)\)</span> algorithms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">([</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="mf">0.2</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="mf">0.2</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);])</span>
<span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Diagonal</span><span class="p">(</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">))</span>
<span class="n">D</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">A</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5×5 Tridiagonal{Float64, Vector{Float64}}:
 0.080983   0.0202457   ⋅          ⋅          ⋅ 
 0.0244522  0.195618   0.0244522   ⋅          ⋅ 
  ⋅         0.0760396  0.608317   0.0760396   ⋅ 
  ⋅          ⋅         0.0858704  0.686963   0.0858704
  ⋅          ⋅          ⋅         0.0113042  0.0452167
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="factorizations">
<h2><a class="toc-backref" href="#id3"><span class="section-number">19.2. </span>Factorizations</a><a class="headerlink" href="#factorizations" title="Permalink to this headline">¶</a></h2>
<p>When you tell a numerical analyst you are solving a linear system using direct methods, their first question is “which factorization?”.</p>
<p>Just as you can factor a number (e.g., <span class="math notranslate nohighlight">\(6 = 3 \times 2\)</span>) you can factor a matrix as the product of other, more
convenient matrices (e.g., <span class="math notranslate nohighlight">\(A = L U\)</span> or <span class="math notranslate nohighlight">\(A = Q R\)</span>, where <span class="math notranslate nohighlight">\(L, U, Q,\)</span> and <span class="math notranslate nohighlight">\(R\)</span> have properties such as being triangular, <a class="reference external" href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal</a>, etc.).</p>
<div class="section" id="inverting-matrices">
<h3><span class="section-number">19.2.1. </span>Inverting Matrices<a class="headerlink" href="#inverting-matrices" title="Permalink to this headline">¶</a></h3>
<p>On paper, since the <a class="reference external" href="https://en.wikipedia.org/wiki/Invertible_matrix#The_invertible_matrix_theorem">Invertible Matrix Theorem</a> tells us that a unique solution is
equivalent to <span class="math notranslate nohighlight">\(A\)</span> being invertible, we often write the solution to <span class="math notranslate nohighlight">\(A x = b\)</span> as</p>
<div class="math notranslate nohighlight">
\[
x = A^{-1} b
\]</div>
<p>What if we do not (directly) use a factorization?</p>
<p>Take a simple linear system of a dense matrix,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4-element Vector{Float64}:
 0.1692573631849904
 0.8286640042337545
 0.05607569601441986
 0.5958786724371172
</pre></div>
</div>
</div>
</div>
<p>On paper, we try to solve the system <span class="math notranslate nohighlight">\(A x = b\)</span> by inverting the matrix,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4-element Vector{Float64}:
 -0.4172203630414315
  0.010479372080586132
  9.118506847017615
 -4.99195905422948
</pre></div>
</div>
</div>
</div>
<p>As we will see throughout, inverting matrices should be used for theory, not for code.  The classic advice that you should <a class="reference external" href="https://www.johndcook.com/blog/2010/01/19/dont-invert-that-matrix">never invert a matrix</a> may be <a class="reference external" href="https://arxiv.org/abs/1201.6035">slightly exaggerated</a>, but is generally good advice.</p>
<p>Solving a system by inverting a matrix is always a little slower, is potentially less accurate, and will sometimes lose crucial sparsity compared to using factorizations.  Moreover, the methods used by libraries to invert matrices are frequently the same factorizations used for computing a system of equations.</p>
<p>Even if you need to solve a system with the same matrix multiple times, you are better off factoring the matrix and using the solver rather than calculating an inverse.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">30</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">)</span>
<span class="k">function</span><span class="w"> </span><span class="n">solve_inverting</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">)</span>
<span class="w">    </span><span class="n">A_inv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inv</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="w">    </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">similar</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">size</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="w">        </span><span class="n">X</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A_inv</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">X</span>
<span class="k">end</span>

<span class="k">function</span><span class="w"> </span><span class="n">solve_factoring</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">)</span>
<span class="w">    </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">similar</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="w">    </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factorize</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">size</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="w">        </span><span class="n">X</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">i</span><span class="p">]</span>
<span class="w">    </span><span class="k">end</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">X</span>
<span class="k">end</span>



<span class="nd">@btime</span><span class="w"> </span><span class="n">solve_inverting</span><span class="p">(</span><span class="o">$</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">B</span><span class="p">)</span>
<span class="nd">@btime</span><span class="w"> </span><span class="n">solve_factoring</span><span class="p">(</span><span class="o">$</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">B</span><span class="p">)</span>

<span class="c"># even better, use the built-in feature for multiple RHS</span>
<span class="nd">@btime</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="o">$</span><span class="n">B</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  297.398 μs (67 allocations: 205.16 KiB)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  225.797 μs (96 allocations: 155.53 KiB)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  194.098 μs (5 allocations: 102.53 KiB)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="triangular-matrices-and-back-forward-substitution">
<h3><span class="section-number">19.2.2. </span>Triangular Matrices and Back/Forward Substitution<a class="headerlink" href="#triangular-matrices-and-back-forward-substitution" title="Permalink to this headline">¶</a></h3>
<p>Some matrices are already in a convenient form and require no further factoring.</p>
<p>For example, consider solving a system with an <code class="docutils literal notranslate"><span class="pre">UpperTriangular</span></code> matrix,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="w"> </span><span class="mf">2.0</span><span class="p">,</span><span class="w"> </span><span class="mf">3.0</span><span class="p">]</span>
<span class="n">U</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">UpperTriangular</span><span class="p">([</span><span class="mf">1.0</span><span class="w"> </span><span class="mf">2.0</span><span class="w"> </span><span class="mf">3.0</span><span class="p">;</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="mf">5.0</span><span class="w"> </span><span class="mf">6.0</span><span class="p">;</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="mf">0.0</span><span class="w"> </span><span class="mf">9.0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3×3 UpperTriangular{Float64, Matrix{Float64}}:
 1.0  2.0  3.0
  ⋅   5.0  6.0
  ⋅    ⋅   9.0
</pre></div>
</div>
</div>
</div>
<p>This system is especially easy to solve using <a class="reference external" href="https://en.wikipedia.org/wiki/Triangular_matrix#Forward_and_back_substitution">back substitution</a>.  In particular, <span class="math notranslate nohighlight">\(x_3 = b_3 / U_{33}, x_2 = (b_2 - x_3 U_{23})/U_{22}\)</span>, etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3-element Vector{Float64}:
 1.1102230246251563e-17
 2.2204460492503132e-17
 0.3333333333333333
</pre></div>
</div>
</div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">LowerTriangular</span></code> matrix has similar properties and can be solved with forward substitution.</p>
<p>The computational order of back substitution and forward substitution is <span class="math notranslate nohighlight">\(O(N^2)\)</span> for dense matrices.  Those fast algorithms are a key reason that factorizations target triangular structures.</p>
</div>
<div class="section" id="lu-decomposition">
<span id="jl-decomposition"></span><h3><span class="section-number">19.2.3. </span>LU Decomposition<a class="headerlink" href="#lu-decomposition" title="Permalink to this headline">¶</a></h3>
<p>The <span class="math notranslate nohighlight">\(LU\)</span> decomposition finds a lower triangular matrix <span class="math notranslate nohighlight">\(L\)</span> and an upper triangular matrix <span class="math notranslate nohighlight">\(U\)</span> such that <span class="math notranslate nohighlight">\(L U = A\)</span>.</p>
<p>For a general dense matrix without any other structure (i.e., not known to be symmetric, tridiagonal, etc.) this is the standard approach to solve a system and exploit the speed of back and forward substitution using the factorization.</p>
<p>The computational order of LU decomposition itself for a dense matrix is <span class="math notranslate nohighlight">\(O(N^3)\)</span> - the same as Gaussian elimination - but it tends
to have a better constant term than others (e.g., half the number of operations of the QR decomposition).  For structured
or sparse matrices, that order drops.</p>
<p>We can see which algorithm Julia will use for the <code class="docutils literal notranslate"><span class="pre">\</span></code> operator by looking at the <code class="docutils literal notranslate"><span class="pre">factorize</span></code> function for a given
matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

<span class="n">Af</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factorize</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w">  </span><span class="c"># chooses the right factorization, LU here</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LU{Float64, Matrix{Float64}, Vector{Int64}}
L factor:
4×4 Matrix{Float64}:
 1.0       0.0       0.0       0.0
 0.415843  1.0       0.0       0.0
 0.837544  0.412374  1.0       0.0
 0.230398  0.451915  0.320159  1.0
U factor:
4×4 Matrix{Float64}:
 0.95462  0.30487   0.196935    0.658974
 0.0      0.334293  0.0898636   0.476012
 0.0      0.0       0.763321   -0.213677
 0.0      0.0       0.0         0.183613
</pre></div>
</div>
</div>
</div>
<p>In this case, it provides an <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(U\)</span> factorization (with <a class="reference external" href="https://en.wikipedia.org/wiki/LU_decomposition#LU_factorization_with_full_pivoting">pivoting</a> ).</p>
<p>With the factorization complete, we can solve different <code class="docutils literal notranslate"><span class="pre">b</span></code> right hand sides.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">Af</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4-element Vector{Float64}:
  0.5483309544545236
  2.4706281021373653
 -0.5369034286036096
 -0.8253373998820174
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">b2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">Af</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4-element Vector{Float64}:
  0.3734712417577138
 -2.072936783435963
 -0.08980746803909484
  1.8007954892470772
</pre></div>
</div>
</div>
</div>
<p>In practice, the decomposition also includes a <span class="math notranslate nohighlight">\(P\)</span> which is a <a class="reference external" href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrix</a> such
that <span class="math notranslate nohighlight">\(P A = L U\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">Af</span><span class="o">.</span><span class="n">P</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">≈</span><span class="w"> </span><span class="n">Af</span><span class="o">.</span><span class="n">L</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Af</span><span class="o">.</span><span class="n">U</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>true
</pre></div>
</div>
</div>
</div>
<p>We can also directly calculate an LU decomposition with <code class="docutils literal notranslate"><span class="pre">lu</span></code> but without the pivoting,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">L</span><span class="p">,</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lu</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">Val</span><span class="p">(</span><span class="nb">false</span><span class="p">))</span><span class="w">  </span><span class="c"># the Val(false) provides a solution without permutation matrices</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LU{Float64, Matrix{Float64}, Vector{Int64}}
L factor:
4×4 Matrix{Float64}:
 1.0        0.0     0.0     0.0
 0.554051   1.0     0.0     0.0
 2.40475   23.5446  1.0     0.0
 2.01409   15.6821  0.5334  1.0
U factor:
4×4 Matrix{Float64}:
 0.396972   0.461071    0.171758   0.750041
 0.0       -0.0341433   0.235206   0.0665844
 0.0        0.0        -5.75392   -2.71239
 0.0        0.0         0.0       -0.573506
</pre></div>
</div>
</div>
</div>
<p>And we can verify the decomposition</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">≈</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">U</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>true
</pre></div>
</div>
</div>
</div>
<p>To see roughly how the solver works, note that we can write the problem <span class="math notranslate nohighlight">\(A x = b\)</span> as <span class="math notranslate nohighlight">\(L U x = b\)</span>.  Let <span class="math notranslate nohighlight">\(U x = y\)</span>, which breaks the
problem into two sub-problems.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
L y &amp;= b\\
U x &amp;= y
\end{aligned}
\end{split}\]</div>
<p>As we saw above, this is the solution to two triangular systems, which can be efficiently done with back or forward substitution in <span class="math notranslate nohighlight">\(O(N^2)\)</span> operations.</p>
<p>To demonstrate this, first using</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4-element Vector{Float64}:
  0.6455522128314688
 -0.2655926740726696
  5.327941090151101
  0.47333580651201745
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">y</span>
<span class="n">x</span><span class="w"> </span><span class="o">≈</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span><span class="w">  </span><span class="c"># Check identical</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>true
</pre></div>
</div>
</div>
</div>
<p>The LU decomposition also has specialized algorithms for structured matrices, such as a <code class="docutils literal notranslate"><span class="pre">Tridiagonal</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">([</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="mf">0.2</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="mf">0.2</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);])</span>
<span class="n">factorize</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">typeof</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LU{Float64, Tridiagonal{Float64, Vector{Float64}}, Vector{Int64}}
</pre></div>
</div>
</div>
</div>
<p>This factorization is the key to the performance of the <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">\</span> <span class="pre">b</span></code> in this case.  For Tridiagonal matrices, the
LU decomposition is <span class="math notranslate nohighlight">\(O(N^2)\)</span>.</p>
<p>Finally, just as a dense matrix without any structure uses an LU decomposition to solve a system,
so will the sparse solvers</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A_sparse</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparse</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">factorize</span><span class="p">(</span><span class="n">A_sparse</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">typeof</span><span class="w">  </span><span class="c"># dropping the tridiagonal structure to just become sparse</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SparseArrays.UMFPACK.UmfpackLU{Float64, Int64}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A_sparse</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A\b for typeof(A) = Tridiagonal{Float64, Vector{Float64}}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  32.200 μs (8 allocations: 47.72 KiB)
A\b for typeof(A) = SparseMatrixCSC{Float64, Int64}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  733.092 μs (79 allocations: 1.03 MiB)
</pre></div>
</div>
</div>
</div>
<p>With sparsity, the computational order is related to the number of non-zeros rather than the size of the matrix itself.</p>
</div>
<div class="section" id="cholesky-decomposition">
<h3><span class="section-number">19.2.4. </span>Cholesky Decomposition<a class="headerlink" href="#cholesky-decomposition" title="Permalink to this headline">¶</a></h3>
<p>For real, symmetric, <a class="reference external" href="https://en.wikipedia.org/wiki/Definiteness_of_a_matrix">positive semi-definite</a> matrices, a Cholesky decomposition is a specialized example of an LU decomposition where <span class="math notranslate nohighlight">\(L = U'\)</span>.</p>
<p>The Cholesky is directly useful on its own (e.g., <a class="reference internal" href="../time_series_models/classical_filtering.html"><span class="doc">Classical Control with Linear Algebra</span></a>), but it is also an efficient factorization to use in solving symmetric positive semi-definite systems.</p>
<p>As always, symmetry allows specialized algorithms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">500</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">A_dense</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="o">&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">B</span><span class="w">  </span><span class="c"># an easy way to generate a symmetric positive semi-definite matrix</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Symmetric</span><span class="p">(</span><span class="n">A_dense</span><span class="p">)</span><span class="w">  </span><span class="c"># flags the matrix as symmetric</span>

<span class="n">factorize</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">typeof</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BunchKaufman{Float64, Matrix{Float64}, Vector{Int64}}
</pre></div>
</div>
</div>
</div>
<p>Here, the <span class="math notranslate nohighlight">\(A\)</span> decomposition is <a class="reference external" href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/index.html#LinearAlgebra.bunchkaufman">Bunch-Kaufman</a> rather than
Cholesky, because Julia doesn’t know that the matrix is positive semi-definite.  We can manually factorize with a Cholesky,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">typeof</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cholesky{Float64, Matrix{Float64}}
</pre></div>
</div>
</div>
</div>
<p>Benchmarking,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span><span class="w">  </span><span class="c"># use the factorization to solve</span>

<span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="n">benchmark_solve</span><span class="p">(</span><span class="n">A_dense</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
<span class="nd">@btime</span><span class="w"> </span><span class="n">cholesky</span><span class="p">(</span><span class="o">$</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">check</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="o">$</span><span class="n">b</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A\b for typeof(A) = Symmetric{Float64, Matrix{Float64}}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  3.074 ms (6 allocations: 2.16 MiB)
A\b for typeof(A) = Matrix{Float64}
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  4.281 ms (4 allocations: 1.92 MiB)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  3.173 ms (3 allocations: 1.91 MiB)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="qr-decomposition">
<h3><span class="section-number">19.2.5. </span>QR Decomposition<a class="headerlink" href="#qr-decomposition" title="Permalink to this headline">¶</a></h3>
<p>Previously, we learned about applications of the QR decomposition to solving the linear least squares.</p>
<p>While in principle the solution to the least-squares problem</p>
<div class="math notranslate nohighlight">
\[
\min_x \| Ax -b \|^2
\]</div>
<p>is <span class="math notranslate nohighlight">\(x = (A'A)^{-1}A'b\)</span>, in practice note that <span class="math notranslate nohighlight">\(A'A\)</span> becomes dense and calculating the inverse is rarely a good idea.</p>
<p>The QR decomposition is a decomposition <span class="math notranslate nohighlight">\(A = Q R\)</span> where <span class="math notranslate nohighlight">\(Q\)</span> is an orthogonal matrix (i.e., <span class="math notranslate nohighlight">\(Q'Q = Q Q' = I\)</span>) and <span class="math notranslate nohighlight">\(R\)</span> is
an upper triangular matrix.</p>
<p>Given the  previous derivation, we showed that we can write the least-squares problem as
the solution to</p>
<div class="math notranslate nohighlight">
\[
R x = Q' b
\]</div>
<p>where, as discussed above, the upper-triangular structure of <span class="math notranslate nohighlight">\(R\)</span> can be solved easily with back substitution.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">\</span></code> operator solves the linear least-squares problem whenever the given <code class="docutils literal notranslate"><span class="pre">A</span></code> is rectangular</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span>
<span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span>
<span class="n">x_true</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">M</span><span class="p">)</span><span class="w"> </span><span class="o">.+</span><span class="w"> </span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3-element Vector{Float64}:
 -0.5238169732680474
 -0.1206305986919548
  0.8918137920710696
</pre></div>
</div>
</div>
</div>
<p>To manually use the QR decomposition in solving linear least squares:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">Af</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">qr</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Af</span><span class="o">.</span><span class="n">Q</span>
<span class="n">R</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">Af</span><span class="o">.</span><span class="n">R</span><span class="p">;</span><span class="w"> </span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">)]</span><span class="w"> </span><span class="c"># Stack with zeros</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">Q</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="o">≈</span><span class="w"> </span><span class="n">A</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">R</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">Q</span><span class="o">&#39;*</span><span class="n">b</span><span class="w">  </span><span class="c"># simplified QR solution for least squares</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Q * R ≈ A = true
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3-element Vector{Float64}:
 -0.5238169732680469
 -0.12063059869195483
  0.8918137920710691
</pre></div>
</div>
</div>
</div>
<p>This stacks the <code class="docutils literal notranslate"><span class="pre">R</span></code> with zeros, but the more specialized algorithm would not multiply directly
in that way.</p>
<p>In some cases, if an LU is not available for a particular matrix structure, the QR factorization
can also be used to solve systems of equations (i.e., not just LLS).  This tends to be about 2 times slower than the LU
but is of the same computational order.</p>
<p>Deriving the approach, where we can now use the inverse since the system is square and we assumed <span class="math notranslate nohighlight">\(A\)</span> was non-singular,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
A x &amp;= b\\
Q R x &amp;= b\\
Q^{-1} Q R x &amp;= Q^{-1} b\\
R x &amp;= Q' b
\end{aligned}
\end{split}\]</div>
<p>where the last step uses the fact that <span class="math notranslate nohighlight">\(Q^{-1} = Q'\)</span> for an orthogonal matrix.</p>
<p>Given the decomposition, the solution for dense matrices is of computational
order <span class="math notranslate nohighlight">\(O(N^2)\)</span>.  To see this, look at the order of each operation.</p>
<ul class="simple">
<li><p>Since <span class="math notranslate nohighlight">\(R\)</span> is an upper-triangular matrix, it can be solved quickly through back substitution with computational order <span class="math notranslate nohighlight">\(O(N^2)\)</span></p></li>
<li><p>A transpose operation is of order <span class="math notranslate nohighlight">\(O(N^2)\)</span></p></li>
<li><p>A matrix-vector product is also <span class="math notranslate nohighlight">\(O(N^2)\)</span></p></li>
</ul>
<p>In all cases, the order would drop depending on the sparsity pattern of the
matrix (and corresponding decomposition).  A key benefit of a QR decomposition is that it tends to
maintain sparsity.</p>
<p>Without implementing the full process, you can form a QR
factorization with <code class="docutils literal notranslate"><span class="pre">qr</span></code> and then use it to solve a system</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">qr</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">b</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A \ b = [-1.0375649582146824, -12.13945385455487, 6.3560880637781665, 3.8473591088663253, 2.045532886550064]
qr(A) \ b = 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-1.0375649582146873, -12.139453854554878, 6.356088063778177, 3.8473591088663257, 2.045532886550066]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="spectral-decomposition">
<h3><span class="section-number">19.2.6. </span>Spectral Decomposition<a class="headerlink" href="#spectral-decomposition" title="Permalink to this headline">¶</a></h3>
<p>A spectral decomposition, also known as an <a class="reference external" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigendecomposition</a>, finds all of the eigenvectors and eigenvalues to decompose a square matrix <code class="docutils literal notranslate"><span class="pre">A</span></code> such that</p>
<div class="math notranslate nohighlight">
\[
A = Q \Lambda Q^{-1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Q\)</span> is a matrix made of the eigenvectors of <span class="math notranslate nohighlight">\(A\)</span> as columns, and <span class="math notranslate nohighlight">\(\Lambda\)</span> is a diagonal matrix of the eigenvalues.  Only square, <a class="reference external" href="https://en.wikipedia.org/wiki/Diagonalizable_matrix">diagonalizable</a> matrices have an eigendecomposition (where a matrix is not diagonalizable if it does not have a full set of linearly independent eigenvectors).</p>
<p>In Julia, whenever you ask for a full set of eigenvectors and eigenvalues, it  decomposes using an algorithm appropriate for the matrix type.  For example, symmetric, Hermitian, and tridiagonal matrices have specialized algorithms.</p>
<p>To see this,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Symmetric</span><span class="p">(</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">))</span><span class="w">  </span><span class="c"># symmetric matrices have real eigenvectors/eigenvalues</span>
<span class="n">A_eig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eigen</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Λ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Diagonal</span><span class="p">(</span><span class="n">A_eig</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A_eig</span><span class="o">.</span><span class="n">vectors</span>
<span class="n">norm</span><span class="p">(</span><span class="n">Q</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Λ</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">inv</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.3961189709663296e-15
</pre></div>
</div>
</div>
</div>
<p>Keep in mind that a real matrix may have complex eigenvalues and eigenvectors, so if you attempt  to check <code class="docutils literal notranslate"><span class="pre">Q</span> <span class="pre">*</span> <span class="pre">Λ</span> <span class="pre">*</span> <span class="pre">inv(Q)</span> <span class="pre">-</span> <span class="pre">A</span></code> - even for a positive-definite matrix - it may not be a real number due to numerical inaccuracy.</p>
</div>
</div>
<div class="section" id="continuous-time-markov-chains-ctmcs">
<h2><a class="toc-backref" href="#id4"><span class="section-number">19.3. </span>Continuous-Time Markov Chains (CTMCs)</a><a class="headerlink" href="#continuous-time-markov-chains-ctmcs" title="Permalink to this headline">¶</a></h2>
<p>In the lecture on <a class="reference internal" href="../introduction_dynamics/finite_markov.html"><span class="doc">discrete-time Markov chains</span></a>, we saw that the transition probability
between state <span class="math notranslate nohighlight">\(x\)</span> and state <span class="math notranslate nohighlight">\(y\)</span> was summarized by the matrix <span class="math notranslate nohighlight">\(P(x, y) := \mathbb P \{ X_{t+1} = y \,|\, X_t = x \}\)</span>.</p>
<p>As a brief introduction to continuous time processes, consider the same state space as in the discrete
case: <span class="math notranslate nohighlight">\(S\)</span> is a finite set with <span class="math notranslate nohighlight">\(n\)</span> elements <span class="math notranslate nohighlight">\(\{x_1, \ldots, x_n\}\)</span>.</p>
<p>A <strong>Markov chain</strong> <span class="math notranslate nohighlight">\(\{X_t\}\)</span> on <span class="math notranslate nohighlight">\(S\)</span> is a sequence of random variables on <span class="math notranslate nohighlight">\(S\)</span> that have the <strong>Markov property</strong>.</p>
<p>In continuous time, the <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_property">Markov Property</a> is more complicated, but intuitively is
the same as the discrete-time case.</p>
<p>That is, knowing the current state is enough to know probabilities for future states.  Or, for realizations <span class="math notranslate nohighlight">\(x(\tau)\in S, \tau \leq t\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb P \{ X(t+s) = y  \,|\, X(t) = x, X(\tau) = x(\tau) \text{ for } 0 \leq \tau \leq t  \} = \mathbb P \{ X(t+s) = y  \,|\, X(t) = x\}
\]</div>
<p>Heuristically, consider a time period <span class="math notranslate nohighlight">\(t\)</span> and a small step forward, <span class="math notranslate nohighlight">\(\Delta\)</span>.  Then the probability to transition from state <span class="math notranslate nohighlight">\(i\)</span> to
state <span class="math notranslate nohighlight">\(j\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb P \{ X(t + \Delta) = j  \,|\, X(t) \} = \begin{cases} q_{ij} \Delta + o(\Delta) &amp; i \neq j\\
                                                              1 + q_{ii} \Delta + o(\Delta) &amp; i = j \end{cases}
\end{split}\]</div>
<p>where the <span class="math notranslate nohighlight">\(q_{ij}\)</span> are “intensity” parameters governing the transition rate, and <span class="math notranslate nohighlight">\(o(\Delta)\)</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Big_O_notation#Little-o_notation">little-o notation</a>.  That is, <span class="math notranslate nohighlight">\(\lim_{\Delta\to 0} o(\Delta)/\Delta = 0\)</span>.</p>
<p>Just as in the discrete case, we can summarize these parameters by an <span class="math notranslate nohighlight">\(N \times N\)</span> matrix, <span class="math notranslate nohighlight">\(Q \in R^{N\times N}\)</span>.</p>
<p>Recall that in the discrete case every element is weakly positive and every row must sum to one.   With continuous time, however, the rows of <span class="math notranslate nohighlight">\(Q\)</span> sum to zero, where the diagonal contains the negative value of jumping out of the current state.  That is,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(q_{ij} \geq 0\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(q_{ii} \leq 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_{j} q_{ij} = 0\)</span></p></li>
</ul>
<p>The <span class="math notranslate nohighlight">\(Q\)</span> matrix is called the intensity matrix, or the infinitesimal generator of the Markov chain.  For example,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Q = \begin{bmatrix} -0.1 &amp; 0.1  &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
                    0.1  &amp;-0.2  &amp; 0.1 &amp;  0 &amp; 0 &amp; 0\\
                    0 &amp; 0.1 &amp; -0.2 &amp; 0.1 &amp; 0 &amp; 0\\
                    0 &amp; 0 &amp; 0.1 &amp; -0.2 &amp; 0.1 &amp; 0\\
                    0 &amp; 0 &amp; 0 &amp; 0.1 &amp; -0.2 &amp; 0.1\\
                    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.1 &amp; -0.1\\
    \end{bmatrix}
\end{split}\]</div>
<p>In the above example, transitions occur only between adjacent states with the same intensity (except for a ``bouncing back’’ of the bottom and top states).</p>
<p>Implementing the <span class="math notranslate nohighlight">\(Q\)</span> using its tridiagonal structure</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">LinearAlgebra</span>
<span class="n">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1</span>
<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">6</span>
<span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">(</span><span class="n">fill</span><span class="p">(</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">α</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="o">-</span><span class="n">α</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6×6 Tridiagonal{Float64, Vector{Float64}}:
 -0.1   0.1    ⋅     ⋅     ⋅     ⋅ 
  0.1  -0.2   0.1    ⋅     ⋅     ⋅ 
   ⋅    0.1  -0.2   0.1    ⋅     ⋅ 
   ⋅     ⋅    0.1  -0.2   0.1    ⋅ 
   ⋅     ⋅     ⋅    0.1  -0.2   0.1
   ⋅     ⋅     ⋅     ⋅    0.1  -0.1
</pre></div>
</div>
</div>
</div>
<p>Here we can use <code class="docutils literal notranslate"><span class="pre">Tridiagonal</span></code> to exploit the structure of the problem.</p>
<p>Consider a simple payoff vector <span class="math notranslate nohighlight">\(r\)</span> associated with each state, and a discount rate <span class="math notranslate nohighlight">\(ρ\)</span>.  Then we can solve for
the expected present discounted value in a way similar to the discrete-time case.</p>
<div class="math notranslate nohighlight">
\[
\rho v = r + Q v
\]</div>
<p>or rearranging slightly, solving the linear system</p>
<div class="math notranslate nohighlight">
\[
(\rho I - Q) v = r
\]</div>
<p>For our example, exploiting the tridiagonal structure,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">range</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="w"> </span><span class="mf">10.0</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">ρ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.05</span>

<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ρ</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Q</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6×6 Tridiagonal{Float64, Vector{Float64}}:
  0.15  -0.1     ⋅      ⋅      ⋅      ⋅ 
 -0.1    0.25  -0.1     ⋅      ⋅      ⋅ 
   ⋅    -0.1    0.25  -0.1     ⋅      ⋅ 
   ⋅      ⋅    -0.1    0.25  -0.1     ⋅ 
   ⋅      ⋅      ⋅    -0.1    0.25  -0.1
   ⋅      ⋅      ⋅      ⋅    -0.1    0.15
</pre></div>
</div>
</div>
</div>
<p>Note that this <span class="math notranslate nohighlight">\(A\)</span> matrix is maintaining the tridiagonal structure of the problem, which leads to an efficient solution to the
linear problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">r</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6-element Vector{Float64}:
  38.15384615384615
  57.23076923076923
  84.92307692307693
 115.07692307692311
 142.76923076923077
 161.84615384615384
</pre></div>
</div>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(Q\)</span> is also used to calculate the evolution of the Markov chain, in direct analogy to the <span class="math notranslate nohighlight">\(ψ_{t+k} = ψ_t P^k\)</span> evolution with the transition matrix <span class="math notranslate nohighlight">\(P\)</span> of the discrete case.</p>
<p>In the continuous case, this becomes the system of linear differential equations</p>
<div class="math notranslate nohighlight">
\[
\dot{ψ}(t) = Q(t)^T ψ(t)
\]</div>
<p>given the initial condition <span class="math notranslate nohighlight">\(\psi(0)\)</span> and where the <span class="math notranslate nohighlight">\(Q(t)\)</span> intensity matrix is allowed to vary with time.  In the simplest case of a constant <span class="math notranslate nohighlight">\(Q\)</span> matrix, this is a simple constant-coefficient system of linear ODEs with coefficients <span class="math notranslate nohighlight">\(Q^T\)</span>.</p>
<p>If a stationary equilibrium exists, note that <span class="math notranslate nohighlight">\(\dot{ψ}(t) = 0\)</span>, and the stationary solution <span class="math notranslate nohighlight">\(ψ^{*}\)</span> needs to satisfy</p>
<div class="math notranslate nohighlight">
\[
0 = Q^T ψ^{*}
\]</div>
<p>Notice that this is of the form <span class="math notranslate nohighlight">\(0 ψ^{*} = Q^T ψ^{*}\)</span> and hence is equivalent to finding the eigenvector associated with the <span class="math notranslate nohighlight">\(\lambda = 0\)</span> eigenvalue of <span class="math notranslate nohighlight">\(Q^T\)</span>.</p>
<p>With our example, we can calculate all of the eigenvalues and eigenvectors</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">λ</span><span class="p">,</span><span class="w"> </span><span class="n">vecs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eigen</span><span class="p">(</span><span class="kt">Array</span><span class="p">(</span><span class="n">Q</span><span class="o">&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigen{Float64, Float64, Matrix{Float64}, Vector{Float64}}
values:
6-element Vector{Float64}:
 -0.3732050807568874
 -0.29999999999999993
 -0.19999999999999998
 -0.09999999999999995
 -0.026794919243112274
  0.0
vectors:
6×6 Matrix{Float64}:
 -0.149429  -0.288675   0.408248   0.5          -0.557678  0.408248
  0.408248   0.57735   -0.408248   1.38778e-16  -0.408248  0.408248
 -0.557678  -0.288675  -0.408248  -0.5          -0.149429  0.408248
  0.557678  -0.288675   0.408248  -0.5           0.149429  0.408248
 -0.408248   0.57735    0.408248   7.63278e-16   0.408248  0.408248
  0.149429  -0.288675  -0.408248   0.5           0.557678  0.408248
</pre></div>
</div>
</div>
</div>
<p>Indeed, there is a <span class="math notranslate nohighlight">\(\lambda = 0\)</span> eigenvalue, which is associated with the last column in the eigenvector.  To turn that into a probability,
we need to normalize it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">vecs</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">N</span><span class="p">]</span><span class="w"> </span><span class="o">./</span><span class="w"> </span><span class="n">sum</span><span class="p">(</span><span class="n">vecs</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="n">N</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6-element Vector{Float64}:
 0.16666666666666657
 0.16666666666666657
 0.1666666666666667
 0.16666666666666682
 0.16666666666666685
 0.16666666666666663
</pre></div>
</div>
</div>
</div>
<div class="section" id="multiple-dimensions">
<h3><span class="section-number">19.3.1. </span>Multiple Dimensions<a class="headerlink" href="#multiple-dimensions" title="Permalink to this headline">¶</a></h3>
<p>A frequent case in discretized models is dealing with Markov chains with multiple “spatial” dimensions (e.g., wealth and income).</p>
<p>After discretizing a process to create a Markov chain, you can always take the Cartesian product of the set of states in order to
enumerate it as a single state variable.</p>
<p>To see this, consider states <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> governed by infinitesimal generators <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span><span class="n">markov_chain_product</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">)</span>
<span class="w">    </span><span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparse</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="w">    </span><span class="n">Qs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">blockdiag</span><span class="p">(</span><span class="n">fill</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">)</span><span class="o">...</span><span class="p">)</span><span class="w">  </span><span class="c"># create diagonal blocks of every operator</span>
<span class="w">    </span><span class="n">As</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kron</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">sparse</span><span class="p">(</span><span class="n">I</span><span class="p">(</span><span class="n">M</span><span class="p">)))</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">As</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Qs</span>
<span class="k">end</span>

<span class="n">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1</span>
<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span>
<span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">(</span><span class="n">fill</span><span class="p">(</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">α</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="o">-</span><span class="n">α</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sparse</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="w"> </span><span class="mf">0.1</span>
<span class="w">    </span><span class="mf">0.2</span><span class="w"> </span><span class="o">-</span><span class="mf">0.2</span><span class="p">])</span>
<span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">L</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">markov_chain_product</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">)</span>
<span class="n">L</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="kt">Matrix</span><span class="w">  </span><span class="c"># display as a dense matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8×8 Matrix{Float64}:
 -0.2   0.1   0.0   0.0   0.1   0.0   0.0   0.0
  0.1  -0.3   0.1   0.0   0.0   0.1   0.0   0.0
  0.0   0.1  -0.3   0.1   0.0   0.0   0.1   0.0
  0.0   0.0   0.1  -0.2   0.0   0.0   0.0   0.1
  0.2   0.0   0.0   0.0  -0.3   0.1   0.0   0.0
  0.0   0.2   0.0   0.0   0.1  -0.4   0.1   0.0
  0.0   0.0   0.2   0.0   0.0   0.1  -0.4   0.1
  0.0   0.0   0.0   0.2   0.0   0.0   0.1  -0.3
</pre></div>
</div>
</div>
</div>
<p>This provides the combined Markov chain for the <span class="math notranslate nohighlight">\((i,j)\)</span> process.  To see the sparsity pattern,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">Plots</span>

<span class="n">spy</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="w"> </span><span class="n">markersize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><?xml version="1.0" encoding="utf-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="600" height="400" viewBox="0 0 2400 1600">
<defs>
  <clipPath id="clip780">
    <rect x="0" y="0" width="2400" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip780)" d="M0 1600 L2400 1600 L2400 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip781">
    <rect x="480" y="0" width="1681" height="1600"/>
  </clipPath>
</defs>
<path clip-path="url(#clip780)" d="M392.865 1486.45 L1832.07 1486.45 L1832.07 47.2441 L392.865 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"/>
<defs>
  <clipPath id="clip782">
    <rect x="392" y="47" width="1440" height="1440"/>
  </clipPath>
</defs>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="392.865,1486.45 1832.07,1486.45 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="627.56,1486.45 627.56,1467.55 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1015.49,1486.45 1015.49,1467.55 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1403.41,1486.45 1403.41,1467.55 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1791.34,1486.45 1791.34,1467.55 "/>
<path clip-path="url(#clip780)" d="M622.212 1544.91 L638.532 1544.91 L638.532 1548.85 L616.587 1548.85 L616.587 1544.91 Q619.249 1542.16 623.833 1537.53 Q628.439 1532.88 629.62 1531.53 Q631.865 1529.01 632.745 1527.27 Q633.648 1525.51 633.648 1523.82 Q633.648 1521.07 631.703 1519.33 Q629.782 1517.6 626.68 1517.6 Q624.481 1517.6 622.027 1518.36 Q619.597 1519.13 616.819 1520.68 L616.819 1515.95 Q619.643 1514.82 622.097 1514.24 Q624.55 1513.66 626.587 1513.66 Q631.958 1513.66 635.152 1516.35 Q638.347 1519.03 638.347 1523.52 Q638.347 1525.65 637.536 1527.57 Q636.749 1529.47 634.643 1532.07 Q634.064 1532.74 630.962 1535.95 Q627.861 1539.15 622.212 1544.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M1018.49 1518.36 L1006.69 1536.81 L1018.49 1536.81 L1018.49 1518.36 M1017.27 1514.29 L1023.15 1514.29 L1023.15 1536.81 L1028.08 1536.81 L1028.08 1540.7 L1023.15 1540.7 L1023.15 1548.85 L1018.49 1548.85 L1018.49 1540.7 L1002.89 1540.7 L1002.89 1536.19 L1017.27 1514.29 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M1403.82 1529.7 Q1400.67 1529.7 1398.82 1531.86 Q1396.99 1534.01 1396.99 1537.76 Q1396.99 1541.49 1398.82 1543.66 Q1400.67 1545.82 1403.82 1545.82 Q1406.96 1545.82 1408.79 1543.66 Q1410.64 1541.49 1410.64 1537.76 Q1410.64 1534.01 1408.79 1531.86 Q1406.96 1529.7 1403.82 1529.7 M1413.1 1515.05 L1413.1 1519.31 Q1411.34 1518.48 1409.53 1518.04 Q1407.75 1517.6 1405.99 1517.6 Q1401.36 1517.6 1398.91 1520.72 Q1396.48 1523.85 1396.13 1530.17 Q1397.5 1528.15 1399.56 1527.09 Q1401.62 1526 1404.09 1526 Q1409.3 1526 1412.31 1529.17 Q1415.34 1532.32 1415.34 1537.76 Q1415.34 1543.08 1412.2 1546.3 Q1409.05 1549.52 1403.82 1549.52 Q1397.82 1549.52 1394.65 1544.94 Q1391.48 1540.33 1391.48 1531.6 Q1391.48 1523.41 1395.37 1518.55 Q1399.26 1513.66 1405.81 1513.66 Q1407.57 1513.66 1409.35 1514.01 Q1411.15 1514.36 1413.1 1515.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M1791.34 1532.44 Q1788 1532.44 1786.08 1534.22 Q1784.18 1536 1784.18 1539.13 Q1784.18 1542.25 1786.08 1544.03 Q1788 1545.82 1791.34 1545.82 Q1794.67 1545.82 1796.59 1544.03 Q1798.51 1542.23 1798.51 1539.13 Q1798.51 1536 1796.59 1534.22 Q1794.69 1532.44 1791.34 1532.44 M1786.66 1530.45 Q1783.65 1529.7 1781.96 1527.64 Q1780.29 1525.58 1780.29 1522.62 Q1780.29 1518.48 1783.23 1516.07 Q1786.2 1513.66 1791.34 1513.66 Q1796.5 1513.66 1799.44 1516.07 Q1802.38 1518.48 1802.38 1522.62 Q1802.38 1525.58 1800.69 1527.64 Q1799.02 1529.7 1796.04 1530.45 Q1799.41 1531.23 1801.29 1533.52 Q1803.19 1535.82 1803.19 1539.13 Q1803.19 1544.15 1800.11 1546.83 Q1797.05 1549.52 1791.34 1549.52 Q1785.62 1549.52 1782.54 1546.83 Q1779.48 1544.15 1779.48 1539.13 Q1779.48 1535.82 1781.38 1533.52 Q1783.28 1531.23 1786.66 1530.45 M1784.95 1523.06 Q1784.95 1525.75 1786.61 1527.25 Q1788.3 1528.76 1791.34 1528.76 Q1794.35 1528.76 1796.04 1527.25 Q1797.75 1525.75 1797.75 1523.06 Q1797.75 1520.38 1796.04 1518.87 Q1794.35 1517.37 1791.34 1517.37 Q1788.3 1517.37 1786.61 1518.87 Q1784.95 1520.38 1784.95 1523.06 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="392.865,47.2441 392.865,1486.45 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="392.865,281.939 405.003,281.939 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="392.865,669.865 405.003,669.865 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="392.865,1057.79 405.003,1057.79 "/>
<polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="392.865,1445.72 405.003,1445.72 "/>
<path clip-path="url(#clip780)" d="M340.545 295.284 L356.865 295.284 L356.865 299.219 L334.92 299.219 L334.92 295.284 Q337.582 292.529 342.166 287.9 Q346.772 283.247 347.953 281.904 Q350.198 279.381 351.078 277.645 Q351.98 275.886 351.98 274.196 Q351.98 271.441 350.036 269.705 Q348.115 267.969 345.013 267.969 Q342.814 267.969 340.36 268.733 Q337.93 269.497 335.152 271.048 L335.152 266.326 Q337.976 265.191 340.43 264.613 Q342.883 264.034 344.92 264.034 Q350.291 264.034 353.485 266.719 Q356.679 269.404 356.679 273.895 Q356.679 276.025 355.869 277.946 Q355.082 279.844 352.976 282.437 Q352.397 283.108 349.295 286.326 Q346.193 289.52 340.545 295.284 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M347.281 656.659 L335.476 675.108 L347.281 675.108 L347.281 656.659 M346.055 652.585 L351.934 652.585 L351.934 675.108 L356.865 675.108 L356.865 678.996 L351.934 678.996 L351.934 687.145 L347.281 687.145 L347.281 678.996 L331.68 678.996 L331.68 674.483 L346.055 652.585 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M345.337 1055.93 Q342.189 1055.93 340.337 1058.08 Q338.508 1060.23 338.508 1063.98 Q338.508 1067.71 340.337 1069.88 Q342.189 1072.04 345.337 1072.04 Q348.485 1072.04 350.314 1069.88 Q352.166 1067.71 352.166 1063.98 Q352.166 1060.23 350.314 1058.08 Q348.485 1055.93 345.337 1055.93 M354.619 1041.27 L354.619 1045.53 Q352.86 1044.7 351.054 1044.26 Q349.272 1043.82 347.513 1043.82 Q342.883 1043.82 340.43 1046.95 Q337.999 1050.07 337.652 1056.39 Q339.018 1054.38 341.078 1053.31 Q343.138 1052.22 345.615 1052.22 Q350.823 1052.22 353.832 1055.39 Q356.865 1058.54 356.865 1063.98 Q356.865 1069.31 353.717 1072.52 Q350.568 1075.74 345.337 1075.74 Q339.342 1075.74 336.17 1071.16 Q332.999 1066.55 332.999 1057.82 Q332.999 1049.63 336.888 1044.77 Q340.777 1039.89 347.328 1039.89 Q349.087 1039.89 350.869 1040.23 Q352.675 1040.58 354.619 1041.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M345.013 1446.58 Q341.68 1446.58 339.758 1448.37 Q337.86 1450.15 337.86 1453.27 Q337.86 1456.4 339.758 1458.18 Q341.68 1459.96 345.013 1459.96 Q348.346 1459.96 350.267 1458.18 Q352.189 1456.38 352.189 1453.27 Q352.189 1450.15 350.267 1448.37 Q348.369 1446.58 345.013 1446.58 M340.337 1444.59 Q337.328 1443.85 335.638 1441.79 Q333.971 1439.73 333.971 1436.77 Q333.971 1432.63 336.911 1430.22 Q339.874 1427.81 345.013 1427.81 Q350.175 1427.81 353.115 1430.22 Q356.054 1432.63 356.054 1436.77 Q356.054 1439.73 354.365 1441.79 Q352.698 1443.85 349.712 1444.59 Q353.092 1445.38 354.967 1447.67 Q356.865 1449.96 356.865 1453.27 Q356.865 1458.3 353.786 1460.98 Q350.73 1463.67 345.013 1463.67 Q339.295 1463.67 336.217 1460.98 Q333.161 1458.3 333.161 1453.27 Q333.161 1449.96 335.059 1447.67 Q336.957 1445.38 340.337 1444.59 M338.624 1437.21 Q338.624 1439.89 340.291 1441.4 Q341.98 1442.9 345.013 1442.9 Q348.022 1442.9 349.712 1441.4 Q351.425 1439.89 351.425 1437.21 Q351.425 1434.52 349.712 1433.02 Q348.022 1431.51 345.013 1431.51 Q341.98 1431.51 340.291 1433.02 Q338.624 1434.52 338.624 1437.21 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><circle clip-path="url(#clip782)" cx="433.597" cy="87.9763" r="36" fill="#781c6d" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="433.597" cy="281.939" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="433.597" cy="863.827" r="36" fill="#fcfea4" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="627.56" cy="87.9763" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="627.56" cy="281.939" r="36" fill="#33095e" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="627.56" cy="475.902" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="627.56" cy="1057.79" r="36" fill="#fcfea4" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="821.522" cy="281.939" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="821.522" cy="475.902" r="36" fill="#33095e" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="821.522" cy="669.865" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="821.522" cy="1251.75" r="36" fill="#fcfea4" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1015.49" cy="475.902" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1015.49" cy="669.865" r="36" fill="#781c6d" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1015.49" cy="1445.72" r="36" fill="#fcfea4" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1209.45" cy="87.9763" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1209.45" cy="863.827" r="36" fill="#33095e" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1209.45" cy="1057.79" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1403.41" cy="281.939" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1403.41" cy="863.827" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1403.41" cy="1057.79" r="36" fill="#000003" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1403.41" cy="1251.75" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1597.37" cy="475.902" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1597.37" cy="1057.79" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1597.37" cy="1251.75" r="36" fill="#000003" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1597.37" cy="1445.72" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1791.34" cy="669.865" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1791.34" cy="1251.75" r="36" fill="#fbb419" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<circle clip-path="url(#clip782)" cx="1791.34" cy="1445.72" r="36" fill="#33095e" fill-rule="evenodd" fill-opacity="1" stroke="none"/>
<defs>
  <clipPath id="clip783">
    <rect x="1880" y="47" width="73" height="1440"/>
  </clipPath>
</defs>
<g clip-path="url(#clip783)">
<image width="72" height="1439" xlink:href="data:image/png;base64,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" transform="translate(1880, 47)"/>
</g>
<path clip-path="url(#clip780)" d="M1988.07 1483.27 L2017.74 1483.27 L2017.74 1487.21 L1988.07 1487.21 L1988.07 1483.27 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2037.84 1468.62 Q2034.23 1468.62 2032.4 1472.18 Q2030.59 1475.72 2030.59 1482.85 Q2030.59 1489.96 2032.4 1493.53 Q2034.23 1497.07 2037.84 1497.07 Q2041.47 1497.07 2043.28 1493.53 Q2045.11 1489.96 2045.11 1482.85 Q2045.11 1475.72 2043.28 1472.18 Q2041.47 1468.62 2037.84 1468.62 M2037.84 1464.91 Q2043.65 1464.91 2046.7 1469.52 Q2049.78 1474.1 2049.78 1482.85 Q2049.78 1491.58 2046.7 1496.19 Q2043.65 1500.77 2037.84 1500.77 Q2032.03 1500.77 2028.95 1496.19 Q2025.89 1491.58 2025.89 1482.85 Q2025.89 1474.1 2028.95 1469.52 Q2032.03 1464.91 2037.84 1464.91 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2058 1494.22 L2062.88 1494.22 L2062.88 1500.1 L2058 1500.1 L2058 1494.22 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2085.92 1469.61 L2074.11 1488.06 L2085.92 1488.06 L2085.92 1469.61 M2084.69 1465.54 L2090.57 1465.54 L2090.57 1488.06 L2095.5 1488.06 L2095.5 1491.95 L2090.57 1491.95 L2090.57 1500.1 L2085.92 1500.1 L2085.92 1491.95 L2070.31 1491.95 L2070.31 1487.44 L2084.69 1465.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M1988.07 1243.4 L2017.74 1243.4 L2017.74 1247.34 L1988.07 1247.34 L1988.07 1243.4 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2037.84 1228.75 Q2034.23 1228.75 2032.4 1232.32 Q2030.59 1235.86 2030.59 1242.99 Q2030.59 1250.09 2032.4 1253.66 Q2034.23 1257.2 2037.84 1257.2 Q2041.47 1257.2 2043.28 1253.66 Q2045.11 1250.09 2045.11 1242.99 Q2045.11 1235.86 2043.28 1232.32 Q2041.47 1228.75 2037.84 1228.75 M2037.84 1225.05 Q2043.65 1225.05 2046.7 1229.65 Q2049.78 1234.24 2049.78 1242.99 Q2049.78 1251.71 2046.7 1256.32 Q2043.65 1260.9 2037.84 1260.9 Q2032.03 1260.9 2028.95 1256.32 Q2025.89 1251.71 2025.89 1242.99 Q2025.89 1234.24 2028.95 1229.65 Q2032.03 1225.05 2037.84 1225.05 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2058 1254.35 L2062.88 1254.35 L2062.88 1260.23 L2058 1260.23 L2058 1254.35 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2087.23 1241.6 Q2090.59 1242.32 2092.47 1244.58 Q2094.36 1246.85 2094.36 1250.19 Q2094.36 1255.3 2090.85 1258.1 Q2087.33 1260.9 2080.85 1260.9 Q2078.67 1260.9 2076.35 1260.46 Q2074.06 1260.05 2071.61 1259.19 L2071.61 1254.68 Q2073.55 1255.81 2075.87 1256.39 Q2078.18 1256.97 2080.71 1256.97 Q2085.1 1256.97 2087.4 1255.23 Q2089.71 1253.5 2089.71 1250.19 Q2089.71 1247.13 2087.56 1245.42 Q2085.43 1243.68 2081.61 1243.68 L2077.58 1243.68 L2077.58 1239.84 L2081.79 1239.84 Q2085.24 1239.84 2087.07 1238.47 Q2088.9 1237.08 2088.9 1234.49 Q2088.9 1231.83 2087 1230.42 Q2085.13 1228.98 2081.61 1228.98 Q2079.69 1228.98 2077.49 1229.4 Q2075.29 1229.82 2072.65 1230.69 L2072.65 1226.53 Q2075.31 1225.79 2077.63 1225.42 Q2079.97 1225.05 2082.03 1225.05 Q2087.35 1225.05 2090.45 1227.48 Q2093.55 1229.88 2093.55 1234.01 Q2093.55 1236.88 2091.91 1238.87 Q2090.27 1240.83 2087.23 1241.6 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M1988.07 1003.54 L2017.74 1003.54 L2017.74 1007.47 L1988.07 1007.47 L1988.07 1003.54 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2037.84 988.883 Q2034.23 988.883 2032.4 992.448 Q2030.59 995.99 2030.59 1003.12 Q2030.59 1010.23 2032.4 1013.79 Q2034.23 1017.33 2037.84 1017.33 Q2041.47 1017.33 2043.28 1013.79 Q2045.11 1010.23 2045.11 1003.12 Q2045.11 995.99 2043.28 992.448 Q2041.47 988.883 2037.84 988.883 M2037.84 985.179 Q2043.65 985.179 2046.7 989.786 Q2049.78 994.369 2049.78 1003.12 Q2049.78 1011.85 2046.7 1016.45 Q2043.65 1021.04 2037.84 1021.04 Q2032.03 1021.04 2028.95 1016.45 Q2025.89 1011.85 2025.89 1003.12 Q2025.89 994.369 2028.95 989.786 Q2032.03 985.179 2037.84 985.179 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2058 1014.48 L2062.88 1014.48 L2062.88 1020.36 L2058 1020.36 L2058 1014.48 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2077.1 1016.43 L2093.42 1016.43 L2093.42 1020.36 L2071.47 1020.36 L2071.47 1016.43 Q2074.13 1013.67 2078.72 1009.05 Q2083.32 1004.39 2084.5 1003.05 Q2086.75 1000.53 2087.63 998.791 Q2088.53 997.031 2088.53 995.341 Q2088.53 992.587 2086.59 990.851 Q2084.67 989.115 2081.56 989.115 Q2079.36 989.115 2076.91 989.879 Q2074.48 990.642 2071.7 992.193 L2071.7 987.471 Q2074.53 986.337 2076.98 985.758 Q2079.43 985.179 2081.47 985.179 Q2086.84 985.179 2090.04 987.865 Q2093.23 990.55 2093.23 995.041 Q2093.23 997.17 2092.42 999.091 Q2091.63 1000.99 2089.53 1003.58 Q2088.95 1004.25 2085.85 1007.47 Q2082.74 1010.67 2077.1 1016.43 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M1988.07 763.669 L2017.74 763.669 L2017.74 767.604 L1988.07 767.604 L1988.07 763.669 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2037.84 749.016 Q2034.23 749.016 2032.4 752.581 Q2030.59 756.122 2030.59 763.252 Q2030.59 770.358 2032.4 773.923 Q2034.23 777.465 2037.84 777.465 Q2041.47 777.465 2043.28 773.923 Q2045.11 770.358 2045.11 763.252 Q2045.11 756.122 2043.28 752.581 Q2041.47 749.016 2037.84 749.016 M2037.84 745.312 Q2043.65 745.312 2046.7 749.919 Q2049.78 754.502 2049.78 763.252 Q2049.78 771.979 2046.7 776.585 Q2043.65 781.168 2037.84 781.168 Q2032.03 781.168 2028.95 776.585 Q2025.89 771.979 2025.89 763.252 Q2025.89 754.502 2028.95 749.919 Q2032.03 745.312 2037.84 745.312 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2058 774.618 L2062.88 774.618 L2062.88 780.497 L2058 780.497 L2058 774.618 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2073.88 776.562 L2081.52 776.562 L2081.52 750.196 L2073.21 751.863 L2073.21 747.604 L2081.47 745.937 L2086.15 745.937 L2086.15 776.562 L2093.79 776.562 L2093.79 780.497 L2073.88 780.497 L2073.88 776.562 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2000.01 509.149 Q1996.4 509.149 1994.57 512.713 Q1992.77 516.255 1992.77 523.385 Q1992.77 530.491 1994.57 534.056 Q1996.4 537.597 2000.01 537.597 Q2003.65 537.597 2005.45 534.056 Q2007.28 530.491 2007.28 523.385 Q2007.28 516.255 2005.45 512.713 Q2003.65 509.149 2000.01 509.149 M2000.01 505.445 Q2005.82 505.445 2008.88 510.051 Q2011.96 514.635 2011.96 523.385 Q2011.96 532.111 2008.88 536.718 Q2005.82 541.301 2000.01 541.301 Q1994.2 541.301 1991.12 536.718 Q1988.07 532.111 1988.07 523.385 Q1988.07 514.635 1991.12 510.051 Q1994.2 505.445 2000.01 505.445 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2000.01 269.281 Q1996.4 269.281 1994.57 272.846 Q1992.77 276.388 1992.77 283.517 Q1992.77 290.624 1994.57 294.189 Q1996.4 297.73 2000.01 297.73 Q2003.65 297.73 2005.45 294.189 Q2007.28 290.624 2007.28 283.517 Q2007.28 276.388 2005.45 272.846 Q2003.65 269.281 2000.01 269.281 M2000.01 265.578 Q2005.82 265.578 2008.88 270.184 Q2011.96 274.767 2011.96 283.517 Q2011.96 292.244 2008.88 296.851 Q2005.82 301.434 2000.01 301.434 Q1994.2 301.434 1991.12 296.851 Q1988.07 292.244 1988.07 283.517 Q1988.07 274.767 1991.12 270.184 Q1994.2 265.578 2000.01 265.578 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2020.17 294.883 L2025.06 294.883 L2025.06 300.763 L2020.17 300.763 L2020.17 294.883 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2036.05 296.827 L2043.69 296.827 L2043.69 270.462 L2035.38 272.128 L2035.38 267.869 L2043.65 266.203 L2048.32 266.203 L2048.32 296.827 L2055.96 296.827 L2055.96 300.763 L2036.05 300.763 L2036.05 296.827 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2000.01 29.414 Q1996.4 29.414 1994.57 32.9788 Q1992.77 36.5204 1992.77 43.65 Q1992.77 50.7565 1994.57 54.3213 Q1996.4 57.8629 2000.01 57.8629 Q2003.65 57.8629 2005.45 54.3213 Q2007.28 50.7565 2007.28 43.65 Q2007.28 36.5204 2005.45 32.9788 Q2003.65 29.414 2000.01 29.414 M2000.01 25.7103 Q2005.82 25.7103 2008.88 30.3168 Q2011.96 34.9001 2011.96 43.65 Q2011.96 52.3768 2008.88 56.9833 Q2005.82 61.5666 2000.01 61.5666 Q1994.2 61.5666 1991.12 56.9833 Q1988.07 52.3768 1988.07 43.65 Q1988.07 34.9001 1991.12 30.3168 Q1994.2 25.7103 2000.01 25.7103 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2020.17 55.0157 L2025.06 55.0157 L2025.06 60.8953 L2020.17 60.8953 L2020.17 55.0157 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><path clip-path="url(#clip780)" d="M2039.27 56.9601 L2055.59 56.9601 L2055.59 60.8953 L2033.65 60.8953 L2033.65 56.9601 Q2036.31 54.2055 2040.89 49.5759 Q2045.5 44.9232 2046.68 43.5806 Q2048.92 41.0574 2049.8 39.3213 Q2050.71 37.5621 2050.71 35.8723 Q2050.71 33.1177 2048.76 31.3816 Q2046.84 29.6455 2043.74 29.6455 Q2041.54 29.6455 2039.09 30.4093 Q2036.66 31.1732 2033.88 32.7241 L2033.88 28.002 Q2036.7 26.8677 2039.16 26.289 Q2041.61 25.7103 2043.65 25.7103 Q2049.02 25.7103 2052.21 28.3955 Q2055.41 31.0806 2055.41 35.5714 Q2055.41 37.701 2054.6 39.6223 Q2053.81 41.5204 2051.7 44.113 Q2051.12 44.7843 2048.02 48.0018 Q2044.92 51.1963 2039.27 56.9601 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1" /><polyline clip-path="url(#clip780)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="1952.07,1486.45 1952.07,1486.45 1976.07,1486.45 1952.07,1486.45 1952.07,1246.58 1976.07,1246.58 1952.07,1246.58 1952.07,1006.71 1976.07,1006.71 1952.07,1006.71 1952.07,766.846 1976.07,766.846 1952.07,766.846 1952.07,526.979 1976.07,526.979 1952.07,526.979 1952.07,287.111 1976.07,287.111 1952.07,287.111 1952.07,47.2441 1976.07,47.2441 1952.07,47.2441 "/>
</svg>
</div></div>
</div>
<p>To calculate a simple dynamic valuation, consider whether the payoff of being in state <span class="math notranslate nohighlight">\((i,j)\)</span> is <span class="math notranslate nohighlight">\(r_{ij} = i + 2j\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">2.0</span><span class="n">j</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="mi">1</span><span class="o">:</span><span class="n">M</span><span class="p">]</span>
<span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vec</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="w">  </span><span class="c"># vectorize it since stacked in same order</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8-element Vector{Float64}:
 3.0
 4.0
 5.0
 6.0
 5.0
 6.0
 7.0
 8.0
</pre></div>
</div>
</div>
</div>
<p>Solving the equation <span class="math notranslate nohighlight">\(\rho v = r + L v\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">ρ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.05</span>
<span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">ρ</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">L</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">r</span>
<span class="n">reshape</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4×2 Matrix{Float64}:
  87.8992   93.6134
  96.1345  101.849
 106.723   112.437
 114.958   120.672
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">reshape</span></code> helps to rearrange it back to being two-dimensional.</p>
<p>To find the stationary distribution, we calculate the eigenvalue and choose the eigenvector associated with <span class="math notranslate nohighlight">\(\lambda=0\)</span> .  In this
case, we can verify that it is the last one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">L_eig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eigen</span><span class="p">(</span><span class="kt">Matrix</span><span class="p">(</span><span class="n">L</span><span class="o">&#39;</span><span class="p">))</span>
<span class="nd">@assert</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">L_eig</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="k">end</span><span class="p">])</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">1E-10</span>

<span class="n">ψ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L_eig</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="o">:</span><span class="p">,</span><span class="k">end</span><span class="p">]</span>
<span class="n">ψ</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ψ</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">sum</span><span class="p">(</span><span class="n">ψ</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>8-element Vector{Float64}:
 0.16666666666666677
 0.1666666666666665
 0.16666666666666682
 0.16666666666666666
 0.08333333333333325
 0.08333333333333345
 0.0833333333333333
 0.08333333333333337
</pre></div>
</div>
</div>
</div>
<p>Reshape this to be two-dimensional if it is helpful for visualization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">reshape</span><span class="p">(</span><span class="n">ψ</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4×2 Matrix{Float64}:
 0.166667  0.0833333
 0.166667  0.0833333
 0.166667  0.0833333
 0.166667  0.0833333
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="irreducibility">
<h3><span class="section-number">19.3.2. </span>Irreducibility<a class="headerlink" href="#irreducibility" title="Permalink to this headline">¶</a></h3>
<p>As with the discrete-time Markov chains, a key question is whether CTMCs are reducible, i.e., whether states communicate.  The problem
is isomorphic to determining whether the directed graph of the Markov chain is <a class="reference external" href="https://en.wikipedia.org/wiki/Strongly_connected_component">strongly connected</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">Graphs</span>
<span class="n">α</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.1</span>
<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">6</span>
<span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">(</span><span class="n">fill</span><span class="p">(</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="n">α</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="o">-</span><span class="n">α</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="n">α</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>6×6 Tridiagonal{Float64, Vector{Float64}}:
 -0.1   0.1    ⋅     ⋅     ⋅     ⋅ 
  0.1  -0.2   0.1    ⋅     ⋅     ⋅ 
   ⋅    0.1  -0.2   0.1    ⋅     ⋅ 
   ⋅     ⋅    0.1  -0.2   0.1    ⋅ 
   ⋅     ⋅     ⋅    0.1  -0.2   0.1
   ⋅     ⋅     ⋅     ⋅    0.1  -0.1
</pre></div>
</div>
</div>
</div>
<p>We can verify that it is possible to move between every pair of states in a finite number of steps with</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">Q_graph</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DiGraph</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">is_strongly_connected</span><span class="p">(</span><span class="n">Q_graph</span><span class="p">);</span><span class="w">  </span><span class="c"># i.e., can follow directional edges to get to every state</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>is_strongly_connected(Q_graph) = true
</pre></div>
</div>
</div>
</div>
<p>Alternatively, as an example of a reducible Markov chain where states <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(2\)</span> cannot jump to state <span class="math notranslate nohighlight">\(3\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">-</span><span class="mf">0.2</span><span class="w"> </span><span class="mf">0.2</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="mf">0.2</span><span class="w"> </span><span class="o">-</span><span class="mf">0.2</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="mf">0.2</span><span class="w"> </span><span class="mf">0.6</span><span class="w"> </span><span class="o">-</span><span class="mf">0.8</span><span class="p">]</span>
<span class="n">Q_graph</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DiGraph</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">is_strongly_connected</span><span class="p">(</span><span class="n">Q_graph</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>is_strongly_connected(Q_graph) = false
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="banded-matrices">
<h2><a class="toc-backref" href="#id5"><span class="section-number">19.4. </span>Banded Matrices</a><a class="headerlink" href="#banded-matrices" title="Permalink to this headline">¶</a></h2>
<p>A tridiagonal matrix has 3 non-zero diagonals:  the main diagonal, the first sub-diagonal (i.e., below the main diagonal), and also the first super-diagonal (i.e., above the main diagonal).</p>
<p>This is a special case of a more general type called a banded matrix, where the number of sub- and super-diagonals can be greater than 1.  The
total width of main-, sub-, and super-diagonals is called the bandwidth.  For example, a tridiagonal matrix has a bandwidth of 3.</p>
<p>An <span class="math notranslate nohighlight">\(N \times N\)</span> banded matrix with bandwidth <span class="math notranslate nohighlight">\(P\)</span> has about <span class="math notranslate nohighlight">\(N P\)</span> nonzeros in its sparsity pattern.</p>
<p>These can be created directly as a dense matrix with <code class="docutils literal notranslate"><span class="pre">diagm</span></code>.  For example, with a bandwidth of three and a zero diagonal,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">diagm</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4×4 Matrix{Int64}:
 0  1  0  0
 4  0  2  0
 0  5  0  3
 0  0  6  0
</pre></div>
</div>
</div>
</div>
<p>Or as a sparse matrix,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">spdiagm</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4×4 SparseMatrixCSC{Int64, Int64} with 6 stored entries:
 ⋅  1  ⋅  ⋅
 4  ⋅  2  ⋅
 ⋅  5  ⋅  3
 ⋅  ⋅  6  ⋅
</pre></div>
</div>
</div>
</div>
<p>Or directly using <a class="reference external" href="https://github.com/JuliaMatrices/BandedMatrices.jl">BandedMatrices.jl</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">BandedMatrices</span>
<span class="n">BandedMatrix</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="o">=&gt;</span><span class="w"> </span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4×4 BandedMatrix{Int64} with bandwidths (1, 1):
 0  1  ⋅  ⋅
 4  0  2  ⋅
 ⋅  5  0  3
 ⋅  ⋅  6  0
</pre></div>
</div>
</div>
</div>
<p>There is also a convenience function for generating random banded matrices</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">brand</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w">  </span><span class="c"># 7x7 matrix, 3 subdiagonals, 1 superdiagonal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7×7 BandedMatrix{Float64} with bandwidths (3, 1):
 0.249137  0.849849   ⋅         ⋅          ⋅         ⋅         ⋅ 
 0.607815  0.253132  0.371965   ⋅          ⋅         ⋅         ⋅ 
 0.724436  0.121212  0.415992  0.25258     ⋅         ⋅         ⋅ 
 0.965024  0.437153  0.729472  0.336554   0.664299   ⋅         ⋅ 
  ⋅        0.976969  0.314399  0.0272443  0.397334  0.919194   ⋅ 
  ⋅         ⋅        0.403747  0.147311   0.74152   0.602738  0.913819
  ⋅         ⋅         ⋅        0.583009   0.578108  0.560479  0.579793
</pre></div>
</div>
</div>
</div>
<p>And, of course, specialized algorithms will be used to exploit the structure when solving linear systems.  In particular, the complexity is related to the <span class="math notranslate nohighlight">\(O(N P_L P_U)\)</span> for upper and lower bandwidths <span class="math notranslate nohighlight">\(P\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="nd">@show</span><span class="w"> </span><span class="n">factorize</span><span class="p">(</span><span class="n">Symmetric</span><span class="p">(</span><span class="n">A</span><span class="p">))</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span><span class="n">typeof</span>
<span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>factorize(Symmetric(A)) |&gt; typeof = LDLt{Float64, Symmetric{Float64, BandedMatrix{Float64, Matrix{Float64}, Base.OneTo{Int64}}}}
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7-element Vector{Float64}:
 -1.6369592693831965
  1.6204127338847327
  1.6129642421666908
  2.2654589446799647
 -1.0154611084639515
 -1.0012568753722133
  0.9146445743712437
</pre></div>
</div>
</div>
</div>
<p>The factorization algorithm uses a specialized LU decomposition for banded matrices.</p>
</div>
<div class="section" id="implementation-details-and-performance">
<span id="implementation-numerics"></span><h2><a class="toc-backref" href="#id6"><span class="section-number">19.5. </span>Implementation Details and Performance</a><a class="headerlink" href="#implementation-details-and-performance" title="Permalink to this headline">¶</a></h2>
<p>Recall the famous quote from Knuth: “97% of the time, premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”  The most common example of premature optimization is trying to use your own mental model of a compiler while writing your code, worried about the efficiency of code, and (usually incorrectly) second-guessing the compiler.</p>
<p>Concretely, the lessons in this section are:</p>
<ol class="simple">
<li><p>Don’t worry about optimizing your code unless you need to.  Code clarity is your first-order concern.</p></li>
<li><p>If you use other people’s packages, they can worry about performance and you don’t need to.</p></li>
<li><p>If you absolutely need that “critical 3%,” your intuition about performance is usually wrong on modern CPUs and GPUs, so let the compiler do its job.</p></li>
<li><p>Benchmarking (e.g., <code class="docutils literal notranslate"><span class="pre">&#64;btime</span></code>) and <a class="reference external" href="https://docs.julialang.org/en/v1/manual/profile/">profiling</a> are the tools to figure out performance bottlenecks.  If 99% of computing time is spent in one small function, then there is no point in optimizing anything else.</p></li>
<li><p>If you benchmark to show that a particular part of the code is an issue, and you can’t find another library that does a better job, then you can worry about performance.</p></li>
</ol>
<p>You will rarely get to step 3, let alone step 5.</p>
<p>However, there is also a corollary:  “don’t pessimize prematurely.” That is, don’t make choices that lead to poor performance without any tradeoff in improved code clarity.  For example, writing your own algorithms when a high-performance algorithm exists in a package or Julia itself, or lazily making a matrix dense and carelessly dropping its structure.</p>
<div class="section" id="implementation-difficulty">
<h3><span class="section-number">19.5.1. </span>Implementation Difficulty<a class="headerlink" href="#implementation-difficulty" title="Permalink to this headline">¶</a></h3>
<p>Numerical analysts sometimes refer to the lowest level of code for basic operations (e.g., a dot product, matrix-matrix product, convolutions) as <code class="docutils literal notranslate"><span class="pre">kernels</span></code>.</p>
<p>That sort of code is difficult to write, and performance depends on the characteristics of the underlying hardware, such as the <a class="reference external" href="https://en.wikipedia.org/wiki/Instruction_set_architecture">instruction set</a> available on the particular CPU, the size of the <a class="reference external" href="https://en.wikipedia.org/wiki/CPU_cache">CPU cache</a>, and the layout of arrays in memory.</p>
<p>Typically, these operations are written in a <a class="reference external" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms">BLAS</a> library, organized into different levels.  The levels roughly correspond to the computational order of the operations:  BLAS Level 1 are <span class="math notranslate nohighlight">\(O(N)\)</span> operations such as linear products, Level 2 are <span class="math notranslate nohighlight">\(O(N^2)\)</span> operations such as matrix-vector products, and Level 3 are roughly <span class="math notranslate nohighlight">\(O(N^3)\)</span>, such as general matrix-matrix products.</p>
<p>An example of a BLAS library is <a class="reference external" href="https://github.com/xianyi/OpenBLAS">OpenBLAS</a>, which is used by default in Julia, or  the <a class="reference external" href="https://en.wikipedia.org/wiki/Math_Kernel_Library">Intel MKL</a>, which is used in Matlab (and in Julia if the <code class="docutils literal notranslate"><span class="pre">MKL.jl</span></code> package is installed).</p>
<p>On top of BLAS are <a class="reference external" href="https://en.wikipedia.org/wiki/LAPACK">LAPACK</a> operations, which are higher-level kernels, such as matrix factorizations and eigenvalue algorithms, and are often in the same libraries (e.g., MKL has both BLAS and LAPACK functionality).</p>
<p>The details of these packages are not especially relevant, but if you are talking about performance, people will inevitably start discussing these different packages and kernels.  There are a few important things to keep in mind:</p>
<ol class="simple">
<li><p>Leave writing kernels to the experts.  Even simple-sounding algorithms can be very complicated to implement with high performance.</p></li>
<li><p>Your intuition about performance of code is probably going to be wrong.  If you use high quality libraries rather than writing your own kernels, you don’t need to use your intuition.</p></li>
<li><p>Don’t get distracted by the jargon or acronyms above if you are reading about performance.</p></li>
</ol>
</div>
<div class="section" id="row-and-column-major-ordering">
<h3><span class="section-number">19.5.2. </span>Row- and Column-Major Ordering<a class="headerlink" href="#row-and-column-major-ordering" title="Permalink to this headline">¶</a></h3>
<p>There is a practical performance issue which may influence your code.  Since memory in a CPU is linear, dense matrices need to be stored by either stacking columns (called <a class="reference external" href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">column-major order</a>) or rows.</p>
<p>The reason this matters is that compilers can generate better performance if they work in contiguous chunks of memory, and this becomes especially important with large matrices due to the interaction with the CPU cache.  Choosing the wrong order when there is no benefit in code clarity is an example of premature pessimization.  The performance difference can be orders of magnitude in some cases, and nothing in others.</p>
<p>One option is to use the functions that let the compiler choose the most efficient way to traverse memory. If you need to choose the looping order yourself, then you might want to experiment with going through columns first and going through rows first.  Other times, let Julia decide, i.e., <code class="docutils literal notranslate"><span class="pre">enumerate</span></code> and <code class="docutils literal notranslate"><span class="pre">eachindex</span></code> will choose the right approach.</p>
<p>Julia, Fortran, and Matlab all use column-major order, while C/C++ and Python use row-major order.  This means that if you find an algorithm written for C/C++/Python, you will sometimes need to make small changes if performance is an issue.</p>
</div>
<div class="section" id="digression-on-allocations-and-in-place-operations">
<h3><span class="section-number">19.5.3. </span>Digression on Allocations and In-place Operations<a class="headerlink" href="#digression-on-allocations-and-in-place-operations" title="Permalink to this headline">¶</a></h3>
<p>While we have usually not considered optimizing code for performance (and have focused on the choice of
algorithms instead), when matrices and vectors become large we need to be more careful.</p>
<p>The most important thing to avoid are excess allocations, which usually occur due to the use of
temporary vectors and matrices when they are not necessary.  Sometimes those extra temporary values
can cause enormous degradations in performance.</p>
<p>However, caution is suggested since
excess allocations are never relevant for scalar values, and allocations frequently create faster code for
smaller matrices/vectors since it can lead to better <a class="reference external" href="https://en.wikipedia.org/wiki/Locality_of_reference">cache locality</a>.</p>
<p>To see this, a convenient tool is the benchmarking</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">BenchmarkTools</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">similar</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="k">function</span><span class="w"> </span><span class="n">f!</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">)</span>
<span class="w">    </span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="o">*</span><span class="n">B</span>
<span class="w">    </span><span class="n">C</span><span class="w"> </span><span class="o">.=</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">.+</span><span class="w"> </span><span class="mi">1</span>
<span class="k">end</span>
<span class="nd">@btime</span><span class="w"> </span><span class="n">f!</span><span class="p">(</span><span class="o">$</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  312.753 ns (1 allocation: 896 bytes)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10×10 Matrix{Float64}:
 3.88622  3.89353  3.77256  3.03683  …  2.3747   2.06317  2.85813  4.12785
 5.00088  4.89479  4.5616   2.93174     2.86965  2.58255  3.32457  5.01006
 4.16559  3.89196  3.48728  2.36875     2.39433  2.35954  2.6446   3.69582
 4.72342  4.26309  4.10953  2.58485     2.80941  2.30148  2.6341   3.95345
 3.83682  4.552    3.991    2.91925     3.08619  2.3282   2.58817  4.06832
 4.50285  4.63918  4.62707  2.32979  …  2.79878  2.68404  3.12501  4.82882
 4.57254  4.50141  4.25523  3.01381     2.91585  2.27358  3.06085  4.17532
 4.642    4.00792  4.38117  2.09846     2.6599   2.62388  3.17824  4.34158
 3.69463  3.4852   3.2082   2.19226     2.29474  2.21371  2.33719  3.25337
 4.76994  4.89471  4.73363  3.13832     2.90939  2.59762  3.38554  4.96909
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">!</span></code> on the <code class="docutils literal notranslate"><span class="pre">f!</span></code> is an informal way to say that the function is mutating, and the first argument (<code class="docutils literal notranslate"><span class="pre">C</span></code> here)
is by convention the modified variable.</p>
<p>In the <code class="docutils literal notranslate"><span class="pre">f!</span></code> function, notice that the <code class="docutils literal notranslate"><span class="pre">D</span></code> is a temporary variable which is created, and then modified afterwards.  But notice that since
<code class="docutils literal notranslate"><span class="pre">C</span></code> is modified directly, there is no need to create the temporary <code class="docutils literal notranslate"><span class="pre">D</span></code> matrix.</p>
<p>This is an example of where an in-place version of the matrix multiplication can help avoid the allocation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="k">function</span><span class="w"> </span><span class="n">f2!</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">)</span>
<span class="w">    </span><span class="n">mul!</span><span class="p">(</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="w">  </span><span class="c"># in-place multiplication</span>
<span class="w">    </span><span class="n">C</span><span class="w"> </span><span class="o">.+=</span><span class="w"> </span><span class="mi">1</span>
<span class="k">end</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">similar</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nd">@btime</span><span class="w"> </span><span class="n">f!</span><span class="p">(</span><span class="o">$</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">B</span><span class="p">)</span>
<span class="nd">@btime</span><span class="w"> </span><span class="n">f2!</span><span class="p">(</span><span class="o">$</span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">$</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  310.927 ns (1 allocation: 896 bytes)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  248.282 ns (0 allocations: 0 bytes)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10×10 Matrix{Float64}:
 3.49484  3.39944  2.78868  3.30176  …  3.95507  3.03323  2.69935  2.94465
 2.55706  1.86998  2.24931  2.39319     3.04391  2.22918  1.87232  2.5985
 4.09158  3.5013   3.62429  3.80519     4.60324  3.11816  3.16512  3.63433
 3.67359  4.13121  3.88012  4.55375     5.39446  3.35861  3.33792  3.67154
 3.39581  4.40599  3.42146  4.39003     5.07657  3.47102  3.28428  3.77209
 3.79119  3.59088  3.16204  4.45079  …  4.98626  3.28275  3.15713  3.47816
 3.85483  4.33821  3.60923  3.93005     5.00665  3.97537  3.35012  4.00407
 4.89039  5.28359  4.77468  5.43633     6.72286  4.54196  4.0695   4.80744
 3.75726  3.34515  3.26204  3.78259     4.45989  3.6188   2.89155  3.41107
 4.08829  4.03342  3.21527  4.17448     4.33974  3.37887  3.08931  3.02885
</pre></div>
</div>
</div>
</div>
<p>Note that in the output of the benchmarking, the <code class="docutils literal notranslate"><span class="pre">f2!</span></code> is non-allocating and is using the pre-allocated <code class="docutils literal notranslate"><span class="pre">C</span></code> variable directly.</p>
<p>Another example of this is solutions to linear equations, where for large solutions you may pre-allocate and reuse the
solution vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">z</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">\</span><span class="w"> </span><span class="n">y</span><span class="w">  </span><span class="c"># creates temporary</span>

<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">factorize</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w">  </span><span class="c"># in-place requires factorization</span>
<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">similar</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="w">  </span><span class="c"># pre-allocate</span>
<span class="n">ldiv!</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">  </span><span class="c"># in-place left divide, using factorization</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10-element Vector{Float64}:
  2.301397405532209
  0.08468738177707942
  0.7473201607390055
 -2.747417513346128
 -1.6932787858580565
  0.12375574392037789
 -1.4428955275936803
  1.3411435637041675
  2.761463941320385
  0.07288425762859364
</pre></div>
</div>
</div>
</div>
<p>However, if you benchmark carefully, you will see that this is sometimes slower.  Avoiding allocations is not always a good
idea - and worrying about it prior to benchmarking is premature optimization.</p>
<p>There are a variety of other non-allocating versions of functions.  For example,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">similar</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">transpose!</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="p">)</span><span class="w">  </span><span class="c"># non-allocating version of B = transpose(A)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10×10 Matrix{Float64}:
 0.811601    0.899243  0.725708   0.284021  …  0.354106  0.112426  0.339154
 0.00790333  0.86226   0.251181   0.474599     0.34251   0.222795  0.0186245
 0.123037    0.668323  0.0619222  0.582055     0.959484  0.32293   0.214821
 0.0012518   0.733197  0.617966   0.151617     0.171723  0.74272   0.209474
 0.0911146   0.822027  0.910671   0.294019     0.832758  0.578819  0.129152
 0.425627    0.822726  0.355305   0.197185  …  0.172408  0.447427  0.823456
 0.668306    0.17692   0.765922   0.667631     0.873703  0.25864   0.0134306
 0.388655    0.931105  0.20858    0.383996     0.799093  0.164938  0.395565
 0.304053    0.695924  0.0553802  0.989633     0.928237  0.739555  0.334263
 0.710061    0.613563  0.883984   0.619658     0.13839   0.282556  0.891416
</pre></div>
</div>
</div>
</div>
<p>Finally, a common source of unnecessary allocations is when taking slices or portions of
matrices.  For example, the following allocates a new matrix <code class="docutils literal notranslate"><span class="pre">B</span></code> and copies the values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">:</span><span class="p">]</span><span class="w">  </span><span class="c"># extract a vector</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>5-element Vector{Float64}:
 0.9230450911751324
 0.08136956335528056
 0.8791975622688366
 0.3124595901751288
 0.47680030576508337
</pre></div>
</div>
</div>
</div>
<p>To see that these are different matrices, note that</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">100.0</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A[2, 1] = 100.0
B[1] = 0.9230450911751324
</pre></div>
</div>
</div>
</div>
<p>Instead of allocating a new matrix, you can take a <code class="docutils literal notranslate"><span class="pre">view</span></code> of a matrix, which provides an
appropriate <code class="docutils literal notranslate"><span class="pre">AbstractArray</span></code> type that doesn’t allocate new memory with the <code class="docutils literal notranslate"><span class="pre">&#64;view</span></code> matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nd">@view</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">:</span><span class="p">]</span><span class="w">  </span><span class="c">#  does not copy the data</span>

<span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">100.0</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="nd">@show</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">];</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A[2, 1] = 100.0
B[1] = 100.0
</pre></div>
</div>
</div>
</div>
<p>But again, you will often find that doing <code class="docutils literal notranslate"><span class="pre">&#64;view</span></code> leads to slower code.  Benchmark
instead, and generally rely on it for large matrices and for contiguous chunks of memory (e.g., columns rather than rows).</p>
<!-- Commenting out.  Worried this will be misused.  We can move this sort of pattern to a performance section later
## Patterns for Preallocated Caches of Results

When the matrices and vectors get large, it can reach a point where it is important to cache the results and reduce allocations.  In general, this should only be attempted when the vectors are large and they would otherwise need to be reallocated many times.

One approach is to create a {doc}`custom type<../getting_started_julia/introduction_to_types>` to hold the results, being very careful to ensure that the type is concrete.


```{code-cell} julia
struct MyResults{T}
    x::Array{T,1}
    A::Array{T,2}
end
MyResults(N) = MyResults(Array{Float64,1}(undef, N), Array{Float64,2}(undef, N, N))    

#Support for inplace copying
import Base.copy!
function copy!(dst::MyResults, src::MyResults)
    dst.x .= src.x
    dst.A .= src.A
end
```

The above code is an **immutable** structure for holding the `x` and `A` buffers.  The fact that it is immutable ensures that the `x` vector itself cannot be changed (even if the values within `x` can be).  This can help ensure that you do not accidentally reallocate and assign a new vector to `x`.

The only other code is an implementation of the in-place `copy!` function, which will allow us to copy all of the results as required by some algorithms.

To create a contrived algorithm, see the following code:

```{code-cell} julia
# By convention, name has ! to denote mutating, and mutate first argument
function calculate_results!(results, val, params)
    (;N, b, C) = params
    B = rand(N,N)  # Contrived.  Assume complicated
    lmul!(val, B)  # val * B -> B inplace, no allocation
    mul!(results.A, B, C) #   B * C -> results.A
    ldiv!(results.x, factorize(results.A), b)  # x = A \ b inplace
end

# Some iterative algorithm
function iterate_values(vals, params)    
    (;N) = params
    
    # preallocate
    results = MyResults(N)
    prev_results = MyResults(N)
    norms = similar(vals)
        
    for (i, val) in enumerate(vals)
        calculate_results!(results, val, params)
        norms[i] = norm(results.x- prev_results.x)            
        println("|x_new - x_old| = ", norms[i])
        copy!(prev_results, results)
    end
    return norms
end

params = (N = 5, C = rand(5,5), b = rand(5))
vals = range(0.0, 1.0, length = 10)
iterate_values(vals, params)
```

A few points:
- This creates an inplace function, `calculate_results!`, which modifies the results given a value and parameters.
- Within the function, it attempts to use inplace versions of the operations where possible, which can help cut down on other allocations
- The iteration simply goes through a list of values, calls the `calculate_results!` and then checks how the `results.x` has changed with a norm.
- Finally, the iteration copies the new results into the previous ones.  This ensure that only a single copy is required for comparison.



This approach can be very helpful for large matrices and arrays, but should be used judiciously and only after {doc}`profiling<../software_engineering/need_for_speed>`  .
-->
</div>
</div>
<div class="section" id="exercises">
<h2><a class="toc-backref" href="#id7"><span class="section-number">19.6. </span>Exercises</a><a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<div class="section" id="exercise-1">
<h3><span class="section-number">19.6.1. </span>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">¶</a></h3>
<p>This exercise is for practice on writing low-level routines (i.e., “kernels”), and to hopefully convince you to leave low-level code to the experts.</p>
<p>The formula for matrix multiplication is deceptively simple.  For example, with the product of square matrices <span class="math notranslate nohighlight">\(C = A B\)</span> of size <span class="math notranslate nohighlight">\(N \times N\)</span>, the <span class="math notranslate nohighlight">\(i,j\)</span> element of <span class="math notranslate nohighlight">\(C\)</span> is</p>
<div class="math notranslate nohighlight">
\[
C_{ij} = \sum_{k=1}^N A_{ik} B_{kj}
\]</div>
<p>Alternatively, you can take a row <span class="math notranslate nohighlight">\(A_{i,:}\)</span> and column <span class="math notranslate nohighlight">\(B_{:, j}\)</span> and use an inner product</p>
<div class="math notranslate nohighlight">
\[
C_{ij} = A_{i,:} \cdot B_{:,j}
\]</div>
<p>Note that the inner product in a discrete space is simply a sum, and has the same complexity as the sum (i.e., <span class="math notranslate nohighlight">\(O(N)\)</span> operations).</p>
<p>For a dense matrix without any structure and using a naive multiplication algorithm, this also makes it clear why the complexity is <span class="math notranslate nohighlight">\(O(N^3)\)</span>: You need to evaluate it for <span class="math notranslate nohighlight">\(N^2\)</span> elements in the matrix and do an <span class="math notranslate nohighlight">\(O(N)\)</span> operation each time.</p>
<p>For this exercise, implement matrix multiplication yourself and compare performance in a few permutations.</p>
<ol class="simple">
<li><p>Use the built-in function in Julia (i.e., <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">A</span> <span class="pre">*</span> <span class="pre">B</span></code>, or, for a better comparison, the in-place version <code class="docutils literal notranslate"><span class="pre">mul!(C,</span> <span class="pre">A,</span> <span class="pre">B)</span></code>, which works with pre-allocated data).</p></li>
<li><p>Loop over each <span class="math notranslate nohighlight">\(C_{ij}\)</span> by the row first (i.e., the <code class="docutils literal notranslate"><span class="pre">i</span></code> index) and use a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop for the inner product.</p></li>
<li><p>Loop over each <span class="math notranslate nohighlight">\(C_{ij}\)</span> by the column first (i.e., the <code class="docutils literal notranslate"><span class="pre">j</span></code> index) and use a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop for the inner product.</p></li>
<li><p>Do the same but use the <code class="docutils literal notranslate"><span class="pre">dot</span></code> product instead of the sum.</p></li>
<li><p>Choose your best implementation of these, and then for matrices of a few different sizes (<code class="docutils literal notranslate"><span class="pre">N=10</span></code>, <code class="docutils literal notranslate"><span class="pre">N=1000</span></code>, etc.), and compare the ratio of performance of your best implementation to the built-in BLAS library.</p></li>
</ol>
<p>A few more hints:</p>
<ul class="simple">
<li><p>You can just use random matrices (e.g., <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">rand(N,</span> <span class="pre">N)</span></code>).</p></li>
<li><p>For all of them, pre-allocate the <span class="math notranslate nohighlight">\(C\)</span> matrix beforehand with <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">similar(A)</span></code> or something equivalent.</p></li>
<li><p>To compare performance, put your code in a function and use the <code class="docutils literal notranslate"><span class="pre">&#64;btime</span></code> macro to time it.</p></li>
</ul>
</div>
<div class="section" id="exercise-2a">
<h3><span class="section-number">19.6.2. </span>Exercise 2a<a class="headerlink" href="#exercise-2a" title="Permalink to this headline">¶</a></h3>
<p>Here we will calculate the evolution of the pdf of a discrete-time Markov chain, <span class="math notranslate nohighlight">\(\psi_t\)</span>, given the initial condition <span class="math notranslate nohighlight">\(\psi_0\)</span>.</p>
<p>Start with a simple symmetric tridiagonal matrix</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span>
<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Tridiagonal</span><span class="p">([</span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">);</span><span class="w"> </span><span class="mf">0.2</span><span class="p">],</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="p">[</span><span class="mf">0.2</span><span class="p">;</span><span class="w"> </span><span class="n">fill</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="o">-</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">A_adjoint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="o">&#39;</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<ol class="simple">
<li><p>Pick some large <code class="docutils literal notranslate"><span class="pre">T</span></code> and use the initial condition <span class="math notranslate nohighlight">\(\psi_0 = \begin{bmatrix} 1 &amp; 0 &amp; \ldots &amp; 0\end{bmatrix}\)</span></p></li>
<li><p>Write code to calculate <span class="math notranslate nohighlight">\(\psi_t\)</span> to some <span class="math notranslate nohighlight">\(T\)</span> by iterating the map for each <span class="math notranslate nohighlight">\(t\)</span>, i.e.,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\psi_{t+1} = A' \psi_t
\]</div>
<ol class="simple">
<li><p>What is the computational order of calculating  <span class="math notranslate nohighlight">\(\psi_T\)</span> using this iteration approach <span class="math notranslate nohighlight">\(T &lt; N\)</span>?</p></li>
<li><p>What is the computational order of <span class="math notranslate nohighlight">\((A')^T = (A' \ldots A')\)</span> and then <span class="math notranslate nohighlight">\(\psi_T = (A')^T \psi_0\)</span> for <span class="math notranslate nohighlight">\(T &lt; N\)</span>?</p></li>
<li><p>Benchmark calculating <span class="math notranslate nohighlight">\(\psi_T\)</span> with the iterative calculation above as well as the direct <span class="math notranslate nohighlight">\(\psi_T = (A')^T \psi_0\)</span> to see which is faster.  You can take the matrix power with just <code class="docutils literal notranslate"><span class="pre">A_adjoint^T</span></code>, which uses specialized algorithms faster and more accurately than repeated matrix multiplication (but with the same computational order).</p></li>
<li><p>Check the same if <span class="math notranslate nohighlight">\(T = 2 N\)</span></p></li>
</ol>
<p><em>Note:</em> The algorithm used in Julia to take matrix powers  depends on the matrix structure, as always.  In the symmetric case, it can use an eigendecomposition, whereas with a general dense matrix it uses <a class="reference external" href="https://doi.org/10.1137/090768539">squaring and scaling</a>.</p>
</div>
<div class="section" id="exercise-2b">
<h3><span class="section-number">19.6.3. </span>Exercise 2b<a class="headerlink" href="#exercise-2b" title="Permalink to this headline">¶</a></h3>
<p>With the same setup as in Exercise 2a, do an <a class="reference external" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">eigendecomposition</a> of <code class="docutils literal notranslate"><span class="pre">A_transpose</span></code>.  That is, use <code class="docutils literal notranslate"><span class="pre">eigen</span></code> to factor the adjoint <span class="math notranslate nohighlight">\(A' = Q \Lambda Q^{-1}\)</span>, where <span class="math notranslate nohighlight">\(Q\)</span> is the matrix of eigenvectors and <span class="math notranslate nohighlight">\(\Lambda\)</span> is the diagonal matrix of eigenvalues.  Calculate <span class="math notranslate nohighlight">\(Q^{-1}\)</span> from the results.</p>
<p>Use the factored matrix to calculate the sequence of <span class="math notranslate nohighlight">\(\psi_t = (A')^t \psi_0\)</span> using the relationship</p>
<div class="math notranslate nohighlight">
\[
\psi_t = Q \Lambda^t Q^{-1} \psi_0
\]</div>
<p>where matrix powers of diagonal matrices are simply the element-wise power of each element.</p>
<p>Benchmark the speed of calculating the sequence of <span class="math notranslate nohighlight">\(\psi_t\)</span> up to <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">=</span> <span class="pre">2N</span></code> using this method.  In principle, the factorization and easy calculation of the power should give you benefits, compared to simply iterating the map as we did in Exercise 2a.  Explain why it does or does not, using computational order of each approach.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "julia-1.9"
        },
        kernelOptions: {
            kernelName: "julia-1.9",
            path: "./tools_and_techniques"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'julia-1.9'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="stationary_densities.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">18. </span>Continuous State Markov Chains</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="iterative_methods_sparsity.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">20. </span>Krylov Methods and Matrix Conditioning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Jesse Perla & Thomas J. Sargent & John Stachurski<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>